{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"sim",
				"simd"
			],
			[
				"I",
				"INT32"
			],
			[
				"AS_2C",
				"AS_2CHARS"
			],
			[
				"MMS",
				"_mm_shuffle_epi32"
			],
			[
				"simd",
				"simd"
			],
			[
				"mmshu",
				"_MM_SHUFFLE"
			],
			[
				"cac",
				"cacheline_size"
			],
			[
				"P",
				"Processor"
			],
			[
				"cp",
				"cpuid"
			],
			[
				"0x",
				"0x7F"
			],
			[
				"core",
				"core_size"
			],
			[
				"ecx",
				"ecx805"
			],
			[
				"cach",
				"cacheline_size"
			],
			[
				"num",
				"num_threads"
			],
			[
				"pr",
				"pragma"
			],
			[
				"hasSS",
				"hasSSE"
			],
			[
				"has",
				"hasAVX"
			],
			[
				"A",
				"AVX"
			],
			[
				"_mm_load",
				"_mm_load_ps"
			],
			[
				"can",
				"canAdd"
			],
			[
				"_mm_sto",
				"_mm256_store_ps"
			],
			[
				"nu",
				"num_packets"
			],
			[
				"pa",
				"packet_size"
			],
			[
				"avx",
				"AVX"
			],
			[
				"si",
				"size2048"
			],
			[
				"size",
				"size4096"
			],
			[
				"cop",
				"copy1024"
			],
			[
				"para",
				"parallel"
			],
			[
				"SI",
				"size128"
			],
			[
				"siz",
				"size128"
			],
			[
				"copy",
				"copy128"
			],
			[
				"hasAV",
				"hasAVX"
			],
			[
				"ec",
				"ecx7"
			],
			[
				"cpui",
				"cpu_info"
			],
			[
				"cpu",
				"cpu_info"
			],
			[
				"dele",
				"deleteHeap"
			],
			[
				"push",
				"push_back	func"
			],
			[
				"unsigned",
				"unsigned_family"
			],
			[
				"Iint",
				"Init	func"
			],
			[
				"pus",
				"push_back	func"
			],
			[
				"Itype",
				"I-type_traits	inc"
			],
			[
				"pop",
				"pop_back	func"
			],
			[
				"beg",
				"begin	func"
			],
			[
				"unsig",
				"unsigned"
			],
			[
				"const",
				"atomic_compare_exchange_strong_explicit	atomic"
			],
			[
				"inc",
				"I-map	inc"
			],
			[
				"res",
				"responseContentLength"
			],
			[
				"resp",
				"responseStatusCode"
			],
			[
				"HTTPHEAD",
				"HTTP_HEADER_CONTENT_TYPE"
			],
			[
				"_s",
				"_status"
			],
			[
				"HTTP_ERROR_",
				"HTTP_ERROR_INVALID_RESPONSE"
			],
			[
				"prin",
				"println"
			],
			[
				"ser",
				"serverIP"
			],
			[
				"send",
				"sendInitialHeaders"
			]
		]
	},
	"buffers":
	[
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/sparse.h",
			"settings":
			{
				"buffer_size": 5630,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/ui.h",
			"settings":
			{
				"buffer_size": 1192,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/string.h",
			"settings":
			{
				"buffer_size": 2875,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/simd.h",
			"settings":
			{
				"buffer_size": 11047,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/variadic.h",
			"settings":
			{
				"buffer_size": 1892,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/kmp.h",
			"settings":
			{
				"buffer_size": 701,
				"line_ending": "Windows",
				"name": "#pragma once"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/util.h",
			"settings":
			{
				"buffer_size": 312,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/fixed.h",
			"settings":
			{
				"buffer_size": 5793,
				"line_ending": "Windows"
			}
		},
		{
			"contents": "/****************************  instrset.h   **********************************\n* Author:        Agner Fog\n* Date created:  2012-05-30\n* Last modified: 2016-11-25\n* Version:       1.25\n* Project:       vector classes\n* Description:\n* Header file for various compiler-specific tasks and other common tasks to \n* vector class library:\n* > selects the supported instruction set\n* > defines integer types\n* > defines compiler version macros\n* > undefines certain macros that prevent function overloading\n* > defines template class to represent compile-time integer constant\n* > defines template for compile-time error messages\n*\n* (c) Copyright 2012-2016 GNU General Public License www.gnu.org/licenses\n******************************************************************************/\n\n#ifndef INSTRSET_H\n#define INSTRSET_H 125\n\n// Detect 64 bit mode\n#if (defined(_M_AMD64) || defined(_M_X64) || defined(__amd64) ) && ! defined(__x86_64__)\n#define __x86_64__ 1  // There are many different macros for this, decide on only one\n#endif\n\n// Find instruction set from compiler macros if INSTRSET not defined\n// Note: Most of these macros are not defined in Microsoft compilers\n#ifndef INSTRSET\n#if defined ( __AVX512F__ ) || defined ( __AVX512__ )\n#define INSTRSET 9\n#elif defined ( __AVX2__ )\n#define INSTRSET 8\n#elif defined ( __AVX__ )\n#define INSTRSET 7\n#elif defined ( __SSE4_2__ )\n#define INSTRSET 6\n#elif defined ( __SSE4_1__ )\n#define INSTRSET 5\n#elif defined ( __SSSE3__ )\n#define INSTRSET 4\n#elif defined ( __SSE3__ )\n#define INSTRSET 3\n#elif defined ( __SSE2__ ) || defined ( __x86_64__ )\n#define INSTRSET 2\n#elif defined ( __SSE__ )\n#define INSTRSET 1\n#elif defined ( _M_IX86_FP )           // Defined in MS compiler. 1: SSE, 2: SSE2\n#define INSTRSET _M_IX86_FP\n#else \n#define INSTRSET 0\n#endif // instruction set defines\n#endif // INSTRSET\n\n// Include the appropriate header file for intrinsic functions\n#if INSTRSET > 7                       // AVX2 and later\n#if defined (__GNUC__) && ! defined (__INTEL_COMPILER)\n#include <x86intrin.h>                 // x86intrin.h includes header files for whatever instruction \n                                       // sets are specified on the compiler command line, such as:\n                                       // xopintrin.h, fma4intrin.h\n#else\n#include <immintrin.h>                 // MS version of immintrin.h covers AVX, AVX2 and FMA3\n#endif // __GNUC__\n#elif INSTRSET == 7\n#include <immintrin.h>                 // AVX\n#elif INSTRSET == 6\n#include <nmmintrin.h>                 // SSE4.2\n#elif INSTRSET == 5\n#include <smmintrin.h>                 // SSE4.1\n#elif INSTRSET == 4\n#include <tmmintrin.h>                 // SSSE3\n#elif INSTRSET == 3\n#include <pmmintrin.h>                 // SSE3\n#elif INSTRSET == 2\n#include <emmintrin.h>                 // SSE2\n#elif INSTRSET == 1\n#include <xmmintrin.h>                 // SSE\n#endif // INSTRSET\n\n#if INSTRSET >= 8 && !defined(__FMA__)\n// Assume that all processors that have AVX2 also have FMA3\n#if defined (__GNUC__) && ! defined (__INTEL_COMPILER) && ! defined (__clang__)\n// Prevent error message in g++ when using FMA intrinsics with avx2:\n#pragma message \"It is recommended to specify also option -mfma when using -mavx2 or higher\"\n#else\n#define __FMA__  1\n#endif\n#endif\n\n// AMD  instruction sets\n#if defined (__XOP__) || defined (__FMA4__)\n#ifdef __GNUC__\n#include <x86intrin.h>                 // AMD XOP (Gnu)\n#else\n#include <ammintrin.h>                 // AMD XOP (Microsoft)\n#endif //  __GNUC__\n#elif defined (__SSE4A__)              // AMD SSE4A\n#include <ammintrin.h>\n#endif // __XOP__ \n\n// FMA3 instruction set\n#if defined (__FMA__) && (defined(__GNUC__) || defined(__clang__))  && ! defined (__INTEL_COMPILER)\n#include <fmaintrin.h> \n#endif // __FMA__ \n\n// FMA4 instruction set\n#if defined (__FMA4__) && (defined(__GNUC__) || defined(__clang__))\n#include <fma4intrin.h> // must have both x86intrin.h and fma4intrin.h, don't know why\n#endif // __FMA4__\n\n\n// Define integer types with known size\n#if defined(__GNUC__) || defined(__clang__) || (defined(_MSC_VER) && _MSC_VER >= 1600)\n  // Compilers supporting C99 or C++0x have stdint.h defining these integer types\n  #include <stdint.h>\n#elif defined(_MSC_VER)\n  // Older Microsoft compilers have their own definitions\n  typedef signed   __int8  int8_t;\n  typedef unsigned __int8  uint8_t;\n  typedef signed   __int16 int16_t;\n  typedef unsigned __int16 uint16_t;\n  typedef signed   __int32 int32_t;\n  typedef unsigned __int32 uint32_t;\n  typedef signed   __int64 int64_t;\n  typedef unsigned __int64 uint64_t;\n  #ifndef _INTPTR_T_DEFINED\n    #define _INTPTR_T_DEFINED\n    #ifdef  __x86_64__\n      typedef int64_t intptr_t;\n    #else\n      typedef int32_t intptr_t;\n    #endif\n  #endif\n#else\n  // This works with most compilers\n  typedef signed   char      int8_t;\n  typedef unsigned char      uint8_t;\n  typedef signed   short int int16_t;\n  typedef unsigned short int uint16_t;\n  typedef signed   int       int32_t;\n  typedef unsigned int       uint32_t;\n  typedef long long          int64_t;\n  typedef unsigned long long uint64_t;\n  #ifdef  __x86_64__\n    typedef int64_t intptr_t;\n  #else\n    typedef int32_t intptr_t;\n  #endif\n#endif\n\n#include <stdlib.h>                              // define abs(int)\n\n#ifdef _MSC_VER                                  // Microsoft compiler or compatible Intel compiler\n#include <intrin.h>                              // define _BitScanReverse(int), __cpuid(int[4],int), _xgetbv(int)\n#endif // _MSC_VER\n\n// functions in instrset_detect.cpp\n#ifdef VCL_NAMESPACE\nnamespace VCL_NAMESPACE {\n#endif\n    int  instrset_detect(void);                      // tells which instruction sets are supported\n    bool hasFMA3(void);                              // true if FMA3 instructions supported\n    bool hasFMA4(void);                              // true if FMA4 instructions supported\n    bool hasXOP(void);                               // true if XOP  instructions supported\n    bool hasAVX512ER(void);                          // true if AVX512ER instructions supported\n#ifdef VCL_NAMESPACE\n}\n#endif\n\n// GCC version\n#if defined(__GNUC__) && !defined (GCC_VERSION) && !defined (__clang__)\n#define GCC_VERSION  ((__GNUC__) * 10000 + (__GNUC_MINOR__) * 100 + (__GNUC_PATCHLEVEL__))\n#endif\n\n// Clang version\n#if defined (__clang__)\n#define CLANG_VERSION  ((__clang_major__) * 10000 + (__clang_minor__) * 100 + (__clang_patchlevel__))\n// Problem: The version number is not consistent across platforms\n// http://llvm.org/bugs/show_bug.cgi?id=12643\n// Apple bug 18746972\n#endif\n\n// Fix problem with non-overloadable macros named min and max in WinDef.h\n#ifdef _MSC_VER\n#if defined (_WINDEF_) && defined(min) && defined(max)\n#undef min\n#undef max\n#endif\n#ifndef NOMINMAX\n#define NOMINMAX\n#endif\n#endif\n\n#ifdef VCL_NAMESPACE\nnamespace VCL_NAMESPACE {\n#endif\n    // Template class to represent compile-time integer constant\n    template <int32_t  n> class Const_int_t {};       // represent compile-time signed integer constant\n    template <uint32_t n> class Const_uint_t {};      // represent compile-time unsigned integer constant\n    #define const_int(n)  (Const_int_t <n>())         // n must be compile-time integer constant\n    #define const_uint(n) (Const_uint_t<n>())         // n must be compile-time unsigned integer constant\n\n    // Template for compile-time error messages\n    template <bool> class Static_error_check {\n    public:  Static_error_check() {};\n    };\n    template <> class Static_error_check<false> {     // generate compile-time error if false\n    private: Static_error_check() {};\n    };\n#ifdef VCL_NAMESPACE\n}\n#endif \n\n\n#endif // INSTRSET_H\n",
			"settings":
			{
				"buffer_size": 7659,
				"line_ending": "Windows",
				"name": "/****************************  instrset.h   ******"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/sparse.cc",
			"settings":
			{
				"buffer_size": 37,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/sparse.t.cc",
			"settings":
			{
				"buffer_size": 711,
				"line_ending": "Windows",
				"name": "#include <game-engine/math/sparse.h>"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/point.t.cc",
			"settings":
			{
				"buffer_size": 2140,
				"line_ending": "Windows",
				"name": "#include <game-engine/math/point.h>"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed.cc",
			"settings":
			{
				"buffer_size": 36,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed.t.cc",
			"settings":
			{
				"buffer_size": 1272,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed_simd.cc",
			"settings":
			{
				"buffer_size": 42,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed_simd.t.cc",
			"settings":
			{
				"buffer_size": 660,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/string.cc",
			"settings":
			{
				"buffer_size": 6929,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/simd.cc",
			"settings":
			{
				"buffer_size": 2073,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/projects/VS/compile.bat",
			"settings":
			{
				"buffer_size": 95,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/string.t.cc",
			"settings":
			{
				"buffer_size": 113,
				"line_ending": "Windows",
				"name": "#include <game-engine/core/string.h>"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/fixed_simd.h",
			"settings":
			{
				"buffer_size": 7876,
				"line_ending": "Windows",
				"name": "#pragma once"
			}
		},
		{
			"contents": "/****************************  vectori128.h   *******************************\n* Author:        Agner Fog\n* Date created:  2012-05-30\n* Last modified: 2016-11-25\n* Version:       1.25\n* Project:       vector classes\n* Description:\n* Header file defining integer vector classes as interface to intrinsic \n* functions in x86 microprocessors with SSE2 and later instruction sets\n* up to AVX.\n*\n* Instructions:\n* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired \n* instruction set, which must be at least SSE2. Specify the supported \n* instruction set by a command line define, e.g. __SSE4_1__ if the \n* compiler does not automatically do so.\n*\n* The following vector classes are defined here:\n* Vec128b   Vector of 128  1-bit unsigned  integers or Booleans\n* Vec16c    Vector of  16  8-bit signed    integers\n* Vec16uc   Vector of  16  8-bit unsigned  integers\n* Vec16cb   Vector of  16  Booleans for use with Vec16c and Vec16uc\n* Vec8s     Vector of   8  16-bit signed   integers\n* Vec8us    Vector of   8  16-bit unsigned integers\n* Vec8sb    Vector of   8  Booleans for use with Vec8s and Vec8us\n* Vec4i     Vector of   4  32-bit signed   integers\n* Vec4ui    Vector of   4  32-bit unsigned integers\n* Vec4ib    Vector of   4  Booleans for use with Vec4i and Vec4ui\n* Vec2q     Vector of   2  64-bit signed   integers\n* Vec2uq    Vector of   2  64-bit unsigned integers\n* Vec2qb    Vector of   2  Booleans for use with Vec2q and Vec2uq\n*\n* Each vector object is represented internally in the CPU as a 128-bit register.\n* This header file defines operators and functions for these vectors.\n*\n* For example:\n* Vec4i a(1,2,3,4), b(5,6,7,8), c;\n* c = a + b;     // now c contains (6,8,10,12)\n*\n* For detailed instructions, see VectorClass.pdf\n*\n* (c) Copyright 2012-2016 GNU General Public License http://www.gnu.org/licenses\n*****************************************************************************/\n#ifndef VECTORI128_H\n#define VECTORI128_H\n\n#include \"instrset.h\"  // Select supported instruction set\n\n#if INSTRSET < 2   // SSE2 required\n#error Please compile for the SSE2 instruction set or higher\n#endif\n\n#ifdef VCL_NAMESPACE\nnamespace VCL_NAMESPACE {\n#endif\n\n/*****************************************************************************\n*\n*          Vector of 128 1-bit unsigned integers or Booleans\n*\n*****************************************************************************/\nclass Vec128b {\nprotected:\n    __m128i xmm; // Integer vector\npublic:\n    // Default constructor:\n    Vec128b() {\n    }\n    // Constructor to broadcast the same value into all elements\n    // Removed because of undesired implicit conversions\n    // Vec128b(int i) {\n    //     xmm = _mm_set1_epi32(-(i & 1));}\n\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec128b(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec128b & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Type cast operator to convert to __m128i used in intrinsics\n    operator __m128i() const {\n        return xmm;\n    }\n    // Member function to load from array (unaligned)\n    Vec128b & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array, aligned by 16\n    // \"load_a\" is faster than \"load\" on older Intel processors (Pentium 4, Pentium M, Core 1,\n    // Merom, Wolfdale) and Atom, but not on other processors from Intel, AMD or VIA.\n    // You may use load_a instead of load if you are certain that p points to an address\n    // divisible by 16.\n    void load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n    }\n    // Member function to store into array (unaligned)\n    void store(void * p) const {\n        _mm_storeu_si128((__m128i*)p, xmm);\n    }\n    // Member function to store into array, aligned by 16\n    // \"store_a\" is faster than \"store\" on older Intel processors (Pentium 4, Pentium M, Core 1,\n    // Merom, Wolfdale) and Atom, but not on other processors from Intel, AMD or VIA.\n    // You may use store_a instead of store if you are certain that p points to an address\n    // divisible by 16.\n    void store_a(void * p) const {\n        _mm_store_si128((__m128i*)p, xmm);\n    }\n    // Member function to change a single bit\n    // Note: This function is inefficient. Use load function if changing more than one bit\n    Vec128b const & set_bit(uint32_t index, int value) {\n        static const union {\n            uint64_t i[4];\n            __m128i  x[2];\n        } u = {{1,0,0,1}};                 // 2 vectors with bit 0 and 64 set, respectively\n        int w = (index >> 6) & 1;          // qword index\n        int bi = index & 0x3F;             // bit index within qword w\n        __m128i mask = u.x[w];\n        mask = _mm_sll_epi64(mask,_mm_cvtsi32_si128(bi)); // mask with bit number b set\n        if (value & 1) {\n            xmm = _mm_or_si128(mask,xmm);\n        }\n        else {\n            xmm = _mm_andnot_si128(mask,xmm);\n        }\n        return *this;\n    }\n    // Member function to get a single bit\n    // Note: This function is inefficient. Use store function if reading more than one bit\n    int get_bit(uint32_t index) const {\n        union {\n            __m128i x;\n            uint8_t i[16];\n        } u;\n        u.x = xmm; \n        int w = (index >> 3) & 0xF;            // byte index\n        int bi = index & 7;                    // bit index within byte w\n        return (u.i[w] >> bi) & 1;\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    bool operator [] (uint32_t index) const {\n        return get_bit(index) != 0;\n    }\n    static int size() {\n        return 128;\n    }\n};\n\n\n// Define operators for this class\n\n// vector operator & : bitwise and\nstatic inline Vec128b operator & (Vec128b const & a, Vec128b const & b) {\n    return _mm_and_si128(a, b);\n}\nstatic inline Vec128b operator && (Vec128b const & a, Vec128b const & b) {\n    return a & b;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec128b operator | (Vec128b const & a, Vec128b const & b) {\n    return _mm_or_si128(a, b);\n}\nstatic inline Vec128b operator || (Vec128b const & a, Vec128b const & b) {\n    return a | b;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec128b operator ^ (Vec128b const & a, Vec128b const & b) {\n    return _mm_xor_si128(a, b);\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec128b operator ~ (Vec128b const & a) {\n    return _mm_xor_si128(a, _mm_set1_epi32(-1));\n}\n\n// vector operator &= : bitwise and\nstatic inline Vec128b & operator &= (Vec128b & a, Vec128b const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator |= : bitwise or\nstatic inline Vec128b & operator |= (Vec128b & a, Vec128b const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^= : bitwise xor\nstatic inline Vec128b & operator ^= (Vec128b & a, Vec128b const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// Define functions for this class\n\n// function andnot: a & ~ b\nstatic inline Vec128b andnot (Vec128b const & a, Vec128b const & b) {\n    return _mm_andnot_si128(b, a);\n}\n\n\n/*****************************************************************************\n*\n*          Generate compile-time constant vector\n*\n*****************************************************************************/\n// Generate a constant vector of 4 integers stored in memory.\n// Can be converted to any integer vector type\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline __m128i constant4i() {\n    static const union {\n        int     i[4];\n        __m128i xmm;\n    } u = {{i0,i1,i2,i3}};\n    return u.xmm;\n}\n\n\n/*****************************************************************************\n*\n*          selectb function\n*\n*****************************************************************************/\n// Select between two sources, byte by byte. Used in various functions and operators\n// Corresponds to this pseudocode:\n// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];\n// Each byte in s must be either 0 (false) or 0xFF (true). No other values are allowed.\n// The implementation depends on the instruction set: \n// If SSE4.1 is supported then only bit 7 in each byte of s is checked, \n// otherwise all bits in s are used.\nstatic inline __m128i selectb (__m128i const & s, __m128i const & a, __m128i const & b) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    return _mm_blendv_epi8 (b, a, s);\n#else\n    return _mm_or_si128(\n        _mm_and_si128(s,a),\n        _mm_andnot_si128(s,b));\n#endif\n}\n\n\n\n/*****************************************************************************\n*\n*          Horizontal Boolean functions\n*\n*****************************************************************************/\n\n// horizontal_and. Returns true if all bits are 1\nstatic inline bool horizontal_and (Vec128b const & a) {\n#if INSTRSET >= 5   // SSE4.1 supported. Use PTEST\n    return _mm_testc_si128(a,constant4i<-1,-1,-1,-1>()) != 0;\n#else\n    __m128i t1 = _mm_unpackhi_epi64(a,a);                  // get 64 bits down\n    __m128i t2 = _mm_and_si128(a,t1);                      // and 64 bits\n#ifdef __x86_64__\n    int64_t t5 = _mm_cvtsi128_si64(t2);                    // transfer 64 bits to integer\n    return  t5 == int64_t(-1);\n#else\n    __m128i t3 = _mm_srli_epi64(t2,32);                    // get 32 bits down\n    __m128i t4 = _mm_and_si128(t2,t3);                     // and 32 bits\n    int     t5 = _mm_cvtsi128_si32(t4);                    // transfer 32 bits to integer\n    return  t5 == -1;\n#endif  // __x86_64__\n#endif  // INSTRSET\n}\n\n// horizontal_or. Returns true if at least one bit is 1\nstatic inline bool horizontal_or (Vec128b const & a) {\n#if INSTRSET >= 5   // SSE4.1 supported. Use PTEST\n    return ! _mm_testz_si128(a,a);\n#else\n    __m128i t1 = _mm_unpackhi_epi64(a,a);                  // get 64 bits down\n    __m128i t2 = _mm_or_si128(a,t1);                       // and 64 bits\n#ifdef __x86_64__\n    int64_t t5 = _mm_cvtsi128_si64(t2);                    // transfer 64 bits to integer\n    return  t5 != int64_t(0);\n#else\n    __m128i t3 = _mm_srli_epi64(t2,32);                    // get 32 bits down\n    __m128i t4 = _mm_or_si128(t2,t3);                      // and 32 bits\n    int     t5 = _mm_cvtsi128_si32(t4);                    // transfer to integer\n    return  t5 != 0;\n#endif  // __x86_64__\n#endif  // INSTRSET\n}\n\n\n\n/*****************************************************************************\n*\n*          Vector of 16 8-bit signed integers\n*\n*****************************************************************************/\n\nclass Vec16c : public Vec128b {\npublic:\n    // Default constructor:\n    Vec16c() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec16c(int i) {\n        xmm = _mm_set1_epi8((char)i);\n    }\n    // Constructor to build from all elements:\n    Vec16c(int8_t i0, int8_t i1, int8_t i2, int8_t i3, int8_t i4, int8_t i5, int8_t i6, int8_t i7,\n        int8_t i8, int8_t i9, int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15) {\n        xmm = _mm_setr_epi8(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec16c(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec16c & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Type cast operator to convert to __m128i used in intrinsics\n    operator __m128i() const {\n        return xmm;\n    }\n    // Member function to load from array (unaligned)\n    Vec16c & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec16c & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Partial load. Load n elements and set the rest to 0\n    Vec16c & load_partial(int n, void const * p) {\n        if      (n >= 16) load(p);\n        else if (n <= 0)  *this = 0;\n        else if (((int)(intptr_t)p & 0xFFF) < 0xFF0) {\n            // p is at least 16 bytes from a page boundary. OK to read 16 bytes\n            load(p);\n        }\n        else {\n            // worst case. read 1 byte at a time and suffer store forwarding penalty\n            char x[16];\n            for (int i = 0; i < n; i++) x[i] = ((char const *)p)[i];\n            load(x);\n        }\n        cutoff(n);\n        return *this;\n    }\n    // Partial store. Store n elements\n    void store_partial(int n, void * p) const {\n        if (n >= 16) {\n            store(p);\n            return;\n        }\n        if (n <= 0) return;\n        // we are not using _mm_maskmoveu_si128 because it is too slow on many processors\n        union {        \n            int8_t  c[16];\n            int16_t s[8];\n            int32_t i[4];\n            int64_t q[2];\n        } u;\n        store(u.c);\n        int j = 0;\n        if (n & 8) {\n            *(int64_t*)p = u.q[0];\n            j += 8;\n        }\n        if (n & 4) {\n            ((int32_t*)p)[j/4] = u.i[j/4];\n            j += 4;\n        }\n        if (n & 2) {\n            ((int16_t*)p)[j/2] = u.s[j/2];\n            j += 2;\n        }\n        if (n & 1) {\n            ((int8_t*)p)[j]    = u.c[j];\n        }\n    }\n    // cut off vector to n elements. The last 16-n elements are set to zero\n    Vec16c & cutoff(int n) {\n        if (uint32_t(n) >= 16) return *this;\n        static const char mask[32] = {-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,\n            0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};\n        *this &= Vec16c().load(mask+16-n);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec16c const & insert(uint32_t index, int8_t value) {\n        static const int8_t maskl[32] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n            -1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};\n        __m128i broad = _mm_set1_epi8(value);  // broadcast value into all elements\n        __m128i mask  = _mm_loadu_si128((__m128i const*)(maskl+16-(index & 0x0F))); // mask with FF at index position\n        xmm = selectb(mask,broad,xmm);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    int8_t extract(uint32_t index) const {\n        int8_t x[16];\n        store(x);\n        return x[index & 0x0F];\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    int8_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n    static int size() {\n        return 16;\n    }\n};\n\n/*****************************************************************************\n*\n*          Vec16cb: Vector of 16 Booleans for use with Vec16c and Vec16uc\n*\n*****************************************************************************/\n\nclass Vec16cb : public Vec16c {\npublic:\n    // Default constructor\n    Vec16cb() {}\n    // Constructor to build from all elements:\n    Vec16cb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,\n        bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) {\n        xmm = Vec16c(-int8_t(x0), -int8_t(x1), -int8_t(x2), -int8_t(x3), -int8_t(x4), -int8_t(x5), -int8_t(x6), -int8_t(x7), \n            -int8_t(x8), -int8_t(x9), -int8_t(x10), -int8_t(x11), -int8_t(x12), -int8_t(x13), -int8_t(x14), -int8_t(x15));\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec16cb(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec16cb & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Constructor to broadcast scalar value:\n    Vec16cb(bool b) : Vec16c(-int8_t(b)) {\n    }\n    // Assignment operator to broadcast scalar value:\n    Vec16cb & operator = (bool b) {\n        *this = Vec16cb(b);\n        return *this;\n    }\nprivate: // Prevent constructing from int, etc.\n    Vec16cb(int b);\n    Vec16cb & operator = (int x);\npublic:\n    Vec16cb & insert (int index, bool a) {\n        Vec16c::insert(index, -(int)a);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    bool extract(uint32_t index) const {\n        return Vec16c::extract(index) != 0;\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    bool operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n\n/*****************************************************************************\n*\n*          Define operators for Vec16cb\n*\n*****************************************************************************/\n\n// vector operator & : bitwise and\nstatic inline Vec16cb operator & (Vec16cb const & a, Vec16cb const & b) {\n    return Vec16cb(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec16cb operator && (Vec16cb const & a, Vec16cb const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec16cb & operator &= (Vec16cb & a, Vec16cb const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec16cb operator | (Vec16cb const & a, Vec16cb const & b) {\n    return Vec16cb(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec16cb operator || (Vec16cb const & a, Vec16cb const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec16cb & operator |= (Vec16cb & a, Vec16cb const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec16cb operator ^ (Vec16cb const & a, Vec16cb const & b) {\n    return Vec16cb(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec16cb & operator ^= (Vec16cb & a, Vec16cb const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec16cb operator ~ (Vec16cb const & a) {\n    return Vec16cb( ~ Vec128b(a));\n}\n\n// vector operator ! : element not\nstatic inline Vec16cb operator ! (Vec16cb const & a) {\n    return ~ a;\n}\n\n// vector function andnot\nstatic inline Vec16cb andnot (Vec16cb const & a, Vec16cb const & b) {\n    return Vec16cb(andnot(Vec128b(a), Vec128b(b)));\n}\n\n// Horizontal Boolean functions for Vec16cb\n\n// horizontal_and. Returns true if all elements are true\nstatic inline bool horizontal_and(Vec16cb const & a) {\n    return _mm_movemask_epi8(a) == 0xFFFF;\n}\n\n// horizontal_or. Returns true if at least one element is true\nstatic inline bool horizontal_or(Vec16cb const & a) {\n#if INSTRSET >= 5   // SSE4.1 supported. Use PTEST\n    return !_mm_testz_si128(a, a);\n#else\n    return _mm_movemask_epi8(a) != 0;\n#endif\n} \n\n\n/*****************************************************************************\n*\n*          Define operators for Vec16c\n*\n*****************************************************************************/\n\n// vector operator + : add element by element\nstatic inline Vec16c operator + (Vec16c const & a, Vec16c const & b) {\n    return _mm_add_epi8(a, b);\n}\n\n// vector operator += : add\nstatic inline Vec16c & operator += (Vec16c & a, Vec16c const & b) {\n    a = a + b;\n    return a;\n}\n\n// postfix operator ++\nstatic inline Vec16c operator ++ (Vec16c & a, int) {\n    Vec16c a0 = a;\n    a = a + 1;\n    return a0;\n}\n\n// prefix operator ++\nstatic inline Vec16c & operator ++ (Vec16c & a) {\n    a = a + 1;\n    return a;\n}\n\n// vector operator - : subtract element by element\nstatic inline Vec16c operator - (Vec16c const & a, Vec16c const & b) {\n    return _mm_sub_epi8(a, b);\n}\n\n// vector operator - : unary minus\nstatic inline Vec16c operator - (Vec16c const & a) {\n    return _mm_sub_epi8(_mm_setzero_si128(), a);\n}\n\n// vector operator -= : add\nstatic inline Vec16c & operator -= (Vec16c & a, Vec16c const & b) {\n    a = a - b;\n    return a;\n}\n\n// postfix operator --\nstatic inline Vec16c operator -- (Vec16c & a, int) {\n    Vec16c a0 = a;\n    a = a - 1;\n    return a0;\n}\n\n// prefix operator --\nstatic inline Vec16c & operator -- (Vec16c & a) {\n    a = a - 1;\n    return a;\n}\n\n// vector operator * : multiply element by element\nstatic inline Vec16c operator * (Vec16c const & a, Vec16c const & b) {\n    // There is no 8-bit multiply in SSE2. Split into two 16-bit multiplies\n    __m128i aodd    = _mm_srli_epi16(a,8);                 // odd numbered elements of a\n    __m128i bodd    = _mm_srli_epi16(b,8);                 // odd numbered elements of b\n    __m128i muleven = _mm_mullo_epi16(a,b);                // product of even numbered elements\n    __m128i mulodd  = _mm_mullo_epi16(aodd,bodd);          // product of odd  numbered elements\n            mulodd  = _mm_slli_epi16(mulodd,8);            // put odd numbered elements back in place\n    __m128i mask    = _mm_set1_epi32(0x00FF00FF);          // mask for even positions\n    __m128i product = selectb(mask,muleven,mulodd);        // interleave even and odd\n    return product;\n}\n\n// vector operator *= : multiply\nstatic inline Vec16c & operator *= (Vec16c & a, Vec16c const & b) {\n    a = a * b;\n    return a;\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec16c operator << (Vec16c const & a, int b) {\n    uint32_t mask = (uint32_t)0xFF >> (uint32_t)b;         // mask to remove bits that are shifted out\n    __m128i am    = _mm_and_si128(a,_mm_set1_epi8((char)mask));  // remove bits that will overflow\n    __m128i res   = _mm_sll_epi16(am,_mm_cvtsi32_si128(b));// 16-bit shifts\n    return res;\n}\n\n// vector operator <<= : shift left\nstatic inline Vec16c & operator <<= (Vec16c & a, int b) {\n    a = a << b;\n    return a;\n}\n\n// vector operator >> : shift right arithmetic all elements\nstatic inline Vec16c operator >> (Vec16c const & a, int b) {\n    __m128i aeven = _mm_slli_epi16(a,8);                   // even numbered elements of a. get sign bit in position\n            aeven = _mm_sra_epi16(aeven,_mm_cvtsi32_si128(b+8)); // shift arithmetic, back to position\n    __m128i aodd  = _mm_sra_epi16(a,_mm_cvtsi32_si128(b)); // shift odd numbered elements arithmetic\n    __m128i mask    = _mm_set1_epi32(0x00FF00FF);          // mask for even positions\n    __m128i res     = selectb(mask,aeven,aodd);            // interleave even and odd\n    return res;\n}\n\n// vector operator >>= : shift right arithmetic\nstatic inline Vec16c & operator >>= (Vec16c & a, int b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator == : returns true for elements for which a == b\nstatic inline Vec16cb operator == (Vec16c const & a, Vec16c const & b) {\n    return _mm_cmpeq_epi8(a,b);\n}\n\n// vector operator != : returns true for elements for which a != b\nstatic inline Vec16cb operator != (Vec16c const & a, Vec16c const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec16cb)_mm_comneq_epi8(a,b);\n#else  // SSE2 instruction set\n    return Vec16cb(Vec16c(~(a == b)));\n#endif\n}\n\n// vector operator > : returns true for elements for which a > b (signed)\nstatic inline Vec16cb operator > (Vec16c const & a, Vec16c const & b) {\n    return _mm_cmpgt_epi8(a,b);\n}\n\n// vector operator < : returns true for elements for which a < b (signed)\nstatic inline Vec16cb operator < (Vec16c const & a, Vec16c const & b) {\n    return b > a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (signed)\nstatic inline Vec16cb operator >= (Vec16c const & a, Vec16c const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec16cb)_mm_comge_epi8(a,b);\n#else  // SSE2 instruction set\n    return Vec16cb(Vec16c(~(b > a)));\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (signed)\nstatic inline Vec16cb operator <= (Vec16c const & a, Vec16c const & b) {\n    return b >= a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec16c operator & (Vec16c const & a, Vec16c const & b) {\n    return Vec16c(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec16c operator && (Vec16c const & a, Vec16c const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec16c & operator &= (Vec16c & a, Vec16c const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec16c operator | (Vec16c const & a, Vec16c const & b) {\n    return Vec16c(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec16c operator || (Vec16c const & a, Vec16c const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec16c & operator |= (Vec16c & a, Vec16c const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec16c operator ^ (Vec16c const & a, Vec16c const & b) {\n    return Vec16c(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec16c & operator ^= (Vec16c & a, Vec16c const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec16c operator ~ (Vec16c const & a) {\n    return Vec16c( ~ Vec128b(a));\n}\n\n// vector operator ! : logical not, returns true for elements == 0\nstatic inline Vec16cb operator ! (Vec16c const & a) {\n    return _mm_cmpeq_epi8(a,_mm_setzero_si128());\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];\n// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.\nstatic inline Vec16c select (Vec16cb const & s, Vec16c const & a, Vec16c const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec16c if_add (Vec16cb const & f, Vec16c const & a, Vec16c const & b) {\n    return a + (Vec16c(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline int32_t horizontal_add (Vec16c const & a) {\n    __m128i sum1 = _mm_sad_epu8(a,_mm_setzero_si128());\n    __m128i sum2 = _mm_shuffle_epi32(sum1,2);\n    __m128i sum3 = _mm_add_epi16(sum1,sum2);\n    int8_t  sum4 = (int8_t)_mm_cvtsi128_si32(sum3);        // truncate to 8 bits\n    return  sum4;                                          // sign extend to 32 bits\n}\n\n// Horizontal add extended: Calculates the sum of all vector elements.\n// Each element is sign-extended before addition to avoid overflow\nstatic inline int32_t horizontal_add_x (Vec16c const & a) {\n#ifdef __XOP__       // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epi8(a);\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // high element\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);              // sum\n    return          _mm_cvtsi128_si32(sum3);\n#elif  INSTRSET >= 4  // SSSE3\n    __m128i aeven = _mm_slli_epi16(a,8);                   // even numbered elements of a. get sign bit in position\n            aeven = _mm_srai_epi16(aeven,8);               // sign extend even numbered elements\n    __m128i aodd  = _mm_srai_epi16(a,8);                   // sign extend odd  numbered elements\n    __m128i sum1  = _mm_add_epi16(aeven,aodd);             // add even and odd elements\n    __m128i sum2  = _mm_hadd_epi16(sum1,sum1);             // horizontally add 8 elements in 3 steps\n    __m128i sum3  = _mm_hadd_epi16(sum2,sum2);\n    __m128i sum4  = _mm_hadd_epi16(sum3,sum3);\n    int16_t sum5  = (int16_t)_mm_cvtsi128_si32(sum4);      // 16 bit sum\n    return  sum5;                                          // sign extend to 32 bits\n#else                 // SSE2\n    __m128i aeven = _mm_slli_epi16(a,8);                   // even numbered elements of a. get sign bit in position\n            aeven = _mm_srai_epi16(aeven,8);               // sign extend even numbered elements\n    __m128i aodd  = _mm_srai_epi16(a,8);                   // sign extend odd  numbered elements\n    __m128i sum1  = _mm_add_epi16(aeven,aodd);             // add even and odd elements\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // 4 high elements\n    __m128i sum3  = _mm_add_epi16(sum1,sum2);              // 4 sums\n    __m128i sum4  = _mm_shuffle_epi32(sum3,0x01);          // 2 high elements\n    __m128i sum5  = _mm_add_epi16(sum3,sum4);              // 2 sums\n    __m128i sum6  = _mm_shufflelo_epi16(sum5,0x01);        // 1 high element\n    __m128i sum7  = _mm_add_epi16(sum5,sum6);              // 1 sum\n    int16_t sum8  = _mm_cvtsi128_si32(sum7);               // 16 bit sum\n    return  sum8;                                          // sign extend to 32 bits\n#endif\n}\n\n\n// function add_saturated: add element by element, signed with saturation\nstatic inline Vec16c add_saturated(Vec16c const & a, Vec16c const & b) {\n    return _mm_adds_epi8(a, b);\n}\n\n// function sub_saturated: subtract element by element, signed with saturation\nstatic inline Vec16c sub_saturated(Vec16c const & a, Vec16c const & b) {\n    return _mm_subs_epi8(a, b);\n}\n\n// function max: a > b ? a : b\nstatic inline Vec16c max(Vec16c const & a, Vec16c const & b) {\n#if INSTRSET >= 5   // SSE4.1\n    return _mm_max_epi8(a,b);\n#else  // SSE2\n    __m128i signbit = _mm_set1_epi32(0x80808080);\n    __m128i a1      = _mm_xor_si128(a,signbit);            // add 0x80\n    __m128i b1      = _mm_xor_si128(b,signbit);            // add 0x80\n    __m128i m1      = _mm_max_epu8(a1,b1);                 // unsigned max\n    return  _mm_xor_si128(m1,signbit);                     // sub 0x80\n#endif\n}\n\n// function min: a < b ? a : b\nstatic inline Vec16c min(Vec16c const & a, Vec16c const & b) {\n#if INSTRSET >= 5   // SSE4.1\n    return _mm_min_epi8(a,b);\n#else  // SSE2\n    __m128i signbit = _mm_set1_epi32(0x80808080);\n    __m128i a1      = _mm_xor_si128(a,signbit);            // add 0x80\n    __m128i b1      = _mm_xor_si128(b,signbit);            // add 0x80\n    __m128i m1      = _mm_min_epu8(a1,b1);                 // unsigned min\n    return  _mm_xor_si128(m1,signbit);                     // sub 0x80\n#endif\n}\n\n// function abs: a >= 0 ? a : -a\nstatic inline Vec16c abs(Vec16c const & a) {\n#if INSTRSET >= 4     // SSSE3 supported\n    return _mm_sign_epi8(a,a);\n#else                 // SSE2\n    __m128i nega = _mm_sub_epi8(_mm_setzero_si128(), a);\n    return _mm_min_epu8(a, nega);   // unsigned min (the negative value is bigger when compared as unsigned)\n#endif\n}\n\n// function abs_saturated: same as abs, saturate if overflow\nstatic inline Vec16c abs_saturated(Vec16c const & a) {\n    __m128i absa   = abs(a);                               // abs(a)\n    __m128i overfl = _mm_cmpgt_epi8(_mm_setzero_si128(),absa);// 0 > a\n    return           _mm_add_epi8(absa,overfl);            // subtract 1 if 0x80\n}\n\n// function rotate_left: rotate each element left by b bits \n// Use negative count to rotate right\nstatic inline Vec16c rotate_left(Vec16c const & a, int b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return _mm_rot_epi8(a,_mm_set1_epi8(b));\n#else  // SSE2 instruction set\n    __m128i bb        = _mm_cvtsi32_si128(b & 7);          // b modulo 8\n    __m128i mbb       = _mm_cvtsi32_si128((8-b) & 7);      // 8-b modulo 8\n    __m128i maskeven  = _mm_set1_epi32(0x00FF00FF);        // mask for even numbered bytes\n    __m128i even      = _mm_and_si128(a,maskeven);         // even numbered bytes of a\n    __m128i odd       = _mm_andnot_si128(maskeven,a);      // odd numbered bytes of a\n    __m128i evenleft  = _mm_sll_epi16(even,bb);            // even bytes of a << b\n    __m128i oddleft   = _mm_sll_epi16(odd,bb);             // odd  bytes of a << b\n    __m128i evenright = _mm_srl_epi16(even,mbb);           // even bytes of a >> 8-b\n    __m128i oddright  = _mm_srl_epi16(odd,mbb);            // odd  bytes of a >> 8-b\n    __m128i evenrot   = _mm_or_si128(evenleft,evenright);  // even bytes of a rotated\n    __m128i oddrot    = _mm_or_si128(oddleft,oddright);    // odd  bytes of a rotated\n    __m128i allrot    = selectb(maskeven,evenrot,oddrot);  // all  bytes rotated\n    return  allrot;\n#endif\n}\n\n\n/*****************************************************************************\n*\n*          Vector of 16 8-bit unsigned integers\n*\n*****************************************************************************/\n\nclass Vec16uc : public Vec16c {\npublic:\n    // Default constructor:\n    Vec16uc() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec16uc(uint32_t i) {\n        xmm = _mm_set1_epi8((char)i);\n    }\n    // Constructor to build from all elements:\n    Vec16uc(uint8_t i0, uint8_t i1, uint8_t i2, uint8_t i3, uint8_t i4, uint8_t i5, uint8_t i6, uint8_t i7,\n        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15) {\n        xmm = _mm_setr_epi8(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec16uc(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec16uc & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Member function to load from array (unaligned)\n    Vec16uc & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec16uc & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec16uc const & insert(uint32_t index, uint8_t value) {\n        Vec16c::insert(index, value);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    uint8_t extract(uint32_t index) const {\n        return Vec16c::extract(index);\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    uint8_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n// Define operators for this class\n\n// vector operator << : shift left all elements\nstatic inline Vec16uc operator << (Vec16uc const & a, uint32_t b) {\n    uint32_t mask = (uint32_t)0xFF >> (uint32_t)b;         // mask to remove bits that are shifted out\n    __m128i am    = _mm_and_si128(a,_mm_set1_epi8((char)mask));  // remove bits that will overflow\n    __m128i res   = _mm_sll_epi16(am,_mm_cvtsi32_si128(b));// 16-bit shifts\n    return res;\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec16uc operator << (Vec16uc const & a, int32_t b) {\n    return a << (uint32_t)b;\n}\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec16uc operator >> (Vec16uc const & a, uint32_t b) {\n    uint32_t mask = (uint32_t)0xFF << (uint32_t)b;         // mask to remove bits that are shifted out\n    __m128i am    = _mm_and_si128(a,_mm_set1_epi8((char)mask));  // remove bits that will overflow\n    __m128i res   = _mm_srl_epi16(am,_mm_cvtsi32_si128(b));// 16-bit shifts\n    return res;\n}\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec16uc operator >> (Vec16uc const & a, int32_t b) {\n    return a >> (uint32_t)b;\n}\n\n// vector operator >>= : shift right logical\nstatic inline Vec16uc & operator >>= (Vec16uc & a, int b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (unsigned)\nstatic inline Vec16cb operator >= (Vec16uc const & a, Vec16uc const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec16cb)_mm_comge_epu8(a,b);\n#else  // SSE2 instruction set\n    return (Vec16cb)_mm_cmpeq_epi8(_mm_max_epu8(a,b),a); // a == max(a,b)\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (unsigned)\nstatic inline Vec16cb operator <= (Vec16uc const & a, Vec16uc const & b) {\n    return b >= a;\n}\n\n// vector operator > : returns true for elements for which a > b (unsigned)\nstatic inline Vec16cb operator > (Vec16uc const & a, Vec16uc const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec16cb)_mm_comgt_epu8(a,b);\n#else  // SSE2 instruction set\n    return Vec16cb(Vec16c(~(b >= a)));\n#endif\n}\n\n// vector operator < : returns true for elements for which a < b (unsigned)\nstatic inline Vec16cb operator < (Vec16uc const & a, Vec16uc const & b) {\n    return b > a;\n}\n\n// vector operator + : add\nstatic inline Vec16uc operator + (Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc (Vec16c(a) + Vec16c(b));\n}\n\n// vector operator - : subtract\nstatic inline Vec16uc operator - (Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc (Vec16c(a) - Vec16c(b));\n}\n\n// vector operator * : multiply\nstatic inline Vec16uc operator * (Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc (Vec16c(a) * Vec16c(b));\n}\n\n// vector operator & : bitwise and\nstatic inline Vec16uc operator & (Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec16uc operator && (Vec16uc const & a, Vec16uc const & b) {\n    return a & b;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec16uc operator | (Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec16uc operator || (Vec16uc const & a, Vec16uc const & b) {\n    return a | b;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec16uc operator ^ (Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc(Vec128b(a) ^ Vec128b(b));\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec16uc operator ~ (Vec16uc const & a) {\n    return Vec16uc( ~ Vec128b(a));\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];\n// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec16uc select (Vec16cb const & s, Vec16uc const & a, Vec16uc const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec16uc if_add (Vec16cb const & f, Vec16uc const & a, Vec16uc const & b) {\n    return a + (Vec16uc(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\n// (Note: horizontal_add_x(Vec16uc) is slightly faster)\nstatic inline uint32_t horizontal_add (Vec16uc const & a) {\n    __m128i sum1 = _mm_sad_epu8(a,_mm_setzero_si128());\n    __m128i sum2 = _mm_shuffle_epi32(sum1,2);\n    __m128i sum3 = _mm_add_epi16(sum1,sum2);\n    uint16_t sum4 = (uint16_t)_mm_cvtsi128_si32(sum3);      // truncate to 16 bits\n    return  sum4;\n}\n\n// Horizontal add extended: Calculates the sum of all vector elements.\n// Each element is zero-extended before addition to avoid overflow\nstatic inline uint32_t horizontal_add_x (Vec16uc const & a) {\n    __m128i sum1 = _mm_sad_epu8(a,_mm_setzero_si128());\n    __m128i sum2 = _mm_shuffle_epi32(sum1,2);\n    __m128i sum3 = _mm_add_epi16(sum1,sum2);\n    return _mm_cvtsi128_si32(sum3);\n}\n\n// function add_saturated: add element by element, unsigned with saturation\nstatic inline Vec16uc add_saturated(Vec16uc const & a, Vec16uc const & b) {\n    return _mm_adds_epu8(a, b);\n}\n\n// function sub_saturated: subtract element by element, unsigned with saturation\nstatic inline Vec16uc sub_saturated(Vec16uc const & a, Vec16uc const & b) {\n    return _mm_subs_epu8(a, b);\n}\n\n// function max: a > b ? a : b\nstatic inline Vec16uc max(Vec16uc const & a, Vec16uc const & b) {\n    return _mm_max_epu8(a,b);\n}\n\n// function min: a < b ? a : b\nstatic inline Vec16uc min(Vec16uc const & a, Vec16uc const & b) {\n    return _mm_min_epu8(a,b);\n}\n\n\n    \n/*****************************************************************************\n*\n*          Vector of 8 16-bit signed integers\n*\n*****************************************************************************/\n\nclass Vec8s : public Vec128b {\npublic:\n    // Default constructor:\n    Vec8s() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec8s(int i) {\n        xmm = _mm_set1_epi16((int16_t)i);\n    }\n    // Constructor to build from all elements:\n    Vec8s(int16_t i0, int16_t i1, int16_t i2, int16_t i3, int16_t i4, int16_t i5, int16_t i6, int16_t i7) {\n        xmm = _mm_setr_epi16(i0, i1, i2, i3, i4, i5, i6, i7);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec8s(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec8s & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Type cast operator to convert to __m128i used in intrinsics\n    operator __m128i() const {\n        return xmm;\n    }\n    // Member function to load from array (unaligned)\n    Vec8s & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec8s & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Partial load. Load n elements and set the rest to 0\n    Vec8s & load_partial(int n, void const * p) {\n        if      (n >= 8) load(p);\n        else if (n <= 0)  *this = 0;\n        else if (((int)(intptr_t)p & 0xFFF) < 0xFF0) {\n            // p is at least 16 bytes from a page boundary. OK to read 16 bytes\n            load(p);\n        }\n        else {\n            // worst case. read 1 byte at a time and suffer store forwarding penalty\n            int16_t x[8];\n            for (int i = 0; i < n; i++) x[i] = ((int16_t const *)p)[i];\n            load(x);\n        }\n        cutoff(n);\n        return *this;\n    }\n    // Partial store. Store n elements\n    void store_partial(int n, void * p) const {\n        if (n >= 8) {\n            store(p);\n            return;\n        }\n        if (n <= 0) return;\n        // we are not using _mm_maskmoveu_si128 because it is too slow on many processors\n        union {        \n            int8_t  c[16];\n            int16_t s[8];\n            int32_t i[4];\n            int64_t q[2];\n        } u;\n        store(u.c);\n        int j = 0;\n        if (n & 4) {\n            *(int64_t*)p = u.q[0];\n            j += 8;\n        }\n        if (n & 2) {\n            ((int32_t*)p)[j/4] = u.i[j/4];\n            j += 4;\n        }\n        if (n & 1) {\n            ((int16_t*)p)[j/2] = u.s[j/2];\n        }\n    }\n    // cut off vector to n elements. The last 8-n elements are set to zero\n    Vec8s & cutoff(int n) {\n        *this = Vec16c(xmm).cutoff(n * 2);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec8s const & insert(uint32_t index, int16_t value) {\n        switch(index) {\n        case 0:\n            xmm = _mm_insert_epi16(xmm,value,0);  break;\n        case 1:\n            xmm = _mm_insert_epi16(xmm,value,1);  break;\n        case 2:\n            xmm = _mm_insert_epi16(xmm,value,2);  break;\n        case 3:\n            xmm = _mm_insert_epi16(xmm,value,3);  break;\n        case 4:\n            xmm = _mm_insert_epi16(xmm,value,4);  break;\n        case 5:\n            xmm = _mm_insert_epi16(xmm,value,5);  break;\n        case 6:\n            xmm = _mm_insert_epi16(xmm,value,6);  break;\n        case 7:\n            xmm = _mm_insert_epi16(xmm,value,7);  break;\n        }\n        return *this;\n    }\n    // Member function extract a single element from vector\n    // Note: This function is inefficient. Use store function if extracting more than one element\n    int16_t extract(uint32_t index) const {\n        switch(index) {\n        case 0:\n            return (int16_t)_mm_extract_epi16(xmm,0);\n        case 1:\n            return (int16_t)_mm_extract_epi16(xmm,1);\n        case 2:\n            return (int16_t)_mm_extract_epi16(xmm,2);\n        case 3:\n            return (int16_t)_mm_extract_epi16(xmm,3);\n        case 4:\n            return (int16_t)_mm_extract_epi16(xmm,4);\n        case 5:\n            return (int16_t)_mm_extract_epi16(xmm,5);\n        case 6:\n            return (int16_t)_mm_extract_epi16(xmm,6);\n        case 7:\n            return (int16_t)_mm_extract_epi16(xmm,7);\n        }\n        return 0;\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    int16_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n    static int size() {\n        return 8;\n    }\n};\n\n/*****************************************************************************\n*\n*          Vec8sb: Vector of 8 Booleans for use with Vec8s and Vec8us\n*\n*****************************************************************************/\n\nclass Vec8sb : public Vec8s {\npublic:\n    // Constructor to build from all elements:\n    Vec8sb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7) {\n        xmm = Vec8s(-int16_t(x0), -int16_t(x1), -int16_t(x2), -int16_t(x3), -int16_t(x4), -int16_t(x5), -int16_t(x6), -int16_t(x7));\n    }\n    // Default constructor:\n    Vec8sb() {\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec8sb(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec8sb & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Constructor to broadcast scalar value:\n    Vec8sb(bool b) : Vec8s(-int16_t(b)) {\n    }\n    // Assignment operator to broadcast scalar value:\n    Vec8sb & operator = (bool b) {\n        *this = Vec8sb(b);\n        return *this;\n    }\nprivate: // Prevent constructing from int, etc.\n    Vec8sb(int b);\n    Vec8sb & operator = (int x);\npublic:\n    Vec8sb & insert (int index, bool a) {\n        Vec8s::insert(index, -(int)a);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    // Note: This function is inefficient. Use store function if extracting more than one element\n    bool extract(uint32_t index) const {\n        return Vec8s::extract(index) != 0;\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    bool operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n\n/*****************************************************************************\n*\n*          Define operators for Vec8sb\n*\n*****************************************************************************/\n\n// vector operator & : bitwise and\nstatic inline Vec8sb operator & (Vec8sb const & a, Vec8sb const & b) {\n    return Vec8sb(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec8sb operator && (Vec8sb const & a, Vec8sb const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec8sb & operator &= (Vec8sb & a, Vec8sb const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec8sb operator | (Vec8sb const & a, Vec8sb const & b) {\n    return Vec8sb(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec8sb operator || (Vec8sb const & a, Vec8sb const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec8sb & operator |= (Vec8sb & a, Vec8sb const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec8sb operator ^ (Vec8sb const & a, Vec8sb const & b) {\n    return Vec8sb(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec8sb & operator ^= (Vec8sb & a, Vec8sb const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec8sb operator ~ (Vec8sb const & a) {\n    return Vec8sb( ~ Vec128b(a));\n}\n\n// vector operator ! : element not\nstatic inline Vec8sb operator ! (Vec8sb const & a) {\n    return ~ a;\n}\n\n// vector function andnot\nstatic inline Vec8sb andnot (Vec8sb const & a, Vec8sb const & b) {\n    return Vec8sb(andnot(Vec128b(a), Vec128b(b)));\n}\n\n// Horizontal Boolean functions for Vec8sb\n\n// horizontal_and. Returns true if all elements are true\nstatic inline bool horizontal_and(Vec8sb const & a) {\n    return _mm_movemask_epi8(a) == 0xFFFF;\n}\n\n// horizontal_or. Returns true if at least one element is true\nstatic inline bool horizontal_or(Vec8sb const & a) {\n#if INSTRSET >= 5   // SSE4.1 supported. Use PTEST\n    return !_mm_testz_si128(a, a);\n#else\n    return _mm_movemask_epi8(a) != 0;\n#endif\n}\n\n\n/*****************************************************************************\n*\n*         operators for Vec8s\n*\n*****************************************************************************/\n\n// vector operator + : add element by element\nstatic inline Vec8s operator + (Vec8s const & a, Vec8s const & b) {\n    return _mm_add_epi16(a, b);\n}\n\n// vector operator += : add\nstatic inline Vec8s & operator += (Vec8s & a, Vec8s const & b) {\n    a = a + b;\n    return a;\n}\n\n// postfix operator ++\nstatic inline Vec8s operator ++ (Vec8s & a, int) {\n    Vec8s a0 = a;\n    a = a + 1;\n    return a0;\n}\n\n// prefix operator ++\nstatic inline Vec8s & operator ++ (Vec8s & a) {\n    a = a + 1;\n    return a;\n}\n\n// vector operator - : subtract element by element\nstatic inline Vec8s operator - (Vec8s const & a, Vec8s const & b) {\n    return _mm_sub_epi16(a, b);\n}\n\n// vector operator - : unary minus\nstatic inline Vec8s operator - (Vec8s const & a) {\n    return _mm_sub_epi16(_mm_setzero_si128(), a);\n}\n\n// vector operator -= : subtract\nstatic inline Vec8s & operator -= (Vec8s & a, Vec8s const & b) {\n    a = a - b;\n    return a;\n}\n\n// postfix operator --\nstatic inline Vec8s operator -- (Vec8s & a, int) {\n    Vec8s a0 = a;\n    a = a - 1;\n    return a0;\n}\n\n// prefix operator --\nstatic inline Vec8s & operator -- (Vec8s & a) {\n    a = a - 1;\n    return a;\n}\n\n// vector operator * : multiply element by element\nstatic inline Vec8s operator * (Vec8s const & a, Vec8s const & b) {\n    return _mm_mullo_epi16(a, b);\n}\n\n// vector operator *= : multiply\nstatic inline Vec8s & operator *= (Vec8s & a, Vec8s const & b) {\n    a = a * b;\n    return a;\n}\n\n// vector operator / : divide all elements by same integer\n// See bottom of file\n\n\n// vector operator << : shift left\nstatic inline Vec8s operator << (Vec8s const & a, int b) {\n    return _mm_sll_epi16(a,_mm_cvtsi32_si128(b));\n}\n\n// vector operator <<= : shift left\nstatic inline Vec8s & operator <<= (Vec8s & a, int b) {\n    a = a << b;\n    return a;\n}\n\n// vector operator >> : shift right arithmetic\nstatic inline Vec8s operator >> (Vec8s const & a, int b) {\n    return _mm_sra_epi16(a,_mm_cvtsi32_si128(b));\n}\n\n// vector operator >>= : shift right arithmetic\nstatic inline Vec8s & operator >>= (Vec8s & a, int b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator == : returns true for elements for which a == b\nstatic inline Vec8sb operator == (Vec8s const & a, Vec8s const & b) {\n    return _mm_cmpeq_epi16(a, b);\n}\n\n// vector operator != : returns true for elements for which a != b\nstatic inline Vec8sb operator != (Vec8s const & a, Vec8s const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec8sb)_mm_comneq_epi16(a,b);\n#else  // SSE2 instruction set\n    return Vec8sb (~(a == b));\n#endif\n}\n\n// vector operator > : returns true for elements for which a > b\nstatic inline Vec8sb operator > (Vec8s const & a, Vec8s const & b) {\n    return _mm_cmpgt_epi16(a, b);\n}\n\n// vector operator < : returns true for elements for which a < b\nstatic inline Vec8sb operator < (Vec8s const & a, Vec8s const & b) {\n    return b > a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (signed)\nstatic inline Vec8sb operator >= (Vec8s const & a, Vec8s const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec8sb)_mm_comge_epi16(a,b);\n#else  // SSE2 instruction set\n    return Vec8sb (~(b > a));\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (signed)\nstatic inline Vec8sb operator <= (Vec8s const & a, Vec8s const & b) {\n    return b >= a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec8s operator & (Vec8s const & a, Vec8s const & b) {\n    return Vec8s(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec8s operator && (Vec8s const & a, Vec8s const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec8s & operator &= (Vec8s & a, Vec8s const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec8s operator | (Vec8s const & a, Vec8s const & b) {\n    return Vec8s(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec8s operator || (Vec8s const & a, Vec8s const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec8s & operator |= (Vec8s & a, Vec8s const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec8s operator ^ (Vec8s const & a, Vec8s const & b) {\n    return Vec8s(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec8s & operator ^= (Vec8s & a, Vec8s const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec8s operator ~ (Vec8s const & a) {\n    return Vec8s( ~ Vec128b(a));\n}\n\n// vector operator ! : logical not, returns true for elements == 0\nstatic inline Vec8s operator ! (Vec8s const & a) {\n    return _mm_cmpeq_epi16(a,_mm_setzero_si128());\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];\n// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec8s select (Vec8sb const & s, Vec8s const & a, Vec8s const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec8s if_add (Vec8sb const & f, Vec8s const & a, Vec8s const & b) {\n    return a + (Vec8s(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline int32_t horizontal_add (Vec8s const & a) {\n#ifdef __XOP__       // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epi16(a);\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // high element\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);              // sum\n    int16_t sum4  = _mm_cvtsi128_si32(sum3);               // truncate to 16 bits\n    return  sum4;                                          // sign extend to 32 bits\n#elif  INSTRSET >= 4  // SSSE3\n    __m128i sum1  = _mm_hadd_epi16(a,a);                   // horizontally add 8 elements in 3 steps\n    __m128i sum2  = _mm_hadd_epi16(sum1,sum1);\n    __m128i sum3  = _mm_hadd_epi16(sum2,sum2);\n    int16_t sum4  = (int16_t)_mm_cvtsi128_si32(sum3);      // 16 bit sum\n    return  sum4;                                          // sign extend to 32 bits\n#else                 // SSE2\n    __m128i sum1  = _mm_shuffle_epi32(a,0x0E);             // 4 high elements\n    __m128i sum2  = _mm_add_epi16(a,sum1);                 // 4 sums\n    __m128i sum3  = _mm_shuffle_epi32(sum2,0x01);          // 2 high elements\n    __m128i sum4  = _mm_add_epi16(sum2,sum3);              // 2 sums\n    __m128i sum5  = _mm_shufflelo_epi16(sum4,0x01);        // 1 high element\n    __m128i sum6  = _mm_add_epi16(sum4,sum5);              // 1 sum\n    int16_t sum7  = _mm_cvtsi128_si32(sum6);               // 16 bit sum\n    return  sum7;                                          // sign extend to 32 bits\n#endif\n}\n\n// Horizontal add extended: Calculates the sum of all vector elements.\n// Elements are sign extended before adding to avoid overflow\nstatic inline int32_t horizontal_add_x (Vec8s const & a) {\n#ifdef __XOP__       // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epi16(a);\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // high element\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);              // sum\n    return          _mm_cvtsi128_si32(sum3);\n#elif  INSTRSET >= 4  // SSSE3\n    __m128i aeven = _mm_slli_epi32(a,16);                  // even numbered elements of a. get sign bit in position\n            aeven = _mm_srai_epi32(aeven,16);              // sign extend even numbered elements\n    __m128i aodd  = _mm_srai_epi32(a,16);                  // sign extend odd  numbered elements\n    __m128i sum1  = _mm_add_epi32(aeven,aodd);             // add even and odd elements\n    __m128i sum2  = _mm_hadd_epi32(sum1,sum1);             // horizontally add 4 elements in 2 steps\n    __m128i sum3  = _mm_hadd_epi32(sum2,sum2);\n    return  _mm_cvtsi128_si32(sum3);\n#else                 // SSE2\n    __m128i aeven = _mm_slli_epi32(a,16);                  // even numbered elements of a. get sign bit in position\n            aeven = _mm_srai_epi32(aeven,16);              // sign extend even numbered elements\n    __m128i aodd  = _mm_srai_epi32(a,16);                  // sign extend odd  numbered elements\n    __m128i sum1  = _mm_add_epi32(aeven,aodd);             // add even and odd elements\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // 2 high elements\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);\n    __m128i sum4  = _mm_shuffle_epi32(sum3,0x01);          // 1 high elements\n    __m128i sum5  = _mm_add_epi32(sum3,sum4);\n    return  _mm_cvtsi128_si32(sum5);                       // 32 bit sum\n#endif\n}\n\n// function add_saturated: add element by element, signed with saturation\nstatic inline Vec8s add_saturated(Vec8s const & a, Vec8s const & b) {\n    return _mm_adds_epi16(a, b);\n}\n\n// function sub_saturated: subtract element by element, signed with saturation\nstatic inline Vec8s sub_saturated(Vec8s const & a, Vec8s const & b) {\n    return _mm_subs_epi16(a, b);\n}\n\n// function max: a > b ? a : b\nstatic inline Vec8s max(Vec8s const & a, Vec8s const & b) {\n    return _mm_max_epi16(a,b);\n}\n\n// function min: a < b ? a : b\nstatic inline Vec8s min(Vec8s const & a, Vec8s const & b) {\n    return _mm_min_epi16(a,b);\n}\n\n// function abs: a >= 0 ? a : -a\nstatic inline Vec8s abs(Vec8s const & a) {\n#if INSTRSET >= 4     // SSSE3 supported\n    return _mm_sign_epi16(a,a);\n#else                 // SSE2\n    __m128i nega = _mm_sub_epi16(_mm_setzero_si128(), a);\n    return _mm_max_epi16(a, nega);\n#endif\n}\n\n// function abs_saturated: same as abs, saturate if overflow\nstatic inline Vec8s abs_saturated(Vec8s const & a) {\n    __m128i absa   = abs(a);                               // abs(a)\n    __m128i overfl = _mm_srai_epi16(absa,15);              // sign\n    return           _mm_add_epi16(absa,overfl);           // subtract 1 if 0x8000\n}\n\n// function rotate_left all elements\n// Use negative count to rotate right\nstatic inline Vec8s rotate_left(Vec8s const & a, int b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return _mm_rot_epi16(a,_mm_set1_epi16(b));\n#else  // SSE2 instruction set\n    __m128i left  = _mm_sll_epi16(a,_mm_cvtsi32_si128(b & 0x0F));      // a << b \n    __m128i right = _mm_srl_epi16(a,_mm_cvtsi32_si128((16-b) & 0x0F)); // a >> (16 - b)\n    __m128i rot   = _mm_or_si128(left,right);                          // or\n    return  rot;\n#endif\n}\n\n\n/*****************************************************************************\n*\n*          Vector of 8 16-bit unsigned integers\n*\n*****************************************************************************/\n\nclass Vec8us : public Vec8s {\npublic:\n    // Default constructor:\n    Vec8us() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec8us(uint32_t i) {\n        xmm = _mm_set1_epi16((int16_t)i);\n    }\n    // Constructor to build from all elements:\n    Vec8us(uint16_t i0, uint16_t i1, uint16_t i2, uint16_t i3, uint16_t i4, uint16_t i5, uint16_t i6, uint16_t i7) {\n        xmm = _mm_setr_epi16(i0, i1, i2, i3, i4, i5, i6, i7);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec8us(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec8us & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Member function to load from array (unaligned)\n    Vec8us & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec8us & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec8us const & insert(uint32_t index, uint16_t value) {\n        Vec8s::insert(index, value);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    uint16_t extract(uint32_t index) const {\n        return Vec8s::extract(index);\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    uint16_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n// Define operators for this class\n\n// vector operator + : add\nstatic inline Vec8us operator + (Vec8us const & a, Vec8us const & b) {\n    return Vec8us (Vec8s(a) + Vec8s(b));\n}\n\n// vector operator - : subtract\nstatic inline Vec8us operator - (Vec8us const & a, Vec8us const & b) {\n    return Vec8us (Vec8s(a) - Vec8s(b));\n}\n\n// vector operator * : multiply\nstatic inline Vec8us operator * (Vec8us const & a, Vec8us const & b) {\n    return Vec8us (Vec8s(a) * Vec8s(b));\n}\n\n// vector operator / : divide\n// See bottom of file\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec8us operator >> (Vec8us const & a, uint32_t b) {\n    return _mm_srl_epi16(a,_mm_cvtsi32_si128(b)); \n}\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec8us operator >> (Vec8us const & a, int32_t b) {\n    return a >> (uint32_t)b;\n}\n\n// vector operator >>= : shift right logical\nstatic inline Vec8us & operator >>= (Vec8us & a, int b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec8us operator << (Vec8us const & a, uint32_t b) {\n    return _mm_sll_epi16(a,_mm_cvtsi32_si128(b)); \n}\n\n// vector operator << : shift left all elements\nstatic inline Vec8us operator << (Vec8us const & a, int32_t b) {\n    return a << (uint32_t)b;\n}\n\n// vector operator >= : returns true for elements for which a >= b (unsigned)\nstatic inline Vec8s operator >= (Vec8us const & a, Vec8us const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return _mm_comge_epu16(a,b);\n#elif INSTRSET >= 5   // SSE4.1\n    __m128i max_ab = _mm_max_epu16(a,b);                   // max(a,b), unsigned\n    return _mm_cmpeq_epi16(a,max_ab);                      // a == max(a,b)\n#else  // SSE2 instruction set\n    __m128i s = _mm_subs_epu16(b,a);                       // b-a, saturated\n    return  _mm_cmpeq_epi16(s, _mm_setzero_si128());       // s == 0 \n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (unsigned)\nstatic inline Vec8s operator <= (Vec8us const & a, Vec8us const & b) {\n    return b >= a;\n}\n\n// vector operator > : returns true for elements for which a > b (unsigned)\nstatic inline Vec8s operator > (Vec8us const & a, Vec8us const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec8s)_mm_comgt_epu16(a,b);\n#else  // SSE2 instruction set\n    return Vec8s (~(b >= a));\n#endif\n}\n\n// vector operator < : returns true for elements for which a < b (unsigned)\nstatic inline Vec8s operator < (Vec8us const & a, Vec8us const & b) {\n    return b > a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec8us operator & (Vec8us const & a, Vec8us const & b) {\n    return Vec8us(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec8us operator && (Vec8us const & a, Vec8us const & b) {\n    return a & b;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec8us operator | (Vec8us const & a, Vec8us const & b) {\n    return Vec8us(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec8us operator || (Vec8us const & a, Vec8us const & b) {\n    return a | b;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec8us operator ^ (Vec8us const & a, Vec8us const & b) {\n    return Vec8us(Vec128b(a) ^ Vec128b(b));\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec8us operator ~ (Vec8us const & a) {\n    return Vec8us( ~ Vec128b(a));\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];\n// Each word in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec8us select (Vec8sb const & s, Vec8us const & a, Vec8us const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec8us if_add (Vec8sb const & f, Vec8us const & a, Vec8us const & b) {\n    return a + (Vec8us(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline uint32_t horizontal_add (Vec8us const & a) {\n#ifdef __XOP__     // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epu16(a);\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // high element\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);              // sum\n    uint16_t sum4 = _mm_cvtsi128_si32(sum3);               // truncate to 16 bits\n    return  sum4;                                          // zero extend to 32 bits\n#elif  INSTRSET >= 4  // SSSE3\n    __m128i sum1  = _mm_hadd_epi16(a,a);                   // horizontally add 8 elements in 3 steps\n    __m128i sum2  = _mm_hadd_epi16(sum1,sum1);\n    __m128i sum3  = _mm_hadd_epi16(sum2,sum2);\n    uint16_t sum4 = (uint16_t)_mm_cvtsi128_si32(sum3);     // 16 bit sum\n    return  sum4;                                          // zero extend to 32 bits\n#else                 // SSE2\n    __m128i sum1  = _mm_shuffle_epi32(a,0x0E);             // 4 high elements\n    __m128i sum2  = _mm_add_epi16(a,sum1);                 // 4 sums\n    __m128i sum3  = _mm_shuffle_epi32(sum2,0x01);          // 2 high elements\n    __m128i sum4  = _mm_add_epi16(sum2,sum3);              // 2 sums\n    __m128i sum5  = _mm_shufflelo_epi16(sum4,0x01);        // 1 high element\n    __m128i sum6  = _mm_add_epi16(sum4,sum5);              // 1 sum\n    uint16_t sum7 = _mm_cvtsi128_si32(sum6);               // 16 bit sum\n    return  sum7;                                          // zero extend to 32 bits\n#endif\n}\n\n// Horizontal add extended: Calculates the sum of all vector elements.\n// Each element is zero-extended before addition to avoid overflow\nstatic inline uint32_t horizontal_add_x (Vec8us const & a) {\n#ifdef __XOP__     // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epu16(a);\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // high element\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);              // sum\n    return          _mm_cvtsi128_si32(sum3);\n#elif INSTRSET >= 4  // SSSE3\n    __m128i mask  = _mm_set1_epi32(0x0000FFFF);            // mask for even positions\n    __m128i aeven = _mm_and_si128(a,mask);                 // even numbered elements of a\n    __m128i aodd  = _mm_srli_epi32(a,16);                  // zero extend odd numbered elements\n    __m128i sum1  = _mm_add_epi32(aeven,aodd);             // add even and odd elements\n    __m128i sum2  = _mm_hadd_epi32(sum1,sum1);             // horizontally add 4 elements in 2 steps\n    __m128i sum3  = _mm_hadd_epi32(sum2,sum2);\n    return  _mm_cvtsi128_si32(sum3);\n#else                 // SSE2\n    __m128i mask  = _mm_set1_epi32(0x0000FFFF);            // mask for even positions\n    __m128i aeven = _mm_and_si128(a,mask);                 // even numbered elements of a\n    __m128i aodd  = _mm_srli_epi32(a,16);                  // zero extend odd numbered elements\n    __m128i sum1  = _mm_add_epi32(aeven,aodd);             // add even and odd elements\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // 2 high elements\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);\n    __m128i sum4  = _mm_shuffle_epi32(sum3,0x01);          // 1 high elements\n    __m128i sum5  = _mm_add_epi32(sum3,sum4);\n    return  _mm_cvtsi128_si32(sum5);               // 16 bit sum\n#endif\n}\n\n// function add_saturated: add element by element, unsigned with saturation\nstatic inline Vec8us add_saturated(Vec8us const & a, Vec8us const & b) {\n    return _mm_adds_epu16(a, b);\n}\n\n// function sub_saturated: subtract element by element, unsigned with saturation\nstatic inline Vec8us sub_saturated(Vec8us const & a, Vec8us const & b) {\n    return _mm_subs_epu16(a, b);\n}\n\n// function max: a > b ? a : b\nstatic inline Vec8us max(Vec8us const & a, Vec8us const & b) {\n#if INSTRSET >= 5   // SSE4.1\n    return _mm_max_epu16(a,b);\n#else  // SSE2\n    __m128i signbit = _mm_set1_epi32(0x80008000);\n    __m128i a1      = _mm_xor_si128(a,signbit);            // add 0x8000\n    __m128i b1      = _mm_xor_si128(b,signbit);            // add 0x8000\n    __m128i m1      = _mm_max_epi16(a1,b1);                // signed max\n    return  _mm_xor_si128(m1,signbit);                     // sub 0x8000\n#endif\n}\n\n// function min: a < b ? a : b\nstatic inline Vec8us min(Vec8us const & a, Vec8us const & b) {\n#if INSTRSET >= 5   // SSE4.1\n    return _mm_min_epu16(a,b);\n#else  // SSE2\n    __m128i signbit = _mm_set1_epi32(0x80008000);\n    __m128i a1      = _mm_xor_si128(a,signbit);            // add 0x8000\n    __m128i b1      = _mm_xor_si128(b,signbit);            // add 0x8000\n    __m128i m1      = _mm_min_epi16(a1,b1);                // signed min\n    return  _mm_xor_si128(m1,signbit);                     // sub 0x8000\n#endif\n}\n\n\n\n/*****************************************************************************\n*\n*          Vector of 4 32-bit signed integers\n*\n*****************************************************************************/\n\nclass Vec4i : public Vec128b {\npublic:\n    // Default constructor:\n    Vec4i() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec4i(int i) {\n        xmm = _mm_set1_epi32(i);\n    }\n    // Constructor to build from all elements:\n    Vec4i(int32_t i0, int32_t i1, int32_t i2, int32_t i3) {\n        xmm = _mm_setr_epi32(i0, i1, i2, i3);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec4i(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec4i & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Type cast operator to convert to __m128i used in intrinsics\n    operator __m128i() const {\n        return xmm;\n    }\n    // Member function to load from array (unaligned)\n    Vec4i & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec4i & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Partial load. Load n elements and set the rest to 0\n    Vec4i & load_partial(int n, void const * p) {\n        switch (n) {\n        case 0:\n            *this = 0;  break;\n        case 1:\n            xmm = _mm_cvtsi32_si128(*(int32_t const*)p);  break;\n        case 2:\n            // intrinsic for movq is missing!\n            xmm = _mm_setr_epi32(((int32_t const*)p)[0], ((int32_t const*)p)[1], 0, 0);  break;\n        case 3:\n            xmm = _mm_setr_epi32(((int32_t const*)p)[0], ((int32_t const*)p)[1], ((int32_t const*)p)[2], 0);  break;\n        case 4:\n            load(p);  break;\n        default: \n            break;\n        }\n        return *this;\n    }\n    // Partial store. Store n elements\n    void store_partial(int n, void * p) const {\n        union {        \n            int32_t i[4];\n            int64_t q[2];\n        } u;\n        switch (n) {\n        case 1:\n            *(int32_t*)p = _mm_cvtsi128_si32(xmm);  break;\n        case 2:\n            // intrinsic for movq is missing!\n            store(u.i);\n            *(int64_t*)p = u.q[0];  break;\n        case 3:\n            store(u.i);\n            *(int64_t*)p     = u.q[0];  \n            ((int32_t*)p)[2] = u.i[2];  break;\n        case 4:\n            store(p);  break;\n        default:\n            break;\n        }\n    }\n    // cut off vector to n elements. The last 4-n elements are set to zero\n    Vec4i & cutoff(int n) {\n        *this = Vec16c(xmm).cutoff(n * 4);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec4i const & insert(uint32_t index, int32_t value) {\n        static const int32_t maskl[8] = {0,0,0,0,-1,0,0,0};\n        __m128i broad = _mm_set1_epi32(value);  // broadcast value into all elements\n        __m128i mask  = _mm_loadu_si128((__m128i const*)(maskl+4-(index & 3))); // mask with FFFFFFFF at index position\n        xmm = selectb(mask,broad,xmm);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    int32_t extract(uint32_t index) const {\n        int32_t x[4];\n        store(x);\n        return x[index & 3];\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    int32_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n    static int size() {\n        return 4;\n    }\n};\n\n\n/*****************************************************************************\n*\n*          Vec4ib: Vector of 4 Booleans for use with Vec4i and Vec4ui\n*\n*****************************************************************************/\nclass Vec4ib : public Vec4i {\npublic:\n    // Default constructor:\n    Vec4ib() {\n    }\n    // Constructor to build from all elements:\n    Vec4ib(bool x0, bool x1, bool x2, bool x3) {\n        xmm = Vec4i(-int32_t(x0), -int32_t(x1), -int32_t(x2), -int32_t(x3));\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec4ib(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec4ib & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Constructor to broadcast scalar value:\n    Vec4ib(bool b) : Vec4i(-int32_t(b)) {\n    }\n    // Assignment operator to broadcast scalar value:\n    Vec4ib & operator = (bool b) {\n        *this = Vec4ib(b);\n        return *this;\n    }\nprivate: // Prevent constructing from int, etc.\n    Vec4ib(int b);\n    Vec4ib & operator = (int x);\npublic:\n    Vec4ib & insert (int index, bool a) {\n        Vec4i::insert(index, -(int)a);\n        return *this;\n    }    \n    // Member function extract a single element from vector\n    bool extract(uint32_t index) const {\n        return Vec4i::extract(index) != 0;\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    bool operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n\n/*****************************************************************************\n*\n*          Define operators for Vec4ib\n*\n*****************************************************************************/\n\n// vector operator & : bitwise and\nstatic inline Vec4ib operator & (Vec4ib const & a, Vec4ib const & b) {\n    return Vec4ib(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec4ib operator && (Vec4ib const & a, Vec4ib const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec4ib & operator &= (Vec4ib & a, Vec4ib const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec4ib operator | (Vec4ib const & a, Vec4ib const & b) {\n    return Vec4ib(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec4ib operator || (Vec4ib const & a, Vec4ib const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec4ib & operator |= (Vec4ib & a, Vec4ib const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec4ib operator ^ (Vec4ib const & a, Vec4ib const & b) {\n    return Vec4ib(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec4ib & operator ^= (Vec4ib & a, Vec4ib const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec4ib operator ~ (Vec4ib const & a) {\n    return Vec4ib( ~ Vec128b(a));\n}\n\n// vector operator ! : element not\nstatic inline Vec4ib operator ! (Vec4ib const & a) {\n    return ~ a;\n}\n\n// vector function andnot\nstatic inline Vec4ib andnot (Vec4ib const & a, Vec4ib const & b) {\n    return Vec4ib(andnot(Vec128b(a), Vec128b(b)));\n}\n\n// Horizontal Boolean functions for Vec4ib\n\n// horizontal_and. Returns true if all elements are true\nstatic inline bool horizontal_and(Vec4ib const & a) {\n    return _mm_movemask_epi8(a) == 0xFFFF;\n}\n\n// horizontal_or. Returns true if at least one element is true\nstatic inline bool horizontal_or(Vec4ib const & a) {\n#if INSTRSET >= 5   // SSE4.1 supported. Use PTEST\n    return !_mm_testz_si128(a, a);\n#else\n    return _mm_movemask_epi8(a) != 0;\n#endif\n}\n\n\n/*****************************************************************************\n*\n*          Operators for Vec4i\n*\n*****************************************************************************/\n\n// vector operator + : add element by element\nstatic inline Vec4i operator + (Vec4i const & a, Vec4i const & b) {\n    return _mm_add_epi32(a, b);\n}\n\n// vector operator += : add\nstatic inline Vec4i & operator += (Vec4i & a, Vec4i const & b) {\n    a = a + b;\n    return a;\n}\n\n// postfix operator ++\nstatic inline Vec4i operator ++ (Vec4i & a, int) {\n    Vec4i a0 = a;\n    a = a + 1;\n    return a0;\n}\n\n// prefix operator ++\nstatic inline Vec4i & operator ++ (Vec4i & a) {\n    a = a + 1;\n    return a;\n}\n\n// vector operator - : subtract element by element\nstatic inline Vec4i operator - (Vec4i const & a, Vec4i const & b) {\n    return _mm_sub_epi32(a, b);\n}\n\n// vector operator - : unary minus\nstatic inline Vec4i operator - (Vec4i const & a) {\n    return _mm_sub_epi32(_mm_setzero_si128(), a);\n}\n\n// vector operator -= : subtract\nstatic inline Vec4i & operator -= (Vec4i & a, Vec4i const & b) {\n    a = a - b;\n    return a;\n}\n\n// postfix operator --\nstatic inline Vec4i operator -- (Vec4i & a, int) {\n    Vec4i a0 = a;\n    a = a - 1;\n    return a0;\n}\n\n// prefix operator --\nstatic inline Vec4i & operator -- (Vec4i & a) {\n    a = a - 1;\n    return a;\n}\n\n// vector operator * : multiply element by element\nstatic inline Vec4i operator * (Vec4i const & a, Vec4i const & b) {\n#if INSTRSET >= 5  // SSE4.1 instruction set\n    return _mm_mullo_epi32(a, b);\n#else\n   __m128i a13    = _mm_shuffle_epi32(a, 0xF5);          // (-,a3,-,a1)\n   __m128i b13    = _mm_shuffle_epi32(b, 0xF5);          // (-,b3,-,b1)\n   __m128i prod02 = _mm_mul_epu32(a, b);                 // (-,a2*b2,-,a0*b0)\n   __m128i prod13 = _mm_mul_epu32(a13, b13);             // (-,a3*b3,-,a1*b1)\n   __m128i prod01 = _mm_unpacklo_epi32(prod02,prod13);   // (-,-,a1*b1,a0*b0) \n   __m128i prod23 = _mm_unpackhi_epi32(prod02,prod13);   // (-,-,a3*b3,a2*b2) \n   return           _mm_unpacklo_epi64(prod01,prod23);   // (ab3,ab2,ab1,ab0)\n#endif\n}\n\n// vector operator *= : multiply\nstatic inline Vec4i & operator *= (Vec4i & a, Vec4i const & b) {\n    a = a * b;\n    return a;\n}\n\n// vector operator / : divide all elements by same integer\n// See bottom of file\n\n\n// vector operator << : shift left\nstatic inline Vec4i operator << (Vec4i const & a, int32_t b) {\n    return _mm_sll_epi32(a,_mm_cvtsi32_si128(b));\n}\n\n// vector operator <<= : shift left\nstatic inline Vec4i & operator <<= (Vec4i & a, int32_t b) {\n    a = a << b;\n    return a;\n}\n\n// vector operator >> : shift right arithmetic\nstatic inline Vec4i operator >> (Vec4i const & a, int32_t b) {\n    return _mm_sra_epi32(a,_mm_cvtsi32_si128(b));\n}\n\n// vector operator >>= : shift right arithmetic\nstatic inline Vec4i & operator >>= (Vec4i & a, int32_t b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator == : returns true for elements for which a == b\nstatic inline Vec4ib operator == (Vec4i const & a, Vec4i const & b) {\n    return _mm_cmpeq_epi32(a, b);\n}\n\n// vector operator != : returns true for elements for which a != b\nstatic inline Vec4ib operator != (Vec4i const & a, Vec4i const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec4ib)_mm_comneq_epi32(a,b);\n#else  // SSE2 instruction set\n    return Vec4ib(Vec4i (~(a == b)));\n#endif\n}\n  \n// vector operator > : returns true for elements for which a > b\nstatic inline Vec4ib operator > (Vec4i const & a, Vec4i const & b) {\n    return _mm_cmpgt_epi32(a, b);\n}\n\n// vector operator < : returns true for elements for which a < b\nstatic inline Vec4ib operator < (Vec4i const & a, Vec4i const & b) {\n    return b > a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (signed)\nstatic inline Vec4ib operator >= (Vec4i const & a, Vec4i const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec4ib)_mm_comge_epi32(a,b);\n#else  // SSE2 instruction set\n    return Vec4ib(Vec4i (~(b > a)));\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (signed)\nstatic inline Vec4ib operator <= (Vec4i const & a, Vec4i const & b) {\n    return b >= a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec4i operator & (Vec4i const & a, Vec4i const & b) {\n    return Vec4i(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec4i operator && (Vec4i const & a, Vec4i const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec4i & operator &= (Vec4i & a, Vec4i const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec4i operator | (Vec4i const & a, Vec4i const & b) {\n    return Vec4i(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec4i operator || (Vec4i const & a, Vec4i const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise and\nstatic inline Vec4i & operator |= (Vec4i & a, Vec4i const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec4i operator ^ (Vec4i const & a, Vec4i const & b) {\n    return Vec4i(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise and\nstatic inline Vec4i & operator ^= (Vec4i & a, Vec4i const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec4i operator ~ (Vec4i const & a) {\n    return Vec4i( ~ Vec128b(a));\n}\n\n// vector operator ! : returns true for elements == 0\nstatic inline Vec4ib operator ! (Vec4i const & a) {\n    return _mm_cmpeq_epi32(a,_mm_setzero_si128());\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];\n// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec4i select (Vec4ib const & s, Vec4i const & a, Vec4i const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec4i if_add (Vec4ib const & f, Vec4i const & a, Vec4i const & b) {\n    return a + (Vec4i(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline int32_t horizontal_add (Vec4i const & a) {\n#ifdef __XOP__       // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epi32(a);\n    __m128i sum2  = _mm_shuffle_epi32(sum1,0x0E);          // high element\n    __m128i sum3  = _mm_add_epi32(sum1,sum2);              // sum\n    return          _mm_cvtsi128_si32(sum3);               // truncate to 32 bits\n#elif  INSTRSET >= 4  // SSSE3\n    __m128i sum1  = _mm_hadd_epi32(a,a);                   // horizontally add 4 elements in 2 steps\n    __m128i sum2  = _mm_hadd_epi32(sum1,sum1);\n    return          _mm_cvtsi128_si32(sum2);               // 32 bit sum\n#else                 // SSE2\n    __m128i sum1  = _mm_shuffle_epi32(a,0x0E);             // 2 high elements\n    __m128i sum2  = _mm_add_epi32(a,sum1);                 // 2 sums\n    __m128i sum3  = _mm_shuffle_epi32(sum2,0x01);          // 1 high element\n    __m128i sum4  = _mm_add_epi32(sum2,sum3);              // 2 sums\n    return          _mm_cvtsi128_si32(sum4);               // 32 bit sum\n#endif\n}\n\n// Horizontal add extended: Calculates the sum of all vector elements.\n// Elements are sign extended before adding to avoid overflow\nstatic inline int64_t horizontal_add_x (Vec4i const & a) {\n#ifdef __XOP__     // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epi32(a);\n#else              // SSE2\n    __m128i signs = _mm_srai_epi32(a,31);                  // sign of all elements\n    __m128i a01   = _mm_unpacklo_epi32(a,signs);           // sign-extended a0, a1\n    __m128i a23   = _mm_unpackhi_epi32(a,signs);           // sign-extended a2, a3\n    __m128i sum1  = _mm_add_epi64(a01,a23);                // add\n#endif\n    __m128i sum2  = _mm_unpackhi_epi64(sum1,sum1);         // high qword\n    __m128i sum3  = _mm_add_epi64(sum1,sum2);              // add\n#if defined (__x86_64__)\n    return          _mm_cvtsi128_si64(sum3);               // 64 bit mode\n#else\n    union {\n        __m128i x;  // silly definition of _mm_storel_epi64 requires __m128i\n        int64_t i;\n    } u;\n    _mm_storel_epi64(&u.x,sum3);\n    return u.i;\n#endif\n}\n\n// function add_saturated: add element by element, signed with saturation\nstatic inline Vec4i add_saturated(Vec4i const & a, Vec4i const & b) {\n    __m128i sum    = _mm_add_epi32(a, b);                  // a + b\n    __m128i axb    = _mm_xor_si128(a, b);                  // check if a and b have different sign\n    __m128i axs    = _mm_xor_si128(a, sum);                // check if a and sum have different sign\n    __m128i overf1 = _mm_andnot_si128(axb,axs);            // check if sum has wrong sign\n    __m128i overf2 = _mm_srai_epi32(overf1,31);            // -1 if overflow\n    __m128i asign  = _mm_srli_epi32(a,31);                 // 1  if a < 0\n    __m128i sat1   = _mm_srli_epi32(overf2,1);             // 7FFFFFFF if overflow\n    __m128i sat2   = _mm_add_epi32(sat1,asign);            // 7FFFFFFF if positive overflow 80000000 if negative overflow\n    return  selectb(overf2,sat2,sum);                      // sum if not overflow, else sat2\n}\n\n// function sub_saturated: subtract element by element, signed with saturation\nstatic inline Vec4i sub_saturated(Vec4i const & a, Vec4i const & b) {\n    __m128i diff   = _mm_sub_epi32(a, b);                  // a + b\n    __m128i axb    = _mm_xor_si128(a, b);                  // check if a and b have different sign\n    __m128i axs    = _mm_xor_si128(a, diff);               // check if a and sum have different sign\n    __m128i overf1 = _mm_and_si128(axb,axs);               // check if sum has wrong sign\n    __m128i overf2 = _mm_srai_epi32(overf1,31);            // -1 if overflow\n    __m128i asign  = _mm_srli_epi32(a,31);                 // 1  if a < 0\n    __m128i sat1   = _mm_srli_epi32(overf2,1);             // 7FFFFFFF if overflow\n    __m128i sat2   = _mm_add_epi32(sat1,asign);            // 7FFFFFFF if positive overflow 80000000 if negative overflow\n    return  selectb(overf2,sat2,diff);                     // diff if not overflow, else sat2\n}\n\n// function max: a > b ? a : b\nstatic inline Vec4i max(Vec4i const & a, Vec4i const & b) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    return _mm_max_epi32(a,b);\n#else\n    __m128i greater = _mm_cmpgt_epi32(a,b);\n    return selectb(greater,a,b);\n#endif\n}\n\n// function min: a < b ? a : b\nstatic inline Vec4i min(Vec4i const & a, Vec4i const & b) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    return _mm_min_epi32(a,b);\n#else\n    __m128i greater = _mm_cmpgt_epi32(a,b);\n    return selectb(greater,b,a);\n#endif\n}\n\n// function abs: a >= 0 ? a : -a\nstatic inline Vec4i abs(Vec4i const & a) {\n#if INSTRSET >= 4     // SSSE3 supported\n    return _mm_sign_epi32(a,a);\n#else                 // SSE2\n    __m128i sign = _mm_srai_epi32(a,31);                   // sign of a\n    __m128i inv  = _mm_xor_si128(a,sign);                  // invert bits if negative\n    return         _mm_sub_epi32(inv,sign);                // add 1\n#endif\n}\n\n// function abs_saturated: same as abs, saturate if overflow\nstatic inline Vec4i abs_saturated(Vec4i const & a) {\n    __m128i absa   = abs(a);                               // abs(a)\n    __m128i overfl = _mm_srai_epi32(absa,31);              // sign\n    return           _mm_add_epi32(absa,overfl);           // subtract 1 if 0x80000000\n}\n\n// function rotate_left all elements\n// Use negative count to rotate right\nstatic inline Vec4i rotate_left(Vec4i const & a, int b) {\n#ifdef __AVX512VL__\n    return _mm_rolv_epi32(a, _mm_set1_epi32(b));\n#elif defined __XOP__  // AMD XOP instruction set\n    return _mm_rot_epi32(a,_mm_set1_epi32(b));\n#else  // SSE2 instruction set\n    __m128i left  = _mm_sll_epi32(a,_mm_cvtsi32_si128(b & 0x1F));      // a << b \n    __m128i right = _mm_srl_epi32(a,_mm_cvtsi32_si128((32-b) & 0x1F)); // a >> (32 - b)\n    __m128i rot   = _mm_or_si128(left,right);                          // or\n    return  rot;\n#endif\n}\n\n\n/*****************************************************************************\n*\n*          Vector of 4 32-bit unsigned integers\n*\n*****************************************************************************/\n\nclass Vec4ui : public Vec4i {\npublic:\n    // Default constructor:\n    Vec4ui() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec4ui(uint32_t i) {\n        xmm = _mm_set1_epi32(i);\n    }\n    // Constructor to build from all elements:\n    Vec4ui(uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3) {\n        xmm = _mm_setr_epi32(i0, i1, i2, i3);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec4ui(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec4ui & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Member function to load from array (unaligned)\n    Vec4ui & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec4ui & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec4ui const & insert(uint32_t index, uint32_t value) {\n        Vec4i::insert(index, value);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    uint32_t extract(uint32_t index) const {\n        return Vec4i::extract(index);\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    uint32_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n// Define operators for this class\n\n// vector operator + : add\nstatic inline Vec4ui operator + (Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui (Vec4i(a) + Vec4i(b));\n}\n\n// vector operator - : subtract\nstatic inline Vec4ui operator - (Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui (Vec4i(a) - Vec4i(b));\n}\n\n// vector operator * : multiply\nstatic inline Vec4ui operator * (Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui (Vec4i(a) * Vec4i(b));\n}\n\n// vector operator / : divide\n// See bottom of file\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec4ui operator >> (Vec4ui const & a, uint32_t b) {\n    return _mm_srl_epi32(a,_mm_cvtsi32_si128(b)); \n}\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec4ui operator >> (Vec4ui const & a, int32_t b) {\n    return a >> (uint32_t)b;\n}\n\n// vector operator >>= : shift right logical\nstatic inline Vec4ui & operator >>= (Vec4ui & a, int b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec4ui operator << (Vec4ui const & a, uint32_t b) {\n    return Vec4ui ((Vec4i)a << (int32_t)b);\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec4ui operator << (Vec4ui const & a, int32_t b) {\n    return Vec4ui ((Vec4i)a << (int32_t)b);\n}\n\n// vector operator > : returns true for elements for which a > b (unsigned)\nstatic inline Vec4ib operator > (Vec4ui const & a, Vec4ui const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec4ib)_mm_comgt_epu32(a,b);\n#else  // SSE2 instruction set\n    __m128i signbit = _mm_set1_epi32(0x80000000);\n    __m128i a1      = _mm_xor_si128(a,signbit);\n    __m128i b1      = _mm_xor_si128(b,signbit);\n    return (Vec4ib)_mm_cmpgt_epi32(a1,b1);                         // signed compare\n#endif\n}\n\n// vector operator < : returns true for elements for which a < b (unsigned)\nstatic inline Vec4ib operator < (Vec4ui const & a, Vec4ui const & b) {\n    return b > a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (unsigned)\nstatic inline Vec4ib operator >= (Vec4ui const & a, Vec4ui const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return (Vec4ib)_mm_comge_epu32(a,b);\n#elif INSTRSET >= 5   // SSE4.1\n    __m128i max_ab = _mm_max_epu32(a,b);                   // max(a,b), unsigned\n    return (Vec4ib)_mm_cmpeq_epi32(a,max_ab);                      // a == max(a,b)\n#else  // SSE2 instruction set\n    return Vec4ib(Vec4i (~(b > a)));\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (unsigned)\nstatic inline Vec4ib operator <= (Vec4ui const & a, Vec4ui const & b) {\n    return b >= a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec4ui operator & (Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec4ui operator && (Vec4ui const & a, Vec4ui const & b) {\n    return a & b;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec4ui operator | (Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec4ui operator || (Vec4ui const & a, Vec4ui const & b) {\n    return a | b;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec4ui operator ^ (Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui(Vec128b(a) ^ Vec128b(b));\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec4ui operator ~ (Vec4ui const & a) {\n    return Vec4ui( ~ Vec128b(a));\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];\n// Each word in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec4ui select (Vec4ib const & s, Vec4ui const & a, Vec4ui const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec4ui if_add (Vec4ib const & f, Vec4ui const & a, Vec4ui const & b) {\n    return a + (Vec4ui(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline uint32_t horizontal_add (Vec4ui const & a) {\n    return horizontal_add((Vec4i)a);\n}\n\n// Horizontal add extended: Calculates the sum of all vector elements.\n// Elements are zero extended before adding to avoid overflow\nstatic inline uint64_t horizontal_add_x (Vec4ui const & a) {\n#ifdef __XOP__     // AMD XOP instruction set\n    __m128i sum1  = _mm_haddq_epu32(a);\n#else              // SSE2\n    __m128i zero  = _mm_setzero_si128();                   // 0\n    __m128i a01   = _mm_unpacklo_epi32(a,zero);            // zero-extended a0, a1\n    __m128i a23   = _mm_unpackhi_epi32(a,zero);            // zero-extended a2, a3\n    __m128i sum1  = _mm_add_epi64(a01,a23);                // add\n#endif\n    __m128i sum2  = _mm_unpackhi_epi64(sum1,sum1);         // high qword\n    __m128i sum3  = _mm_add_epi64(sum1,sum2);              // add\n#if defined(_M_AMD64) || defined(_M_X64) || defined(__x86_64__) || defined(__amd64)\n    return          _mm_cvtsi128_si64(sum3);               // 64 bit mode\n#else\n    union {\n        __m128i x;  // silly definition of _mm_storel_epi64 requires __m128i\n        uint64_t i;\n    } u;\n    _mm_storel_epi64(&u.x,sum3);\n    return u.i;\n#endif\n}\n\n// function add_saturated: add element by element, unsigned with saturation\nstatic inline Vec4ui add_saturated(Vec4ui const & a, Vec4ui const & b) {\n    Vec4ui sum      = a + b;\n    Vec4ui aorb     = Vec4ui(a | b);\n    Vec4ui overflow = Vec4ui(sum < aorb);                  // overflow if a + b < (a | b)\n    return Vec4ui (sum | overflow);                        // return 0xFFFFFFFF if overflow\n}\n\n// function sub_saturated: subtract element by element, unsigned with saturation\nstatic inline Vec4ui sub_saturated(Vec4ui const & a, Vec4ui const & b) {\n    Vec4ui diff      = a - b;\n    Vec4ui underflow = Vec4ui(diff > a);                   // underflow if a - b > a\n    return _mm_andnot_si128(underflow,diff);               // return 0 if underflow\n}\n\n// function max: a > b ? a : b\nstatic inline Vec4ui max(Vec4ui const & a, Vec4ui const & b) {\n#if INSTRSET >= 5   // SSE4.1\n    return _mm_max_epu32(a,b);\n#else  // SSE2\n    return select(a > b, a, b);\n#endif\n}\n\n// function min: a < b ? a : b\nstatic inline Vec4ui min(Vec4ui const & a, Vec4ui const & b) {\n#if INSTRSET >= 5   // SSE4.1\n    return _mm_min_epu32(a,b);\n#else  // SSE2\n    return select(a > b, b, a);\n#endif\n}\n\n\n/*****************************************************************************\n*\n*          Vector of 2 64-bit signed integers\n*\n*****************************************************************************/\n\nclass Vec2q : public Vec128b {\npublic:\n    // Default constructor:\n    Vec2q() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec2q(int64_t i) {\n#if defined (_MSC_VER) && _MSC_VER < 1900 && ! defined(__INTEL_COMPILER)\n        // MS compiler has no _mm_set1_epi64x in 32 bit mode\n#if defined(__x86_64__)                                    // 64 bit mode\n#if _MSC_VER < 1700\n        __m128i x1 = _mm_cvtsi64_si128(i);                 // 64 bit load\n        xmm = _mm_unpacklo_epi64(x1,x1);                   // broadcast\n#else\n		xmm =  _mm_set1_epi64x(i);\n#endif\n#else\n        union {\n            int64_t q[2];\n            int32_t r[4];\n        } u;\n        u.q[0] = u.q[1] = i;\n        xmm = _mm_setr_epi32(u.r[0], u.r[1], u.r[2], u.r[3]);\n        /*    // this will use an mm register and produce store forwarding stall:\n        union {\n            __m64 m;\n            int64_t ii;\n        } u;\n        u.ii = i;\n        xmm = _mm_set1_epi64(u.m);\n		_m_empty();        */\n\n#endif  // __x86_64__\n#else   // Other compilers\n        xmm = _mm_set1_epi64x(i);\n#endif\n    }\n    // Constructor to build from all elements:\n    Vec2q(int64_t i0, int64_t i1) {\n#if defined (_MSC_VER)  && _MSC_VER < 1900 && ! defined(__INTEL_COMPILER)\n        // MS compiler has no _mm_set_epi64x in 32 bit mode\n#if defined(__x86_64__)                                    // 64 bit mode\n#if _MSC_VER < 1700\n        __m128i x0 = _mm_cvtsi64_si128(i0);                // 64 bit load\n        __m128i x1 = _mm_cvtsi64_si128(i1);                // 64 bit load\n        xmm = _mm_unpacklo_epi64(x0,x1);                   // combine\n#else\n		xmm = _mm_set_epi64x(i1, i0);\n#endif\n#else   // MS compiler in 32-bit mode\n        union {\n            int64_t q[2];\n            int32_t r[4];\n        } u;\n        u.q[0] = i0;  u.q[1] = i1;\n		// this is inefficient, but other solutions are worse\n        xmm = _mm_setr_epi32(u.r[0], u.r[1], u.r[2], u.r[3]);\n#endif  // __x86_64__\n#else   // Other compilers\n        xmm = _mm_set_epi64x(i1, i0);\n#endif\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec2q(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec2q & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Type cast operator to convert to __m128i used in intrinsics\n    operator __m128i() const {\n        return xmm;\n    }\n    // Member function to load from array (unaligned)\n    Vec2q & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec2q & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Partial load. Load n elements and set the rest to 0\n    Vec2q & load_partial(int n, void const * p) {\n        switch (n) {\n        case 0:\n            *this = 0;  break;\n        case 1:\n            // intrinsic for movq is missing!\n            *this = Vec2q(*(int64_t const*)p, 0);  break;\n        case 2:\n            load(p);  break;\n        default: \n            break;\n        }\n        return *this;\n    }\n    // Partial store. Store n elements\n    void store_partial(int n, void * p) const {\n        switch (n) {\n        case 1:\n            int64_t q[2];\n            store(q);\n            *(int64_t*)p = q[0];  break;\n        case 2:\n            store(p);  break;\n        default:\n            break;\n        }\n    }\n    // cut off vector to n elements. The last 2-n elements are set to zero\n    Vec2q & cutoff(int n) {\n        *this = Vec16c(xmm).cutoff(n * 8);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec2q const & insert(uint32_t index, int64_t value) {\n#if INSTRSET >= 5 && defined(__x86_64__)  // SSE4.1 supported, 64 bit mode\n        if (index == 0) {\n            xmm = _mm_insert_epi64(xmm,value,0);\n        }\n        else {\n            xmm = _mm_insert_epi64(xmm,value,1);\n        }\n\n#else               // SSE2\n#if defined(__x86_64__)                                      // 64 bit mode\n        __m128i v = _mm_cvtsi64_si128(value);                // 64 bit load\n#else\n        union {\n            __m128i m;\n            int64_t ii;\n        } u;\n        u.ii = value;\n        __m128i v = _mm_loadl_epi64(&u.m);\n#endif\n        if (index == 0) {\n            v = _mm_unpacklo_epi64(v,v);     \n            xmm = _mm_unpackhi_epi64(v,xmm);\n        }\n        else {  // index = 1\n            xmm = _mm_unpacklo_epi64(xmm,v);\n        }\n#endif\n        return *this;\n    }\n    // Member function extract a single element from vector\n    int64_t extract(uint32_t index) const {\n        int64_t x[2];\n        store(x);\n        return x[index & 1];\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    int64_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n    static int size() {\n        return 2;\n    }\n};\n\n/*****************************************************************************\n*\n*          Vec2qb: Vector of 2 Booleans for use with Vec2q and Vec2uq\n*\n*****************************************************************************/\n// Definition will be different for the AVX512 instruction set\nclass Vec2qb : public Vec2q {\npublic:\n    // Default constructor:\n    Vec2qb() {\n    }\n    // Constructor to build from all elements:\n    Vec2qb(bool x0, bool x1) {\n        xmm = Vec2q(-int64_t(x0), -int64_t(x1));\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec2qb(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec2qb & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Constructor to broadcast scalar value:\n    Vec2qb(bool b) : Vec2q(-int64_t(b)) {\n    }\n    // Assignment operator to broadcast scalar value:\n    Vec2qb & operator = (bool b) {\n        *this = Vec2qb(b);\n        return *this;\n    }\nprivate: // Prevent constructing from int, etc.\n    Vec2qb(int b);\n    Vec2qb & operator = (int x);\npublic:\n    Vec2qb & insert (int index, bool a) {\n        Vec2q::insert(index, -(int64_t)a);\n        return *this;\n    }    \n    // Member function extract a single element from vector\n    bool extract(uint32_t index) const {\n        return Vec2q::extract(index) != 0;\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    bool operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n\n/*****************************************************************************\n*\n*          Define operators for Vec2qb\n*\n*****************************************************************************/\n\n// vector operator & : bitwise and\nstatic inline Vec2qb operator & (Vec2qb const & a, Vec2qb const & b) {\n    return Vec2qb(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec2qb operator && (Vec2qb const & a, Vec2qb const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec2qb & operator &= (Vec2qb & a, Vec2qb const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec2qb operator | (Vec2qb const & a, Vec2qb const & b) {\n    return Vec2qb(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec2qb operator || (Vec2qb const & a, Vec2qb const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec2qb & operator |= (Vec2qb & a, Vec2qb const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec2qb operator ^ (Vec2qb const & a, Vec2qb const & b) {\n    return Vec2qb(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec2qb & operator ^= (Vec2qb & a, Vec2qb const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec2qb operator ~ (Vec2qb const & a) {\n    return Vec2qb( ~ Vec128b(a));\n}\n\n// vector operator ! : element not\nstatic inline Vec2qb operator ! (Vec2qb const & a) {\n    return ~ a;\n}\n\n// vector function andnot\nstatic inline Vec2qb andnot (Vec2qb const & a, Vec2qb const & b) {\n    return Vec2qb(andnot(Vec128b(a), Vec128b(b)));\n}\n\n// Horizontal Boolean functions for Vec2qb\n\n// horizontal_and. Returns true if all elements are true\nstatic inline bool horizontal_and(Vec2qb const & a) {\n    return _mm_movemask_epi8(a) == 0xFFFF;\n}\n\n// horizontal_or. Returns true if at least one element is true\nstatic inline bool horizontal_or(Vec2qb const & a) {\n#if INSTRSET >= 5   // SSE4.1 supported. Use PTEST\n    return !_mm_testz_si128(a, a);\n#else\n    return _mm_movemask_epi8(a) != 0;\n#endif\n} \n\n\n/*****************************************************************************\n*\n*          Operators for Vec2q\n*\n*****************************************************************************/\n\n// vector operator + : add element by element\nstatic inline Vec2q operator + (Vec2q const & a, Vec2q const & b) {\n    return _mm_add_epi64(a, b);\n}\n\n// vector operator += : add\nstatic inline Vec2q & operator += (Vec2q & a, Vec2q const & b) {\n    a = a + b;\n    return a;\n}\n\n// postfix operator ++\nstatic inline Vec2q operator ++ (Vec2q & a, int) {\n    Vec2q a0 = a;\n    a = a + 1;\n    return a0;\n}\n\n// prefix operator ++\nstatic inline Vec2q & operator ++ (Vec2q & a) {\n    a = a + 1;\n    return a;\n}\n\n// vector operator - : subtract element by element\nstatic inline Vec2q operator - (Vec2q const & a, Vec2q const & b) {\n    return _mm_sub_epi64(a, b);\n}\n\n// vector operator - : unary minus\nstatic inline Vec2q operator - (Vec2q const & a) {\n    return _mm_sub_epi64(_mm_setzero_si128(), a);\n}\n\n// vector operator -= : subtract\nstatic inline Vec2q & operator -= (Vec2q & a, Vec2q const & b) {\n    a = a - b;\n    return a;\n}\n\n// postfix operator --\nstatic inline Vec2q operator -- (Vec2q & a, int) {\n    Vec2q a0 = a;\n    a = a - 1;\n    return a0;\n}\n\n// prefix operator --\nstatic inline Vec2q & operator -- (Vec2q & a) {\n    a = a - 1;\n    return a;\n}\n\n// vector operator * : multiply element by element\nstatic inline Vec2q operator * (Vec2q const & a, Vec2q const & b) {\n#if defined (__AVX512DQ__) && defined (__AVX512VL__)\n    return _mm_mullo_epi64(a, b);\n#elif INSTRSET >= 5   // SSE4.1 supported\n    // instruction does not exist. Split into 32-bit multiplies\n    __m128i bswap   = _mm_shuffle_epi32(b,0xB1);           // b0H,b0L,b1H,b1L (swap H<->L)\n    __m128i prodlh  = _mm_mullo_epi32(a,bswap);            // a0Lb0H,a0Hb0L,a1Lb1H,a1Hb1L, 32 bit L*H products\n    __m128i zero    = _mm_setzero_si128();                 // 0\n    __m128i prodlh2 = _mm_hadd_epi32(prodlh,zero);         // a0Lb0H+a0Hb0L,a1Lb1H+a1Hb1L,0,0\n    __m128i prodlh3 = _mm_shuffle_epi32(prodlh2,0x73);     // 0, a0Lb0H+a0Hb0L, 0, a1Lb1H+a1Hb1L\n    __m128i prodll  = _mm_mul_epu32(a,b);                  // a0Lb0L,a1Lb1L, 64 bit unsigned products\n    __m128i prod    = _mm_add_epi64(prodll,prodlh3);       // a0Lb0L+(a0Lb0H+a0Hb0L)<<32, a1Lb1L+(a1Lb1H+a1Hb1L)<<32\n    return  prod;\n#else               // SSE2\n    int64_t aa[2], bb[2];\n    a.store(aa);                                           // split into elements\n    b.store(bb);\n    return Vec2q(aa[0]*bb[0], aa[1]*bb[1]);                // multiply elements separetely\n#endif\n}\n\n// vector operator *= : multiply\nstatic inline Vec2q & operator *= (Vec2q & a, Vec2q const & b) {\n    a = a * b;\n    return a;\n}\n\n// vector operator << : shift left\nstatic inline Vec2q operator << (Vec2q const & a, int32_t b) {\n    return _mm_sll_epi64(a,_mm_cvtsi32_si128(b));\n}\n\n// vector operator <<= : shift left\nstatic inline Vec2q & operator <<= (Vec2q & a, int32_t b) {\n    a = a << b;\n    return a;\n}\n\n// vector operator >> : shift right arithmetic\nstatic inline Vec2q operator >> (Vec2q const & a, int32_t b) {\n    // instruction does not exist. Split into 32-bit shifts\n    if (b <= 32) {\n        __m128i bb   = _mm_cvtsi32_si128(b);               // b\n        __m128i sra  = _mm_sra_epi32(a,bb);                // a >> b signed dwords\n        __m128i srl  = _mm_srl_epi64(a,bb);                // a >> b unsigned qwords\n        __m128i mask = _mm_setr_epi32(0,-1,0,-1);          // mask for signed high part\n        return  selectb(mask,sra,srl);\n    }\n    else {  // b > 32\n        __m128i bm32 = _mm_cvtsi32_si128(b-32);            // b - 32\n        __m128i sign = _mm_srai_epi32(a,31);               // sign of a\n        __m128i sra2 = _mm_sra_epi32(a,bm32);              // a >> (b-32) signed dwords\n        __m128i sra3 = _mm_srli_epi64(sra2,32);            // a >> (b-32) >> 32 (second shift unsigned qword)\n        __m128i mask = _mm_setr_epi32(0,-1,0,-1);          // mask for high part containing only sign\n        return  selectb(mask,sign,sra3);\n    }\n}\n\n// vector operator >>= : shift right arithmetic\nstatic inline Vec2q & operator >>= (Vec2q & a, int32_t b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator == : returns true for elements for which a == b\nstatic inline Vec2qb operator == (Vec2q const & a, Vec2q const & b) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    return _mm_cmpeq_epi64(a, b);\n#else               // SSE2\n    // no 64 compare instruction. Do two 32 bit compares\n    __m128i com32  = _mm_cmpeq_epi32(a,b);                 // 32 bit compares\n    __m128i com32s = _mm_shuffle_epi32(com32,0xB1);        // swap low and high dwords\n    __m128i test   = _mm_and_si128(com32,com32s);          // low & high\n    __m128i teste  = _mm_srai_epi32(test,31);              // extend sign bit to 32 bits\n    __m128i testee = _mm_shuffle_epi32(teste,0xF5);        // extend sign bit to 64 bits\n    return  Vec2qb(Vec2q(testee));\n#endif\n}\n\n// vector operator != : returns true for elements for which a != b\nstatic inline Vec2qb operator != (Vec2q const & a, Vec2q const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return Vec2qb(_mm_comneq_epi64(a,b));\n#else  // SSE2 instruction set\n    return Vec2qb(Vec2q(~(a == b)));\n#endif\n}\n  \n// vector operator < : returns true for elements for which a < b\nstatic inline Vec2qb operator < (Vec2q const & a, Vec2q const & b) {\n#if INSTRSET >= 6   // SSE4.2 supported\n    return Vec2qb(Vec2q(_mm_cmpgt_epi64(b, a)));\n#else               // SSE2\n    // no 64 compare instruction. Subtract\n    __m128i s      = _mm_sub_epi64(a,b);                   // a-b\n    // a < b if a and b have same sign and s < 0 or (a < 0 and b >= 0)\n    // The latter () corrects for overflow\n    __m128i axb    = _mm_xor_si128(a,b);                   // a ^ b\n    __m128i anb    = _mm_andnot_si128(b,a);                // a & ~b\n    __m128i snaxb  = _mm_andnot_si128(axb,s);              // s & ~(a ^ b)\n    __m128i or1    = _mm_or_si128(anb,snaxb);              // (a & ~b) | (s & ~(a ^ b))\n    __m128i teste  = _mm_srai_epi32(or1,31);               // extend sign bit to 32 bits\n    __m128i testee = _mm_shuffle_epi32(teste,0xF5);        // extend sign bit to 64 bits\n    return  testee;\n#endif\n}\n\n// vector operator > : returns true for elements for which a > b\nstatic inline Vec2qb operator > (Vec2q const & a, Vec2q const & b) {\n    return b < a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (signed)\nstatic inline Vec2qb operator >= (Vec2q const & a, Vec2q const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return Vec2qb(_mm_comge_epi64(a,b));\n#else  // SSE2 instruction set\n    return Vec2qb(Vec2q(~(a < b)));\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (signed)\nstatic inline Vec2qb operator <= (Vec2q const & a, Vec2q const & b) {\n    return b >= a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec2q operator & (Vec2q const & a, Vec2q const & b) {\n    return Vec2q(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec2q operator && (Vec2q const & a, Vec2q const & b) {\n    return a & b;\n}\n// vector operator &= : bitwise and\nstatic inline Vec2q & operator &= (Vec2q & a, Vec2q const & b) {\n    a = a & b;\n    return a;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec2q operator | (Vec2q const & a, Vec2q const & b) {\n    return Vec2q(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec2q operator || (Vec2q const & a, Vec2q const & b) {\n    return a | b;\n}\n// vector operator |= : bitwise or\nstatic inline Vec2q & operator |= (Vec2q & a, Vec2q const & b) {\n    a = a | b;\n    return a;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec2q operator ^ (Vec2q const & a, Vec2q const & b) {\n    return Vec2q(Vec128b(a) ^ Vec128b(b));\n}\n// vector operator ^= : bitwise xor\nstatic inline Vec2q & operator ^= (Vec2q & a, Vec2q const & b) {\n    a = a ^ b;\n    return a;\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec2q operator ~ (Vec2q const & a) {\n    return Vec2q( ~ Vec128b(a));\n}\n\n// vector operator ! : logical not, returns true for elements == 0\nstatic inline Vec2qb operator ! (Vec2q const & a) {\n    return a == Vec2q(_mm_setzero_si128());\n}\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];\n// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec2q select (Vec2qb const & s, Vec2q const & a, Vec2q const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec2q if_add (Vec2qb const & f, Vec2q const & a, Vec2q const & b) {\n    return a + (Vec2q(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline int64_t horizontal_add (Vec2q const & a) {\n    __m128i sum1  = _mm_shuffle_epi32(a,0x0E);             // high element\n    __m128i sum2  = _mm_add_epi64(a,sum1);                 // sum\n#if defined(__x86_64__)\n    return          _mm_cvtsi128_si64(sum2);               // 64 bit mode\n#else\n    union {\n        __m128i x;  // silly definition of _mm_storel_epi64 requires __m128i\n        int64_t i;\n    } u;\n    _mm_storel_epi64(&u.x,sum2);\n    return u.i;\n#endif\n}\n\n// function max: a > b ? a : b\nstatic inline Vec2q max(Vec2q const & a, Vec2q const & b) {\n    return select(a > b, a, b);\n}\n\n// function min: a < b ? a : b\nstatic inline Vec2q min(Vec2q const & a, Vec2q const & b) {\n    return select(a < b, a, b);\n}\n\n// function abs: a >= 0 ? a : -a\nstatic inline Vec2q abs(Vec2q const & a) {\n#if INSTRSET >= 6     // SSE4.2 supported\n    __m128i sign  = _mm_cmpgt_epi64(_mm_setzero_si128(),a);// 0 > a\n#else                 // SSE2\n    __m128i signh = _mm_srai_epi32(a,31);                  // sign in high dword\n    __m128i sign  = _mm_shuffle_epi32(signh,0xF5);         // copy sign to low dword\n#endif\n    __m128i inv   = _mm_xor_si128(a,sign);                 // invert bits if negative\n    return          _mm_sub_epi64(inv,sign);               // add 1\n}\n\n// function abs_saturated: same as abs, saturate if overflow\nstatic inline Vec2q abs_saturated(Vec2q const & a) {\n    __m128i absa   = abs(a);                               // abs(a)\n#if INSTRSET >= 6     // SSE4.2 supported\n    __m128i overfl = _mm_cmpgt_epi64(_mm_setzero_si128(),absa);// 0 > a\n#else                 // SSE2\n    __m128i signh = _mm_srai_epi32(absa,31);               // sign in high dword\n    __m128i overfl= _mm_shuffle_epi32(signh,0xF5);         // copy sign to low dword\n#endif\n    return           _mm_add_epi64(absa,overfl);           // subtract 1 if 0x8000000000000000\n}\n\n// function rotate_left all elements\n// Use negative count to rotate right\nstatic inline Vec2q rotate_left(Vec2q const & a, int b) {\n#ifdef __AVX512VL__\n    return _mm_rolv_epi64(a, _mm_set1_epi64x(int64_t(b)));\n#elif defined __XOP__  // AMD XOP instruction set\n    return (Vec2q)_mm_rot_epi64(a,Vec2q(b));\n#else  // SSE2 instruction set\n    __m128i left  = _mm_sll_epi64(a,_mm_cvtsi32_si128(b & 0x3F));      // a << b \n    __m128i right = _mm_srl_epi64(a,_mm_cvtsi32_si128((64-b) & 0x3F)); // a >> (64 - b)\n    __m128i rot   = _mm_or_si128(left,right);                          // or\n    return  (Vec2q)rot;\n#endif\n}\n\n\n/*****************************************************************************\n*\n*          Vector of 2 64-bit unsigned integers\n*\n*****************************************************************************/\n\nclass Vec2uq : public Vec2q {\npublic:\n    // Default constructor:\n    Vec2uq() {\n    }\n    // Constructor to broadcast the same value into all elements:\n    Vec2uq(uint64_t i) {\n        xmm = Vec2q(i);\n    }\n    // Constructor to build from all elements:\n    Vec2uq(uint64_t i0, uint64_t i1) {\n        xmm = Vec2q(i0, i1);\n    }\n    // Constructor to convert from type __m128i used in intrinsics:\n    Vec2uq(__m128i const & x) {\n        xmm = x;\n    }\n    // Assignment operator to convert from type __m128i used in intrinsics:\n    Vec2uq & operator = (__m128i const & x) {\n        xmm = x;\n        return *this;\n    }\n    // Member function to load from array (unaligned)\n    Vec2uq & load(void const * p) {\n        xmm = _mm_loadu_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to load from array (aligned)\n    Vec2uq & load_a(void const * p) {\n        xmm = _mm_load_si128((__m128i const*)p);\n        return *this;\n    }\n    // Member function to change a single element in vector\n    // Note: This function is inefficient. Use load function if changing more than one element\n    Vec2uq const & insert(uint32_t index, uint64_t value) {\n        Vec2q::insert(index, value);\n        return *this;\n    }\n    // Member function extract a single element from vector\n    uint64_t extract(uint32_t index) const {\n        return Vec2q::extract(index);\n    }\n    // Extract a single element. Use store function if extracting more than one element.\n    // Operator [] can only read an element, not write.\n    uint64_t operator [] (uint32_t index) const {\n        return extract(index);\n    }\n};\n\n// Define operators for this class\n\n// vector operator + : add\nstatic inline Vec2uq operator + (Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq (Vec2q(a) + Vec2q(b));\n}\n\n// vector operator - : subtract\nstatic inline Vec2uq operator - (Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq (Vec2q(a) - Vec2q(b));\n}\n\n// vector operator * : multiply element by element\nstatic inline Vec2uq operator * (Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq (Vec2q(a) * Vec2q(b));\n}\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec2uq operator >> (Vec2uq const & a, uint32_t b) {\n    return _mm_srl_epi64(a,_mm_cvtsi32_si128(b)); \n}\n\n// vector operator >> : shift right logical all elements\nstatic inline Vec2uq operator >> (Vec2uq const & a, int32_t b) {\n    return a >> (uint32_t)b;\n}\n\n// vector operator >>= : shift right logical\nstatic inline Vec2uq & operator >>= (Vec2uq & a, int b) {\n    a = a >> b;\n    return a;\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec2uq operator << (Vec2uq const & a, uint32_t b) {\n    return Vec2uq ((Vec2q)a << (int32_t)b);\n}\n\n// vector operator << : shift left all elements\nstatic inline Vec2uq operator << (Vec2uq const & a, int32_t b) {\n    return Vec2uq ((Vec2q)a << b);\n}\n\n// vector operator > : returns true for elements for which a > b (unsigned)\nstatic inline Vec2qb operator > (Vec2uq const & a, Vec2uq const & b) {\n#if defined ( __XOP__ ) // AMD XOP instruction set\n    return Vec2qb(_mm_comgt_epu64(a,b));\n#elif INSTRSET >= 6 // SSE4.2\n    __m128i sign64 = constant4i<0,(int32_t)0x80000000,0,(int32_t)0x80000000>();\n    __m128i aflip  = _mm_xor_si128(a, sign64);             // flip sign bits to use signed compare\n    __m128i bflip  = _mm_xor_si128(b, sign64);\n    Vec2q   cmp    = _mm_cmpgt_epi64(aflip,bflip);\n    return Vec2qb(cmp);\n#else  // SSE2 instruction set\n    __m128i sign32  = _mm_set1_epi32(0x80000000);          // sign bit of each dword\n    __m128i aflip   = _mm_xor_si128(a,sign32);             // a with sign bits flipped to use signed compare\n    __m128i bflip   = _mm_xor_si128(b,sign32);             // b with sign bits flipped to use signed compare\n    __m128i equal   = _mm_cmpeq_epi32(a,b);                // a == b, dwords\n    __m128i bigger  = _mm_cmpgt_epi32(aflip,bflip);        // a > b, dwords\n    __m128i biggerl = _mm_shuffle_epi32(bigger,0xA0);      // a > b, low dwords copied to high dwords\n    __m128i eqbig   = _mm_and_si128(equal,biggerl);        // high part equal and low part bigger\n    __m128i hibig   = _mm_or_si128(bigger,eqbig);          // high part bigger or high part equal and low part bigger\n    __m128i big     = _mm_shuffle_epi32(hibig,0xF5);       // result copied to low part\n    return  Vec2qb(Vec2q(big));\n#endif\n}\n\n// vector operator < : returns true for elements for which a < b (unsigned)\nstatic inline Vec2qb operator < (Vec2uq const & a, Vec2uq const & b) {\n    return b > a;\n}\n\n// vector operator >= : returns true for elements for which a >= b (unsigned)\nstatic inline Vec2qb operator >= (Vec2uq const & a, Vec2uq const & b) {\n#ifdef __XOP__  // AMD XOP instruction set\n    return Vec2qb(_mm_comge_epu64(a,b));\n#else  // SSE2 instruction set\n    return  Vec2qb(Vec2q(~(b > a)));\n#endif\n}\n\n// vector operator <= : returns true for elements for which a <= b (unsigned)\nstatic inline Vec2qb operator <= (Vec2uq const & a, Vec2uq const & b) {\n    return b >= a;\n}\n\n// vector operator & : bitwise and\nstatic inline Vec2uq operator & (Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq(Vec128b(a) & Vec128b(b));\n}\nstatic inline Vec2uq operator && (Vec2uq const & a, Vec2uq const & b) {\n    return a & b;\n}\n\n// vector operator | : bitwise or\nstatic inline Vec2uq operator | (Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq(Vec128b(a) | Vec128b(b));\n}\nstatic inline Vec2uq operator || (Vec2uq const & a, Vec2uq const & b) {\n    return a | b;\n}\n\n// vector operator ^ : bitwise xor\nstatic inline Vec2uq operator ^ (Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq(Vec128b(a) ^ Vec128b(b));\n}\n\n// vector operator ~ : bitwise not\nstatic inline Vec2uq operator ~ (Vec2uq const & a) {\n    return Vec2uq( ~ Vec128b(a));\n}\n\n\n// Functions for this class\n\n// Select between two operands. Corresponds to this pseudocode:\n// for (int i = 0; i < 2; i++) result[i] = s[i] ? a[i] : b[i];\n// Each word in s must be either 0 (false) or -1 (true). No other values are allowed.\n// (s is signed)\nstatic inline Vec2uq select (Vec2qb const & s, Vec2uq const & a, Vec2uq const & b) {\n    return selectb(s,a,b);\n}\n\n// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]\nstatic inline Vec2uq if_add (Vec2qb const & f, Vec2uq const & a, Vec2uq const & b) {\n    return a + (Vec2uq(f) & b);\n}\n\n// Horizontal add: Calculates the sum of all vector elements.\n// Overflow will wrap around\nstatic inline uint64_t horizontal_add (Vec2uq const & a) {\n    return horizontal_add((Vec2q)a);\n}\n\n// function max: a > b ? a : b\nstatic inline Vec2uq max(Vec2uq const & a, Vec2uq const & b) {\n    return select(a > b, a, b);\n}\n\n// function min: a < b ? a : b\nstatic inline Vec2uq min(Vec2uq const & a, Vec2uq const & b) {\n    return select(a > b, b, a);\n}\n\n\n/*****************************************************************************\n*\n*          Vector permute functions\n*\n******************************************************************************\n*\n* These permute functions can reorder the elements of a vector and optionally\n* set some elements to zero. \n*\n* The indexes are inserted as template parameters in <>. These indexes must be\n* constants. Each template parameter is an index to the element you want to \n* select. A negative index will generate zero. an index of -256 means don't care.\n*\n* Example:\n* Vec4i a(10,11,12,13);         // a is (10,11,12,13)\n* Vec4i b, c;\n* b = permute4i<0,0,2,2>(a);    // b is (10,10,12,12)\n* c = permute4i<3,2,-1,-1>(a);  // c is (13,12, 0, 0)\n*\n* The permute functions for vectors of 8-bit integers are inefficient if \n* the SSSE3 instruction set or later is not enabled.\n*\n* A lot of the code here is metaprogramming aiming to find the instructions\n* that best fit the template parameters and instruction set. The metacode\n* will be reduced out to leave only a few vector instructions in release\n* mode with optimization on.\n*****************************************************************************/\n\ntemplate <int i0, int i1>\nstatic inline Vec2q permute2q(Vec2q const & a) {\n    if (i0 == 0) {\n        if (i1 == 0) {       // 0,0\n            return _mm_unpacklo_epi64(a, a);\n        }\n        else if (i1 == 1 || i1 == -0x100) {  // 0,1\n            return a;\n        }\n        else {               // 0,-1\n            // return _mm_mov_epi64(a); // doesn't work with MS VS 2008\n            return _mm_and_si128(a, constant4i<-1,-1,0,0>());\n        }\n    }\n    else if (i0 == 1) {\n        if (i1 == 0) {       // 1,0\n            return _mm_shuffle_epi32(a, 0x4E);\n        }\n        else if (i1 == 1) {  // 1,1\n            return _mm_unpackhi_epi64(a, a);\n        }\n        else {               // 1,-1\n            return _mm_srli_si128(a, 8);\n        }\n    }\n    else { // i0 < 0\n        if (i1 == 0) {       // -1,0\n            return _mm_slli_si128(a, 8);\n        }\n        else if (i1 == 1) {  // -1,1\n            if (i0 == -0x100) return a;\n            return _mm_and_si128(a, constant4i<0,0,-1,-1>());\n        }\n        else {               // -1,-1\n            return _mm_setzero_si128();\n        }\n    }\n}\n\ntemplate <int i0, int i1>\nstatic inline Vec2uq permute2uq(Vec2uq const & a) {\n    return Vec2uq (permute2q <i0, i1> ((__m128i)a));\n}\n\n// permute vector Vec4i\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline Vec4i permute4i(Vec4i const & a) {\n\n    // Combine all the indexes into a single bitfield, with 4 bits for each\n    const uint32_t m1 = (i0&3) | (i1&3)<<4 | (i2&3)<<8 | (i3&3)<<12; \n\n    // Mask to zero out negative indexes\n    const uint32_t mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12;\n\n    // Mask indicating required zeroing of all indexes, with 4 bits for each, 0 for index = -1, 0xF for index >= 0 or -256\n    const uint32_t ssz = ((i0 & 0x80) ? 0 : 0xF) | ((i1 & 0x80) ? 0 : 0xF) << 4 | ((i2 & 0x80) ? 0 : 0xF) << 8 | ((i3 & 0x80) ? 0 : 0xF) << 12;\n\n    // Mask indicating 0 for don't care, 0xF for non-negative value of required zeroing\n    const uint32_t md = mz | ~ ssz;\n\n    // Test if permutation needed\n    const bool do_shuffle = ((m1 ^ 0x00003210) & mz) != 0;\n\n    // is zeroing needed\n    const bool do_zero    = (ssz != 0xFFFF);\n\n    if (mz == 0) {\n        return _mm_setzero_si128();    // special case: all zero or don't care\n    }\n    // Test if we can do with 64-bit permute only\n    if ((m1 & 0x0101 & mz) == 0        // even indexes are even or negative\n    && (~m1 & 0x1010 & mz) == 0        // odd  indexes are odd  or negative\n    && ((m1 ^ ((m1 + 0x0101) << 4)) & 0xF0F0 & mz & (mz << 4)) == 0  // odd index == preceding even index +1 or at least one of them negative\n    && ((mz ^ (mz << 4)) & 0xF0F0 & md & md << 4) == 0) {      // each pair of indexes are both negative or both positive or one of them don't care\n        const int j0 = i0 >= 0 ? i0 / 2 : (i0 & 0x80) ? i0 : i1 >= 0 ? i1/2 : i1;\n        const int j1 = i2 >= 0 ? i2 / 2 : (i2 & 0x80) ? i2 : i3 >= 0 ? i3/2 : i3;\n        return Vec4i(permute2q<j0, j1> (Vec2q(a)));    // 64 bit permute\n    }\n#if  INSTRSET >= 4  // SSSE3\n    if (do_shuffle && do_zero) {\n        // With SSSE3 we can do both with the PSHUFB instruction\n        const int j0 = (i0 & 3) << 2;\n        const int j1 = (i1 & 3) << 2;\n        const int j2 = (i2 & 3) << 2;\n        const int j3 = (i3 & 3) << 2;\n        __m128i mask1 = constant4i <\n            i0 < 0 ? -1 : j0 | (j0+1)<<8 | (j0+2)<<16 | (j0+3) << 24,\n            i1 < 0 ? -1 : j1 | (j1+1)<<8 | (j1+2)<<16 | (j1+3) << 24,\n            i2 < 0 ? -1 : j2 | (j2+1)<<8 | (j2+2)<<16 | (j2+3) << 24,\n            i3 < 0 ? -1 : j3 | (j3+1)<<8 | (j3+2)<<16 | (j3+3) << 24 > ();\n        return _mm_shuffle_epi8(a,mask1);\n    }\n#endif\n    __m128i t1;\n\n    if (do_shuffle) {  // permute\n        t1 = _mm_shuffle_epi32(a, (i0&3) | (i1&3)<<2 | (i2&3)<<4 | (i3&3)<<6);\n    }\n    else {\n        t1 = a;\n    }\n    if (do_zero) {     // set some elements to zero\n        __m128i mask2 = constant4i< -int(i0>=0), -int(i1>=0), -int(i2>=0), -int(i3>=0) >();\n        t1 = _mm_and_si128(t1,mask2);\n    }\n    return t1;\n}\n\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline Vec4ui permute4ui(Vec4ui const & a) {\n    return Vec4ui (permute4i <i0,i1,i2,i3> (a));\n}\n\ntemplate <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>\nstatic inline Vec8s permute8s(Vec8s const & a) {\n    if ((i0 & i1 & i2 & i3 & i4 & i5 & i6 & i7) < 0) {\n        return _mm_setzero_si128();  // special case: all zero\n    }\n#if  INSTRSET >= 4  // SSSE3\n\n    // special case: rotate\n    if (i0>=0 && i0 < 8 && i1==((i0+1)&7) && i2==((i0+2)&7) && i3==((i0+3)&7) && i4==((i0+4)&7) && i5==((i0+5)&7) && i6==((i0+6)&7) && i7==((i0+7)&7)) {\n        if (i0 == 0) return a;  // do nothing\n        return _mm_alignr_epi8(a, a, (i0 & 7) * 2);\n    }    \n    \n    // General case: Use PSHUFB\n    const int j0 = i0 < 0 ? 0xFFFF : ( (i0 & 7) * 2 | ((i0 & 7) * 2 + 1) << 8 );\n    const int j1 = i1 < 0 ? 0xFFFF : ( (i1 & 7) * 2 | ((i1 & 7) * 2 + 1) << 8 );\n    const int j2 = i2 < 0 ? 0xFFFF : ( (i2 & 7) * 2 | ((i2 & 7) * 2 + 1) << 8 );\n    const int j3 = i3 < 0 ? 0xFFFF : ( (i3 & 7) * 2 | ((i3 & 7) * 2 + 1) << 8 );\n    const int j4 = i4 < 0 ? 0xFFFF : ( (i4 & 7) * 2 | ((i4 & 7) * 2 + 1) << 8 );\n    const int j5 = i5 < 0 ? 0xFFFF : ( (i5 & 7) * 2 | ((i5 & 7) * 2 + 1) << 8 );\n    const int j6 = i6 < 0 ? 0xFFFF : ( (i6 & 7) * 2 | ((i6 & 7) * 2 + 1) << 8 );\n    const int j7 = i7 < 0 ? 0xFFFF : ( (i7 & 7) * 2 | ((i7 & 7) * 2 + 1) << 8 );\n    __m128i mask = constant4i < j0 | j1 << 16, j2 | j3 << 16, j4 | j5 << 16, j6 | j7 << 16 > ();\n    return _mm_shuffle_epi8(a,mask);\n\n#else   // SSE2 has no simple solution. Find the optimal permute method.\n    // Without proper metaprogramming features, we have to use constant expressions \n    // and if-statements to make sure these calculations are resolved at compile time.\n    // All this should produce at most 8 instructions in the final code, depending\n    // on the template parameters.\n\n    // Temporary vectors\n    __m128i t1, t2, t3, t4, t5, t6, t7;\n\n    // Combine all the indexes into a single bitfield, with 4 bits for each\n    const int m1 = (i0&7) | (i1&7)<<4 | (i2&7)<<8 | (i3&7)<<12 \n        | (i4&7)<<16 | (i5&7)<<20 | (i6&7)<<24 | (i7&7)<<28; \n\n    // Mask to zero out negative indexes\n    const int m2 = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12\n        | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;\n\n    // Test if we can do without permute\n    const bool case0 = ((m1 ^ 0x76543210) & m2) == 0; // all indexes point to their own place or negative\n\n    // Test if we can do with 32-bit permute only\n    const bool case1 = \n        (m1 & 0x01010101 & m2) == 0        // even indexes are even or negative\n        && (~m1 & 0x10101010 & m2) == 0    // odd  indexes are odd  or negative\n        && ((m1 ^ ((m1 + 0x01010101) << 4)) & 0xF0F0F0F0 & m2 & (m2 << 4)) == 0; // odd index == preceding even index +1 or at least one of them negative\n\n    // Test if we can do with 16-bit permute only\n    const bool case2 = \n        (((m1 & 0x44444444) ^ 0x44440000) & m2) == 0;  // indexes 0-3 point to lower 64 bits, 1-7 to higher 64 bits, or negative\n\n    if (case0) {\n        // no permute needed\n        t7 = a;\n    }\n    else if (case1) {\n        // 32 bit permute only\n        const int j0 = i0 >= 0 ? i0/2 : i1 >= 0 ? i1/2 : 0;\n        const int j1 = i2 >= 0 ? i2/2 : i3 >= 0 ? i3/2 : 0;\n        const int j2 = i4 >= 0 ? i4/2 : i5 >= 0 ? i5/2 : 0;\n        const int j3 = i6 >= 0 ? i6/2 : i7 >= 0 ? i7/2 : 0;\n        t7 = _mm_shuffle_epi32(a, (j0&3) | (j1&3)<<2 | (j2&3)<<4 | (j3&3)<<6 );\n    }\n    else if (case2) {\n        // 16 bit permute only\n        const int j0 = i0 >= 0 ? i0&3 : 0;\n        const int j1 = i1 >= 0 ? i1&3 : 1;\n        const int j2 = i2 >= 0 ? i2&3 : 2;\n        const int j3 = i3 >= 0 ? i3&3 : 3;\n        const int j4 = i4 >= 0 ? i4&3 : 0;\n        const int j5 = i5 >= 0 ? i5&3 : 1;\n        const int j6 = i6 >= 0 ? i6&3 : 2;\n        const int j7 = i7 >= 0 ? i7&3 : 3;\n        if (j0!=0 || j1!=1 || j2!=2 || j3!=3) {            \n            t1 = _mm_shufflelo_epi16(a, j0 | j1<<2 | j2<<4 | j3<<6);\n        }\n        else t1 = a;\n        if (j4!=0 || j5!=1 || j6!=2 || j7!=3) {            \n            t7 = _mm_shufflehi_epi16(t1, j4 | j5<<2 | j6<<4 | j7<<6);\n        }\n        else t7 = t1;\n    }\n    else {\n        // Need at least two permute steps\n\n        // Index to where each dword of a is needed\n        const int nn = (m1 & 0x66666666) | 0x88888888; // indicate which dwords are needed\n        const int n0 = ((((uint32_t)(nn ^ 0x00000000) - 0x22222222) & 0x88888888) ^ 0x88888888) & m2;\n        const int n1 = ((((uint32_t)(nn ^ 0x22222222) - 0x22222222) & 0x88888888) ^ 0x88888888) & m2;\n        const int n2 = ((((uint32_t)(nn ^ 0x44444444) - 0x22222222) & 0x88888888) ^ 0x88888888) & m2;\n        const int n3 = ((((uint32_t)(nn ^ 0x66666666) - 0x22222222) & 0x88888888) ^ 0x88888888) & m2;\n        // indicate which dwords are needed in low half\n        const int l0 = (n0 & 0xFFFF) != 0;\n        const int l1 = (n1 & 0xFFFF) != 0;\n        const int l2 = (n2 & 0xFFFF) != 0;\n        const int l3 = (n3 & 0xFFFF) != 0;\n        // indicate which dwords are needed in high half\n        const int h0 = (n0 & 0xFFFF0000) != 0;\n        const int h1 = (n1 & 0xFFFF0000) != 0;\n        const int h2 = (n2 & 0xFFFF0000) != 0;\n        const int h3 = (n3 & 0xFFFF0000) != 0;\n\n        // Test if we can do with two permute steps\n        const bool case3 = l0 + l1 + l2 + l3 <= 2  &&  h0 + h1 + h2 + h3 <= 2;\n\n        if (case3) {\n            // one 32-bit permute followed by one 16-bit permute in each half.\n            // Find permute indices for 32-bit permute\n            const int j0 = l0 ? 0 : l1 ? 1 : l2 ? 2 : 3;\n            const int j1 = l3 ? 3 : l2 ? 2 : l1 ? 1 : 0;\n            const int j2 = h0 ? 0 : h1 ? 1 : h2 ? 2 : 3;\n            const int j3 = h3 ? 3 : h2 ? 2 : h1 ? 1 : 0;\n\n            // Find permute indices for low 16-bit permute\n            const int r0 = i0 < 0 ? 0 : (i0>>1 == j0 ? 0 : 2) + (i0 & 1);\n            const int r1 = i1 < 0 ? 1 : (i1>>1 == j0 ? 0 : 2) + (i1 & 1);\n            const int r2 = i2 < 0 ? 2 : (i2>>1 == j1 ? 2 : 0) + (i2 & 1);\n            const int r3 = i3 < 0 ? 3 : (i3>>1 == j1 ? 2 : 0) + (i3 & 1);\n\n            // Find permute indices for high 16-bit permute\n            const int s0 = i4 < 0 ? 0 : (i4>>1 == j2 ? 0 : 2) + (i4 & 1);\n            const int s1 = i5 < 0 ? 1 : (i5>>1 == j2 ? 0 : 2) + (i5 & 1);\n            const int s2 = i6 < 0 ? 2 : (i6>>1 == j3 ? 2 : 0) + (i6 & 1);\n            const int s3 = i7 < 0 ? 3 : (i7>>1 == j3 ? 2 : 0) + (i7 & 1);\n\n            // 32-bit permute\n            t1 = _mm_shuffle_epi32 (a, j0 | j1<<2 | j2<<4 | j3<<6);\n            // 16-bit permutes\n            if (r0!=0 || r1!=1 || r2!=2 || r3!=3) {  // 16 bit permute of low  half\n                t2 = _mm_shufflelo_epi16(t1, r0 | r1<<2 | r2<<4 | r3<<6);\n            }\n            else t2 = t1;\n            if (s0!=0 || s1!=1 || s2!=2 || s3!=3) {  // 16 bit permute of high half                \n                t7 = _mm_shufflehi_epi16(t2, s0 | s1<<2 | s2<<4 | s3<<6);\n            }\n            else t7 = t2;\n        }\n        else {\n            // Worst case. We need two sets of 16-bit permutes\n            t1 = _mm_shuffle_epi32(a, 0x4E);  // swap low and high 64-bits\n\n            // Find permute indices for low 16-bit permute from swapped t1\n            const int r0 = i0 < 4 ? 0 : i0 & 3;\n            const int r1 = i1 < 4 ? 1 : i1 & 3;\n            const int r2 = i2 < 4 ? 2 : i2 & 3;\n            const int r3 = i3 < 4 ? 3 : i3 & 3;\n            // Find permute indices for high 16-bit permute from swapped t1\n            const int s0 = i4 < 0 || i4 >= 4 ? 0 : i4 & 3;\n            const int s1 = i5 < 0 || i5 >= 4 ? 1 : i5 & 3;\n            const int s2 = i6 < 0 || i6 >= 4 ? 2 : i6 & 3;\n            const int s3 = i7 < 0 || i7 >= 4 ? 3 : i7 & 3;\n            // Find permute indices for low 16-bit permute from direct a\n            const int u0 = i0 < 0 || i0 >= 4 ? 0 : i0 & 3;\n            const int u1 = i1 < 0 || i1 >= 4 ? 1 : i1 & 3;\n            const int u2 = i2 < 0 || i2 >= 4 ? 2 : i2 & 3;\n            const int u3 = i3 < 0 || i3 >= 4 ? 3 : i3 & 3;\n            // Find permute indices for high 16-bit permute from direct a\n            const int v0 = i4 < 4 ? 0 : i4 & 3;\n            const int v1 = i5 < 4 ? 1 : i5 & 3;\n            const int v2 = i6 < 4 ? 2 : i6 & 3;\n            const int v3 = i7 < 4 ? 3 : i7 & 3;\n\n            // 16-bit permutes\n            if (r0!=0 || r1!=1 || r2!=2 || r3!=3) {  // 16 bit permute of low  half\n                t2 = _mm_shufflelo_epi16(t1, r0 | r1<<2 | r2<<4 | r3<<6);\n            }\n            else t2 = t1;\n            if (u0!=0 || u1!=1 || u2!=2 || u3!=3) {  // 16 bit permute of low  half\n                t3 = _mm_shufflelo_epi16(a, u0 | u1<<2 | u2<<4 | u3<<6);\n            }\n            else t3 = a;\n            if (s0!=0 || s1!=1 || s2!=2 || s3!=3) {  // 16 bit permute of low  half\n                t4 = _mm_shufflehi_epi16(t2, s0 | s1<<2 | s2<<4 | s3<<6);\n            }\n            else t4 = t2;\n            if (v0!=0 || v1!=1 || v2!=2 || v3!=3) {  // 16 bit permute of low  half\n                t5 = _mm_shufflehi_epi16(t3, v0 | v1<<2 | v2<<4 | v3<<6);\n            }\n            else t5 = t3;\n            // merge data from t4 and t5\n            t6  = constant4i <\n                ((i0 & 4) ? 0xFFFF : 0) | ((i1 & 4) ? 0xFFFF0000 : 0),\n                ((i2 & 4) ? 0xFFFF : 0) | ((i3 & 4) ? 0xFFFF0000 : 0),\n                ((i4 & 4) ? 0 : 0xFFFF) | ((i5 & 4) ? 0 : 0xFFFF0000),\n                ((i6 & 4) ? 0 : 0xFFFF) | ((i7 & 4) ? 0 : 0xFFFF0000) > ();\n            t7 = selectb(t6,t4,t5);  // select between permuted data t4 and t5\n        }\n    }\n    // Set any elements to zero if required\n    if (m2 != -1 && ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) & 0x80)) {\n        // some elements need to be set to 0\n        __m128i mask = constant4i <\n            (i0 < 0 ? 0xFFFF0000 : -1) & (i1 < 0 ? 0x0000FFFF : -1),\n            (i2 < 0 ? 0xFFFF0000 : -1) & (i3 < 0 ? 0x0000FFFF : -1),\n            (i4 < 0 ? 0xFFFF0000 : -1) & (i5 < 0 ? 0x0000FFFF : -1),\n            (i6 < 0 ? 0xFFFF0000 : -1) & (i7 < 0 ? 0x0000FFFF : -1) > ();\n        return  _mm_and_si128(t7,mask);\n    }\n    else {\n        return  t7;\n    }\n#endif\n}\n\ntemplate <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>\nstatic inline Vec8us permute8us(Vec8us const & a) {\n    return Vec8us (permute8s <i0,i1,i2,i3,i4,i5,i6,i7> (a));\n}\n\n\ntemplate <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7, \n          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 > \nstatic inline Vec16c permute16c(Vec16c const & a) {\n\n    __m128i temp;\n\n    // Combine all even indexes into a single bitfield, with 4 bits for each\n    const uint32_t me = (i0&15) | (i2&15)<<4 | (i4&15)<<8 | (i6&15)<<12 \n        | (i8&15)<<16 | (i10&15)<<20 | (i12&15)<<24 | (i14&15)<<28; \n\n    // Combine all odd indexes into a single bitfield, with 4 bits for each\n    const uint32_t mo = (i1&15) | (i3&15)<<4 | (i5&15)<<8 | (i7&15)<<12 \n        | (i9&15)<<16 | (i11&15)<<20 | (i13&15)<<24 | (i15&15)<<28; \n\n    // Mask indicating sign of all even indexes, with 4 bits for each, 0 for negative, 0xF for non-negative\n    const uint32_t se = (i0<0?0:0xF) | (i2<0?0:0xF)<<4 | (i4<0?0:0xF)<<8 | (i6<0?0:0xF)<<12\n        | (i8<0?0:0xF)<<16 | (i10<0?0:0xF)<<20 | (i12<0?0:0xF)<<24 | (i14<0?0:0xF)<<28;\n\n    // Mask indicating sign of all odd indexes, with 4 bits for each, 0 for negative, 0xF for non-negative\n    const uint32_t so = (i1<0?0:0xF) | (i3<0?0:0xF)<<4 | (i5<0?0:0xF)<<8 | (i7<0?0:0xF)<<12\n        | (i9<0?0:0xF)<<16 | (i11<0?0:0xF)<<20 | (i13<0?0:0xF)<<24 | (i15<0?0:0xF)<<28;\n\n    // Mask indicating sign of all indexes, with 2 bits for each, 0 for negative (means set to zero or don't care), 0x3 for non-negative\n    const uint32_t ss = (se & 0x33333333) | (so & 0xCCCCCCCC);\n\n    // Mask indicating required zeroing of all indexes, with 2 bits for each, 0 for index = -1, 3 for index >= 0 or -256\n    const uint32_t ssz = ((i0&0x80)?0:3) | ((i1 &0x80)?0:3)<< 2 | ((i2 &0x80)?0:3)<< 4 | ((i3 &0x80)?0:3)<< 6 | \n                    ((i4 &0x80)?0:3)<< 8 | ((i5 &0x80)?0:3)<<10 | ((i6 &0x80)?0:3)<<12 | ((i7 &0x80)?0:3)<<14 | \n                    ((i8 &0x80)?0:3)<<16 | ((i9 &0x80)?0:3)<<18 | ((i10&0x80)?0:3)<<20 | ((i11&0x80)?0:3)<<22 | \n                    ((i12&0x80)?0:3)<<24 | ((i13&0x80)?0:3)<<26 | ((i14&0x80)?0:3)<<28 | ((i15&0x80)?0:3)<<30 ;\n\n    // These indexes are used only to avoid bogus compiler warnings in false branches\n    const int I0  = i0  > 0 ? (i0  & 0xF) : 0;\n    const int I15 = i15 > 0 ? (i15 & 0xF) : 0;\n\n    // special case: all zero\n    if (ss == 0) {\n        return _mm_setzero_si128();  \n    }\n\n    // remember if extra zeroing is needed\n    bool do_and_zero = (ssz != 0xFFFFFFFFu);\n\n    // check for special shortcut cases\n    int shortcut = 0;\n\n    // check if any permutation\n    if (((me ^ 0xECA86420) & se) == 0 && ((mo ^ 0xFDB97531) & so) == 0) {\n        shortcut = 1;\n    }\n    // check if we can use punpcklbw\n    else if (((me ^ 0x76543210) & se) == 0 && ((mo ^ 0x76543210) & so) == 0) {\n        shortcut = 2;\n    }\n    // check if we can use punpckhbw\n    else if (((me ^ 0xFEDCBA98) & se) == 0 && ((mo ^ 0xFEDCBA98) & so) == 0) {\n        shortcut = 3;\n    }\n\n    #if defined (_MSC_VER) && ! defined(__INTEL_COMPILER)\n    #pragma warning(disable: 4307)  // disable MS warning C4307: '+' : integral constant overflow\n    #endif\n\n    // check if we can use byte shift right\n    else if (i0 > 0 && ((me ^ (uint32_t(I0)*0x11111111u + 0xECA86420u)) & se) == 0 && \n    ((mo ^ (uint32_t(I0)*0x11111111u + 0xFDB97531u)) & so) == 0) {\n        shortcut = 4;\n        do_and_zero = ((0xFFFFFFFFu >> 2*I0) & ~ ssz) != 0;\n    }\n    // check if we can use byte shift left\n    else if (i15 >= 0 && i15 < 15 &&         \n    ((mo ^ (uint32_t(I15*0x11111111u) - (0x02468ACEu & so))) & so) == 0 && \n    ((me ^ (uint32_t(I15*0x11111111u) - (0x13579BDFu & se))) & se) == 0) {\n        shortcut = 5;\n        do_and_zero = ((0xFFFFFFFFu << 2*(15-I15)) & ~ ssz) != 0;\n    }\n\n#if  INSTRSET >= 4  // SSSE3 (PSHUFB available only under SSSE3)\n\n    // special case: rotate\n    if (i0>0 && i0 < 16    && i1==((i0+1)&15) && i2 ==((i0+2 )&15) && i3 ==((i0+3 )&15) && i4 ==((i0+4 )&15) && i5 ==((i0+5 )&15) && i6 ==((i0+6 )&15) && i7 ==((i0+7 )&15) \n    && i8==((i0+8)&15) && i9==((i0+9)&15) && i10==((i0+10)&15) && i11==((i0+11)&15) && i12==((i0+12)&15) && i13==((i0+13)&15) && i14==((i0+14)&15) && i15==((i0+15)&15)) {\n        temp = _mm_alignr_epi8(a, a, i0 & 15);\n        shortcut = -1;\n    }\n    if (shortcut == 0 || do_and_zero) {\n        // general case: use PSHUFB\n        __m128i mask = constant4i< \n            (i0  & 0xFF) | (i1  & 0xFF) << 8 | (i2  & 0xFF) << 16 | (i3  & 0xFF) << 24 ,\n            (i4  & 0xFF) | (i5  & 0xFF) << 8 | (i6  & 0xFF) << 16 | (i7  & 0xFF) << 24 ,\n            (i8  & 0xFF) | (i9  & 0xFF) << 8 | (i10 & 0xFF) << 16 | (i11 & 0xFF) << 24 ,\n            (i12 & 0xFF) | (i13 & 0xFF) << 8 | (i14 & 0xFF) << 16 | (i15 & 0xFF) << 24 > ();\n        temp = _mm_shuffle_epi8(a,mask);\n        shortcut = -1;\n        do_and_zero = false;\n    }\n\n#endif\n\n    // Check if we can use 16-bit permute. Even numbered indexes must be even and odd numbered\n    // indexes must be equal to the preceding index + 1, except for negative indexes.\n    if (shortcut == 0 && (me & 0x11111111 & se) == 0 && ((mo ^ 0x11111111) & 0x11111111 & so) == 0 && ((me ^ mo) & 0xEEEEEEEE & se & so) == 0) {\n        temp = permute8s <\n            i0  >= 0 ? i0 /2 : i1  >= 0 ? i1 /2 : (i0  | i1 ),\n            i2  >= 0 ? i2 /2 : i3  >= 0 ? i3 /2 : (i2  | i3 ),\n            i4  >= 0 ? i4 /2 : i5  >= 0 ? i5 /2 : (i4  | i5 ),\n            i6  >= 0 ? i6 /2 : i7  >= 0 ? i7 /2 : (i6  | i7 ),\n            i8  >= 0 ? i8 /2 : i9  >= 0 ? i9 /2 : (i8  | i9 ),\n            i10 >= 0 ? i10/2 : i11 >= 0 ? i11/2 : (i10 | i11),\n            i12 >= 0 ? i12/2 : i13 >= 0 ? i13/2 : (i12 | i13),\n            i14 >= 0 ? i14/2 : i15 >= 0 ? i15/2 : (i14 | i15) > (Vec8s(a));\n        shortcut = 100;\n        do_and_zero = (se != so && ssz != 0xFFFFFFFFu);\n    }\n  \n    // Check if we can use 16-bit permute with bytes swapped. Even numbered indexes must be odd and odd \n    // numbered indexes must be equal to the preceding index - 1, except for negative indexes.\n    // (this case occurs when reversing byte order)\n    if (shortcut == 0 && ((me ^ 0x11111111) & 0x11111111 & se) == 0 && (mo & 0x11111111 & so) == 0 && ((me ^ mo) & 0xEEEEEEEE & se & so) == 0) {\n        Vec16c swapped = Vec16c(rotate_left(Vec8s(a), 8)); // swap odd and even bytes\n        temp = permute8s <\n            i0  >= 0 ? i0 /2 : i1  >= 0 ? i1 /2 : (i0  | i1 ),\n            i2  >= 0 ? i2 /2 : i3  >= 0 ? i3 /2 : (i2  | i3 ),\n            i4  >= 0 ? i4 /2 : i5  >= 0 ? i5 /2 : (i4  | i5 ),\n            i6  >= 0 ? i6 /2 : i7  >= 0 ? i7 /2 : (i6  | i7 ),\n            i8  >= 0 ? i8 /2 : i9  >= 0 ? i9 /2 : (i8  | i9 ),\n            i10 >= 0 ? i10/2 : i11 >= 0 ? i11/2 : (i10 | i11),\n            i12 >= 0 ? i12/2 : i13 >= 0 ? i13/2 : (i12 | i13),\n            i14 >= 0 ? i14/2 : i15 >= 0 ? i15/2 : (i14 | i15) > (Vec8s(swapped));\n        shortcut = 101;\n        do_and_zero = (se != so && ssz != 0xFFFFFFFFu);\n    }\n\n    // all shortcuts end here\n    if (shortcut) {\n        switch (shortcut) {\n        case 1:\n            temp = a;  break;\n        case 2:\n            temp = _mm_unpacklo_epi8(a,a);  break;\n        case 3:\n            temp = _mm_unpackhi_epi8(a,a);  break;\n        case 4:\n            temp = _mm_srli_si128(a, I0);  break;\n        case 5:\n            temp = _mm_slli_si128(a, 15-I15);  break;\n        default:\n            break;  // result is already in temp\n        }\n        if (do_and_zero) {\n            // additional zeroing needed\n            __m128i maskz = constant4i < \n                (i0  < 0 ? 0 : 0xFF) | (i1  < 0 ? 0 : 0xFF00) | (i2  < 0 ? 0 : 0xFF0000) | (i3  < 0 ? 0 : 0xFF000000) ,\n                (i4  < 0 ? 0 : 0xFF) | (i5  < 0 ? 0 : 0xFF00) | (i6  < 0 ? 0 : 0xFF0000) | (i7  < 0 ? 0 : 0xFF000000) ,\n                (i8  < 0 ? 0 : 0xFF) | (i9  < 0 ? 0 : 0xFF00) | (i10 < 0 ? 0 : 0xFF0000) | (i11 < 0 ? 0 : 0xFF000000) ,\n                (i12 < 0 ? 0 : 0xFF) | (i13 < 0 ? 0 : 0xFF00) | (i14 < 0 ? 0 : 0xFF0000) | (i15 < 0 ? 0 : 0xFF000000) > ();\n            temp = _mm_and_si128(temp, maskz);\n        }\n        return temp;\n    }\n\n    // complicated cases: use 16-bit permute up to four times\n    const bool e2e = (~me & 0x11111111 & se) != 0;  // even bytes of source to even bytes of destination\n    const bool e2o = (~mo & 0x11111111 & so) != 0;  // even bytes of source to odd  bytes of destination\n    const bool o2e = (me  & 0x11111111 & se) != 0;  // odd  bytes of source to even bytes of destination\n    const bool o2o = (mo  & 0x11111111 & so) != 0;  // odd  bytes of source to odd  bytes of destination\n    \n    Vec16c swapped, te2e, te2o, to2e, to2o, combeven, combodd;\n\n    if (e2o || o2e) swapped = rotate_left(Vec8s(a), 8); // swap odd and even bytes\n\n    // even-to-even bytes\n    if (e2e) te2e = permute8s <(i0&1)?-1:i0/2, (i2&1)?-1:i2/2, (i4&1)?-1:i4/2, (i6&1)?-1:i6/2,\n        (i8&1)?-1:i8/2, (i10&1)?-1:i10/2, (i12&1)?-1:i12/2, (i14&1)?-1:i14/2> (Vec8s(a));                 \n    // odd-to-even bytes\n    if (o2e) to2e = permute8s <(i0&1)?i0/2:-1, (i2&1)?i2/2:-1, (i4&1)?i4/2:-1, (i6&1)?i6/2:-1,\n        (i8&1)?i8/2:-1, (i10&1)?i10/2:-1, (i12&1)?i12/2:-1, (i14&1)?i14/2:-1> (Vec8s(swapped));\n    // even-to-odd bytes\n    if (e2o) te2o = permute8s <(i1&1)?-1:i1/2, (i3&1)?-1:i3/2, (i5&1)?-1:i5/2, (i7&1)?-1:i7/2, \n        (i9&1)?-1:i9/2, (i11&1)?-1:i11/2, (i13&1)?-1:i13/2, (i15&1)?-1:i15/2> (Vec8s(swapped));\n    // odd-to-odd bytes\n    if (o2o) to2o = permute8s <(i1&1)?i1/2:-1, (i3&1)?i3/2:-1, (i5&1)?i5/2:-1, (i7&1)?i7/2:-1,\n        (i9&1)?i9/2:-1, (i11&1)?i11/2:-1, (i13&1)?i13/2:-1, (i15&1)?i15/2:-1> (Vec8s(a));\n\n    if (e2e && o2e) combeven = te2e | to2e;\n    else if (e2e)   combeven = te2e;\n    else if (o2e)   combeven = to2e;\n    else            combeven = _mm_setzero_si128();\n\n    if (e2o && o2o) combodd  = te2o | to2o;\n    else if (e2o)   combodd  = te2o;\n    else if (o2o)   combodd  = to2o;\n    else            combodd  = _mm_setzero_si128();\n\n    __m128i maske = constant4i <     // mask used even bytes\n        (i0  < 0 ? 0 : 0xFF) | (i2  < 0 ? 0 : 0xFF0000),\n        (i4  < 0 ? 0 : 0xFF) | (i6  < 0 ? 0 : 0xFF0000),\n        (i8  < 0 ? 0 : 0xFF) | (i10 < 0 ? 0 : 0xFF0000),\n        (i12 < 0 ? 0 : 0xFF) | (i14 < 0 ? 0 : 0xFF0000) > ();\n    __m128i masko = constant4i <     // mask used odd bytes\n        (i1  < 0 ? 0 : 0xFF00) | (i3  < 0 ? 0 : 0xFF000000),\n        (i5  < 0 ? 0 : 0xFF00) | (i7  < 0 ? 0 : 0xFF000000),\n        (i9  < 0 ? 0 : 0xFF00) | (i11 < 0 ? 0 : 0xFF000000),\n        (i13 < 0 ? 0 : 0xFF00) | (i15 < 0 ? 0 : 0xFF000000) > ();\n\n    return  _mm_or_si128(            // combine even and odd bytes\n        _mm_and_si128(combeven, maske),\n        _mm_and_si128(combodd, masko));\n}\n\ntemplate <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7, \n          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 > \nstatic inline Vec16uc permute16uc(Vec16uc const & a) {\n    return Vec16uc (permute16c <i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a));\n}\n\n\n/*****************************************************************************\n*\n*          Vector blend functions\n*\n******************************************************************************\n*\n* These blend functions can mix elements from two different vectors and\n* optionally set some elements to zero. \n*\n* The indexes are inserted as template parameters in <>. These indexes must be\n* constants. Each template parameter is an index to the element you want to \n* select, where higher indexes indicate an element from the second source\n* vector. For example, if each vector has 4 elements, then indexes 0 - 3\n* will select an element from the first vector and indexes 4 - 7 will select \n* an element from the second vector. A negative index will generate zero.\n*\n* The blend functions for vectors of 8-bit integers are inefficient if \n* the SSSE3 instruction set or later is not enabled.\n*\n* Example:\n* Vec4i a(100,101,102,103);         // a is (100, 101, 102, 103)\n* Vec4i b(200,201,202,203);         // b is (200, 201, 202, 203)\n* Vec4i c;\n* c = blend4i<1,4,-1,7> (a,b);      // c is (101, 200,   0, 203)\n*\n* A lot of the code here is metaprogramming aiming to find the instructions\n* that best fit the template parameters and instruction set. The metacode\n* will be reduced out to leave only a few vector instructions in release\n* mode with optimization on.\n*****************************************************************************/\n\ntemplate <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7, \n          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 > \nstatic inline Vec16c blend16c(Vec16c const & a, Vec16c const & b) {\n\n    // Combine bit 0-3 of all even indexes into a single bitfield, with 4 bits for each\n    const int me = (i0&15) | (i2&15)<<4 | (i4&15)<<8 | (i6&15)<<12 \n        | (i8&15)<<16 | (i10&15)<<20 | (i12&15)<<24 | (i14&15)<<28; \n\n    // Combine bit 0-3 of all odd indexes into a single bitfield, with 4 bits for each\n    const int mo = (i1&15) | (i3&15)<<4 | (i5&15)<<8 | (i7&15)<<12 \n        | (i9&15)<<16 | (i11&15)<<20 | (i13&15)<<24 | (i15&15)<<28; \n\n    // Mask indicating sign of all even indexes, with 4 bits for each, 0 for negative, 0xF for non-negative\n    const int se = (i0<0?0:0xF) | (i2<0?0:0xF)<<4 | (i4<0?0:0xF)<<8 | (i6<0?0:0xF)<<12\n        | (i8<0?0:0xF)<<16 | (i10<0?0:0xF)<<20 | (i12<0?0:0xF)<<24 | (i14<0?0:0xF)<<28;\n\n    // Mask indicating sign of all odd indexes, with 4 bits for each, 0 for negative, 0xF for non-negative\n    const int so = (i1<0?0:0xF) | (i3<0?0:0xF)<<4 | (i5<0?0:0xF)<<8 | (i7<0?0:0xF)<<12\n        | (i9<0?0:0xF)<<16 | (i11<0?0:0xF)<<20 | (i13<0?0:0xF)<<24 | (i15<0?0:0xF)<<28;\n\n    // Combine bit 4 of all even indexes into a single bitfield, with 4 bits for each\n    const int ne = (i0&16)>>4 | (i2&16) | (i4&16)<<4 | (i6&16)<<8 \n        | (i8&16)<<12 | (i10&16)<<16 | (i12&16)<<20 | (i14&16)<<24; \n\n    // Combine bit 4 of all odd indexes into a single bitfield, with 4 bits for each\n    const int no = (i1&16)>>4 | (i3&16) | (i5&16)<<4 | (i7&16)<<8\n        | (i9&16)<<12 | (i11&16)<<16 | (i13&16)<<20 | (i15&16)<<24; \n\n    // Check if zeroing needed\n    const bool do_zero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15) & 0x80) != 0; // needs zeroing\n\n    // no elements from b\n    if (((ne & se) | (no & so)) == 0) {\n        return permute16c <i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (a);\n    }\n\n    // no elements from a\n    if ((((ne^0x11111111) & se) | ((no^0x11111111) & so)) == 0) {\n        return permute16c <i0^16, i1^16, i2^16, i3^16, i4^16, i5^16, i6^16, i7^16, i8^16, i9^16, i10^16, i11^16, i12^16, i13^16, i14^16, i15^16> (b);\n    }\n    __m128i t;\n\n    // check if we can use punpcklbw\n    if (((me ^ 0x76543210) & se) == 0 && ((mo ^ 0x76543210) & so) == 0) {\n        if ((ne & se) == 0 && ((no ^ 0x11111111) & so) == 0) {        \n            t = _mm_unpacklo_epi8(a,b);\n        }\n        if ((no & so) == 0 && ((ne ^ 0x11111111) & se) == 0) {        \n            t = _mm_unpacklo_epi8(b,a);\n        }\n        if (do_zero) {\n            // additional zeroing needed\n            __m128i maskz = constant4i < \n                (i0  < 0 ? 0 : 0xFF) | (i1  < 0 ? 0 : 0xFF00) | (i2  < 0 ? 0 : 0xFF0000) | (i3  < 0 ? 0 : 0xFF000000) ,\n                (i4  < 0 ? 0 : 0xFF) | (i5  < 0 ? 0 : 0xFF00) | (i6  < 0 ? 0 : 0xFF0000) | (i7  < 0 ? 0 : 0xFF000000) ,\n                (i8  < 0 ? 0 : 0xFF) | (i9  < 0 ? 0 : 0xFF00) | (i10 < 0 ? 0 : 0xFF0000) | (i11 < 0 ? 0 : 0xFF000000) ,\n                (i12 < 0 ? 0 : 0xFF) | (i13 < 0 ? 0 : 0xFF00) | (i14 < 0 ? 0 : 0xFF0000) | (i15 < 0 ? 0 : 0xFF000000) > ();\n            t = _mm_and_si128(t, maskz);\n        }\n        return t;\n    }\n\n    // check if we can use punpckhbw\n    if (((me ^ 0xFEDCBA98) & se) == 0 && ((mo ^ 0xFEDCBA98) & so) == 0) {\n        if ((ne & se) == 0 && ((no ^ 0x11111111) & so) == 0) {        \n            t = _mm_unpackhi_epi8(a,b);\n        }\n        if ((no & so) == 0 && ((ne ^ 0x11111111) & se) == 0) {        \n            t = _mm_unpackhi_epi8(b,a);\n        }\n        if (do_zero) {\n            // additional zeroing needed\n            __m128i maskz = constant4i < \n                (i0  < 0 ? 0 : 0xFF) | (i1  < 0 ? 0 : 0xFF00) | (i2  < 0 ? 0 : 0xFF0000) | (i3  < 0 ? 0 : 0xFF000000) ,\n                (i4  < 0 ? 0 : 0xFF) | (i5  < 0 ? 0 : 0xFF00) | (i6  < 0 ? 0 : 0xFF0000) | (i7  < 0 ? 0 : 0xFF000000) ,\n                (i8  < 0 ? 0 : 0xFF) | (i9  < 0 ? 0 : 0xFF00) | (i10 < 0 ? 0 : 0xFF0000) | (i11 < 0 ? 0 : 0xFF000000) ,\n                (i12 < 0 ? 0 : 0xFF) | (i13 < 0 ? 0 : 0xFF00) | (i14 < 0 ? 0 : 0xFF0000) | (i15 < 0 ? 0 : 0xFF000000) > ();\n            t = _mm_and_si128(t, maskz);\n        }\n        return t;\n    }\n    \n#if  INSTRSET >= 4  // SSSE3\n    // special case: shift left\n    if (i0 > 0 && i0 < 16 && i1==i0+1 && i2==i0+2 && i3==i0+3 && i4==i0+4 && i5==i0+5 && i6==i0+6 && i7==i0+7 && \n        i8==i0+8 && i9==i0+9 && i10==i0+10 && i11==i0+11 && i12==i0+12 && i13==i0+13 && i14==i0+14 && i15==i0+15) {\n        return _mm_alignr_epi8(b, a, (i0 & 15));\n    }\n\n    // special case: shift right\n    if (i0 > 15 && i0 < 32 && i1==((i0+1)&31) && i2 ==((i0+2 )&31) && i3 ==((i0+3 )&31) && i4 ==((i0+4 )&31) && i5 ==((i0+5 )&31) && i6 ==((i0+6 )&31) && i7 ==((i0+7 )&31) && \n        i8==((i0+8 )&31)   && i9==((i0+9)&31) && i10==((i0+10)&31) && i11==((i0+11)&31) && i12==((i0+12)&31) && i13==((i0+13)&31) && i14==((i0+14)&31) && i15==((i0+15)&31)) {\n        return _mm_alignr_epi8(a, b, (i0 & 15));\n    }\n#endif\n\n#if INSTRSET >= 5   // SSE4.1 supported\n    // special case: blend without permute\n    if (((me ^ 0xECA86420) & se) == 0 && ((mo ^ 0xFDB97531) & so) == 0) {\n        __m128i maskbl = constant4i<\n            ((i0 & 16) ? 0xFF : 0) | ((i1 & 16) ? 0xFF00 : 0) | ((i2 & 16) ? 0xFF0000 : 0) | ((i3 & 16) ? 0xFF000000 : 0) ,\n            ((i4 & 16) ? 0xFF : 0) | ((i5 & 16) ? 0xFF00 : 0) | ((i6 & 16) ? 0xFF0000 : 0) | ((i7 & 16) ? 0xFF000000 : 0) ,\n            ((i8 & 16) ? 0xFF : 0) | ((i9 & 16) ? 0xFF00 : 0) | ((i10& 16) ? 0xFF0000 : 0) | ((i11& 16) ? 0xFF000000 : 0) ,\n            ((i12& 16) ? 0xFF : 0) | ((i13& 16) ? 0xFF00 : 0) | ((i14& 16) ? 0xFF0000 : 0) | ((i15& 16) ? 0xFF000000 : 0) > ();\n        t = _mm_blendv_epi8(a, b, maskbl);\n        if (do_zero) {\n            // additional zeroing needed\n            __m128i maskz = constant4i < \n                (i0  < 0 ? 0 : 0xFF) | (i1  < 0 ? 0 : 0xFF00) | (i2  < 0 ? 0 : 0xFF0000) | (i3  < 0 ? 0 : 0xFF000000) ,\n                (i4  < 0 ? 0 : 0xFF) | (i5  < 0 ? 0 : 0xFF00) | (i6  < 0 ? 0 : 0xFF0000) | (i7  < 0 ? 0 : 0xFF000000) ,\n                (i8  < 0 ? 0 : 0xFF) | (i9  < 0 ? 0 : 0xFF00) | (i10 < 0 ? 0 : 0xFF0000) | (i11 < 0 ? 0 : 0xFF000000) ,\n                (i12 < 0 ? 0 : 0xFF) | (i13 < 0 ? 0 : 0xFF00) | (i14 < 0 ? 0 : 0xFF0000) | (i15 < 0 ? 0 : 0xFF000000) > ();\n            t = _mm_and_si128(t, maskz);\n        }\n        return t;\n    }\n#endif // SSE4.1\n\n#if defined ( __XOP__ )    // Use AMD XOP instruction VPPERM\n    __m128i mask = constant4i<\n        (i0 <0 ? 0x80 : (i0 &31)) | (i1 <0 ? 0x80 : (i1 &31)) << 8 | (i2 <0 ? 0x80 : (i2 &31)) << 16 | (i3 <0 ? 0x80 : (i3 &31)) << 24,\n        (i4 <0 ? 0x80 : (i4 &31)) | (i5 <0 ? 0x80 : (i5 &31)) << 8 | (i6 <0 ? 0x80 : (i6 &31)) << 16 | (i7 <0 ? 0x80 : (i7 &31)) << 24,\n        (i8 <0 ? 0x80 : (i8 &31)) | (i9 <0 ? 0x80 : (i9 &31)) << 8 | (i10<0 ? 0x80 : (i10&31)) << 16 | (i11<0 ? 0x80 : (i11&31)) << 24,\n        (i12<0 ? 0x80 : (i12&31)) | (i13<0 ? 0x80 : (i13&31)) << 8 | (i14<0 ? 0x80 : (i14&31)) << 16 | (i15<0 ? 0x80 : (i15&31)) << 24 > ();\n    return _mm_perm_epi8(a, b, mask);\n\n#elif  INSTRSET >= 4  // SSSE3\n   \n    // general case. Use PSHUFB\n    __m128i maska = constant4i<\n        ((i0 & 0x90) ? 0xFF : (i0 &15)) | ((i1 & 0x90) ? 0xFF : (i1 &15)) << 8 | ((i2 & 0x90) ? 0xFF : (i2 &15)) << 16 | ((i3 & 0x90) ? 0xFF : (i3 &15)) << 24,\n        ((i4 & 0x90) ? 0xFF : (i4 &15)) | ((i5 & 0x90) ? 0xFF : (i5 &15)) << 8 | ((i6 & 0x90) ? 0xFF : (i6 &15)) << 16 | ((i7 & 0x90) ? 0xFF : (i7 &15)) << 24,\n        ((i8 & 0x90) ? 0xFF : (i8 &15)) | ((i9 & 0x90) ? 0xFF : (i9 &15)) << 8 | ((i10& 0x90) ? 0xFF : (i10&15)) << 16 | ((i11& 0x90) ? 0xFF : (i11&15)) << 24,\n        ((i12& 0x90) ? 0xFF : (i12&15)) | ((i13& 0x90) ? 0xFF : (i13&15)) << 8 | ((i14& 0x90) ? 0xFF : (i14&15)) << 16 | ((i15& 0x90) ? 0xFF : (i15&15)) << 24 > ();\n    __m128i maskb = constant4i<\n        (((i0^0x10) & 0x90) ? 0xFF : (i0 &15)) | (((i1^0x10) & 0x90) ? 0xFF : (i1 &15)) << 8 | (((i2^0x10) & 0x90) ? 0xFF : (i2 &15)) << 16 | (((i3^0x10) & 0x90) ? 0xFF : (i3 &15)) << 24,\n        (((i4^0x10) & 0x90) ? 0xFF : (i4 &15)) | (((i5^0x10) & 0x90) ? 0xFF : (i5 &15)) << 8 | (((i6^0x10) & 0x90) ? 0xFF : (i6 &15)) << 16 | (((i7^0x10) & 0x90) ? 0xFF : (i7 &15)) << 24,\n        (((i8^0x10) & 0x90) ? 0xFF : (i8 &15)) | (((i9^0x10) & 0x90) ? 0xFF : (i9 &15)) << 8 | (((i10^0x10)& 0x90) ? 0xFF : (i10&15)) << 16 | (((i11^0x10)& 0x90) ? 0xFF : (i11&15)) << 24,\n        (((i12^0x10)& 0x90) ? 0xFF : (i12&15)) | (((i13^0x10)& 0x90) ? 0xFF : (i13&15)) << 8 | (((i14^0x10)& 0x90) ? 0xFF : (i14&15)) << 16 | (((i15^0x10)& 0x90) ? 0xFF : (i15&15)) << 24 > ();\n    __m128i a1 = _mm_shuffle_epi8(a,maska);\n    __m128i b1 = _mm_shuffle_epi8(b,maskb);\n    return       _mm_or_si128(a1,b1);\n\n#else                 // SSE2\n    // combine two permutes\n    __m128i a1 = permute16c <\n        (uint32_t)i0  < 16 ? i0  : -1,\n        (uint32_t)i1  < 16 ? i1  : -1,\n        (uint32_t)i2  < 16 ? i2  : -1,\n        (uint32_t)i3  < 16 ? i3  : -1,\n        (uint32_t)i4  < 16 ? i4  : -1,\n        (uint32_t)i5  < 16 ? i5  : -1,\n        (uint32_t)i6  < 16 ? i6  : -1,\n        (uint32_t)i7  < 16 ? i7  : -1,\n        (uint32_t)i8  < 16 ? i8  : -1,\n        (uint32_t)i9  < 16 ? i9  : -1,\n        (uint32_t)i10 < 16 ? i10 : -1,\n        (uint32_t)i11 < 16 ? i11 : -1,\n        (uint32_t)i12 < 16 ? i12 : -1,\n        (uint32_t)i13 < 16 ? i13 : -1,\n        (uint32_t)i14 < 16 ? i14 : -1,\n        (uint32_t)i15 < 16 ? i15 : -1 > (a);\n    __m128i b1 = permute16c <\n        (uint32_t)(i0 ^16) < 16 ? (i0 ^16) : -1,\n        (uint32_t)(i1 ^16) < 16 ? (i1 ^16) : -1,\n        (uint32_t)(i2 ^16) < 16 ? (i2 ^16) : -1,\n        (uint32_t)(i3 ^16) < 16 ? (i3 ^16) : -1,\n        (uint32_t)(i4 ^16) < 16 ? (i4 ^16) : -1,\n        (uint32_t)(i5 ^16) < 16 ? (i5 ^16) : -1,\n        (uint32_t)(i6 ^16) < 16 ? (i6 ^16) : -1,\n        (uint32_t)(i7 ^16) < 16 ? (i7 ^16) : -1,        \n        (uint32_t)(i8 ^16) < 16 ? (i8 ^16) : -1,\n        (uint32_t)(i9 ^16) < 16 ? (i9 ^16) : -1,\n        (uint32_t)(i10^16) < 16 ? (i10^16) : -1,\n        (uint32_t)(i11^16) < 16 ? (i11^16) : -1,\n        (uint32_t)(i12^16) < 16 ? (i12^16) : -1,\n        (uint32_t)(i13^16) < 16 ? (i13^16) : -1,\n        (uint32_t)(i14^16) < 16 ? (i14^16) : -1,\n        (uint32_t)(i15^16) < 16 ? (i15^16) : -1 > (b);\n    return   _mm_or_si128(a1,b1);\n\n#endif\n}\n\ntemplate <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7, \n          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 > \nstatic inline Vec16uc blend16uc(Vec16uc const & a, Vec16uc const & b) {\n    return Vec16uc( blend16c<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a,b));\n}\n\n\ntemplate <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>\nstatic inline Vec8s blend8s(Vec8s const & a, Vec8s const & b) {\n\n    // Combine all the indexes into a single bitfield, with 4 bits for each\n    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 \n        | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28; \n\n    // Mask to zero out negative indexes\n    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12\n        | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;\n\n    // Some elements must be set to zero\n    const bool do_zero = (mz != -1) && ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) & 0x80) != 0;\n\n    // temp contains temporary result, some zeroing needs to be done\n    bool zeroing_pending = false;\n\n    // partially finished result\n    __m128i temp;\n\n    if ((m1 & 0x88888888 & mz) == 0) {\n        // no elements from b\n        return permute8s <i0, i1, i2, i3, i4, i5, i6, i7> (a);\n    }\n\n    if (((m1^0x88888888) & 0x88888888 & mz) == 0) {\n        // no elements from a\n        return permute8s <i0&~8, i1&~8, i2&~8, i3&~8, i4&~8, i5&~8, i6&~8, i7&~8> (b);\n    }\n\n    // special case: PUNPCKLWD \n    if (((m1 ^ 0xB3A29180) & mz) == 0) {\n        temp = _mm_unpacklo_epi16(a, b);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n    if (((m1 ^ 0x3B2A1908) & mz) == 0) {\n        temp = _mm_unpacklo_epi16(b, a);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n    // special case: PUNPCKHWD \n    if (((m1 ^ 0xF7E6D5C4) & mz) == 0) {\n        temp = _mm_unpackhi_epi16(a, b);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n    if (((m1 ^ 0x7F6E5D4C) & mz) == 0) {\n        temp = _mm_unpackhi_epi16(b, a);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n\n#if  INSTRSET >= 4  // SSSE3\n    // special case: shift left\n    if (i0 > 0 && i0 < 8 && ((m1 ^ ((i0 & 7) * 0x11111111u + 0x76543210u)) & mz) == 0) {\n        temp = _mm_alignr_epi8(b, a, (i0 & 7) * 2);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n\n    // special case: shift right\n    if (i0 > 8 && i0 < 16 && ((m1 ^ 0x88888888 ^ ((i0 & 7) * 0x11111111u + 0x76543210u)) & mz) == 0) {\n        temp = _mm_alignr_epi8(a, b, (i0 & 7) * 2);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n#endif // SSSE3\n\n#if INSTRSET >= 5   // SSE4.1 supported\n    // special case: blending without permuting\n    if ((((m1 & ~0x88888888) ^ 0x76543210) & mz) == 0) {\n        temp = _mm_blend_epi16(a, b, (i0>>3&1) | (i1>>3&1)<<1 | (i2>>3&1)<<2 | (i3>>3&1)<<3 \n            | (i4>>3&1)<<4 | (i5>>3&1)<<5 | (i6>>3&1)<<6 | (i7>>3&1)<<7);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n#endif // SSE4.1\n\n    if (zeroing_pending) {\n        // additional zeroing of temp needed\n        __m128i maskz = constant4i < \n            (i0 < 0 ? 0 : 0xFFFF) | (i1 < 0 ? 0 : 0xFFFF0000) ,\n            (i2 < 0 ? 0 : 0xFFFF) | (i3 < 0 ? 0 : 0xFFFF0000) ,\n            (i4 < 0 ? 0 : 0xFFFF) | (i5 < 0 ? 0 : 0xFFFF0000) ,\n            (i6 < 0 ? 0 : 0xFFFF) | (i7 < 0 ? 0 : 0xFFFF0000) > ();\n        return _mm_and_si128(temp, maskz);\n    }        \n\n    // general case\n#ifdef __XOP__     // Use AMD XOP instruction PPERM\n    __m128i mask = constant4i <\n        (i0 < 0 ? 0x8080 : (i0*2 & 31) | ((i0*2 & 31)+1)<<8) | (i1 < 0 ? 0x80800000 : ((i1*2 & 31)<<16) | ((i1*2 & 31)+1)<<24),\n        (i2 < 0 ? 0x8080 : (i2*2 & 31) | ((i2*2 & 31)+1)<<8) | (i3 < 0 ? 0x80800000 : ((i3*2 & 31)<<16) | ((i3*2 & 31)+1)<<24),\n        (i4 < 0 ? 0x8080 : (i4*2 & 31) | ((i4*2 & 31)+1)<<8) | (i5 < 0 ? 0x80800000 : ((i5*2 & 31)<<16) | ((i5*2 & 31)+1)<<24),\n        (i6 < 0 ? 0x8080 : (i6*2 & 31) | ((i6*2 & 31)+1)<<8) | (i7 < 0 ? 0x80800000 : ((i7*2 & 31)<<16) | ((i7*2 & 31)+1)<<24) > ();\n    return _mm_perm_epi8(a, b, mask);\n#else  \n    // combine two permutes\n    __m128i a1 = permute8s <\n        (uint32_t)i0 < 8 ? i0 : -1,\n        (uint32_t)i1 < 8 ? i1 : -1,\n        (uint32_t)i2 < 8 ? i2 : -1,\n        (uint32_t)i3 < 8 ? i3 : -1,\n        (uint32_t)i4 < 8 ? i4 : -1,\n        (uint32_t)i5 < 8 ? i5 : -1,\n        (uint32_t)i6 < 8 ? i6 : -1,\n        (uint32_t)i7 < 8 ? i7 : -1 > (a);\n    __m128i b1 = permute8s <\n        (uint32_t)(i0^8) < 8 ? (i0^8) : -1,\n        (uint32_t)(i1^8) < 8 ? (i1^8) : -1,\n        (uint32_t)(i2^8) < 8 ? (i2^8) : -1,\n        (uint32_t)(i3^8) < 8 ? (i3^8) : -1,\n        (uint32_t)(i4^8) < 8 ? (i4^8) : -1,\n        (uint32_t)(i5^8) < 8 ? (i5^8) : -1,\n        (uint32_t)(i6^8) < 8 ? (i6^8) : -1,\n        (uint32_t)(i7^8) < 8 ? (i7^8) : -1 > (b);\n    return   _mm_or_si128(a1,b1);\n\n#endif\n}\n\ntemplate <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>\nstatic inline Vec8us blend8us(Vec8us const & a, Vec8us const & b) {\n    return Vec8us(blend8s<i0,i1,i2,i3,i4,i5,i6,i7> (a,b));\n}\n\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline Vec4i blend4i(Vec4i const & a, Vec4i const & b) {\n\n    // Combine all the indexes into a single bitfield, with 8 bits for each\n    const int m1 = (i0 & 7) | (i1 & 7) << 8 | (i2 & 7) << 16 | (i3 & 7) << 24; \n\n    // Mask to zero out negative indexes\n    const int mz = (i0 < 0 ? 0 : 0xFF) | (i1 < 0 ? 0 : 0xFF) << 8 | (i2 < 0 ? 0 : 0xFF) << 16 | (i3 < 0 ? 0 : 0xFF) << 24;\n\n    // Some elements must be set to zero\n    const bool do_zero = (mz != -1) && ((i0 | i1 | i2 | i3) & 0x80) != 0;\n\n    // temp contains temporary result, some zeroing needs to be done\n    bool zeroing_pending = false;\n\n    // partially finished result\n    __m128i temp;\n#if defined (_MSC_VER) || defined (__clang__)\n    temp = a;  // avoid spurious warning message for temp unused\n#endif\n\n    // special case: no elements from b\n    if ((m1 & 0x04040404 & mz) == 0) {\n        return permute4i<i0,i1,i2,i3>(a);\n    }\n\n    // special case: no elements from a\n    if (((m1^0x04040404) & 0x04040404 & mz) == 0) {\n        return permute4i<i0&~4, i1&~4, i2&~4, i3&~4>(b);\n    }\n\n    // special case: PUNPCKLDQ\n    if (((m1 ^ 0x05010400) & mz) == 0) {\n        temp = _mm_unpacklo_epi32(a, b);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n    if (((m1 ^ 0x01050004) & mz) == 0) {\n        temp = _mm_unpacklo_epi32(b, a);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n\n    // special case: PUNPCKHDQ \n    if (((m1 ^ 0x07030602) & mz) == 0) {\n        temp = _mm_unpackhi_epi32(a, b);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n    if (((m1 ^ 0x03070206) & mz) == 0) {\n        temp = _mm_unpackhi_epi32(b, a);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n\n#if  INSTRSET >= 4  // SSSE3\n    // special case: shift left\n    if (i0 > 0 && i0 < 4 && ((m1 ^ ((i0 & 3) * 0x01010101u + 0x03020100u)) & mz) == 0) {\n        temp = _mm_alignr_epi8(b, a, (i0 & 3) * 4);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n\n    // special case: shift right\n    if (i0 > 4 && i0 < 8 && ((m1 ^ 0x04040404 ^ ((i0 & 3) * 0x01010101u + 0x03020100u)) & mz) == 0) {\n        temp = _mm_alignr_epi8(a, b, (i0 & 3) * 4);\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n#endif // SSSE3\n\n#if INSTRSET >= 5   // SSE4.1 supported\n    if ((((m1 & ~0x04040404) ^ 0x03020100) & mz) == 0) {\n        // blending without permuting\n        temp = _mm_blend_epi16(a, b, ((i0>>2)&1)*3 | ((((i1>>2)&1)*3)<<2) | ((((i2>>2)&1)*3)<<4) | ((((i3>>2)&1)*3)<<6));\n        if (do_zero) zeroing_pending = true; else return temp;\n    }\n#endif // SSE4.1\n\n    if (zeroing_pending) {\n        // additional zeroing of temp needed\n        __m128i maskz = constant4i < (i0 < 0 ? 0 : -1), (i1 < 0 ? 0 : -1), (i2 < 0 ? 0 : -1), (i3 < 0 ? 0 : -1) > ();\n        return _mm_and_si128(temp, maskz);\n    }        \n\n    // general case\n#ifdef __XOP__     // Use AMD XOP instruction PPERM\n    __m128i mask = constant4i <\n        i0 < 0 ? 0x80808080 : (i0*4 & 31) + (((i0*4 & 31) + 1) << 8) + (((i0*4 & 31) + 2) << 16) + (((i0*4 & 31) + 3) << 24),\n        i1 < 0 ? 0x80808080 : (i1*4 & 31) + (((i1*4 & 31) + 1) << 8) + (((i1*4 & 31) + 2) << 16) + (((i1*4 & 31) + 3) << 24),\n        i2 < 0 ? 0x80808080 : (i2*4 & 31) + (((i2*4 & 31) + 1) << 8) + (((i2*4 & 31) + 2) << 16) + (((i2*4 & 31) + 3) << 24),\n        i3 < 0 ? 0x80808080 : (i3*4 & 31) + (((i3*4 & 31) + 1) << 8) + (((i3*4 & 31) + 2) << 16) + (((i3*4 & 31) + 3) << 24) > ();\n    return _mm_perm_epi8(a, b, mask);\n\n#else  // combine two permutes\n    __m128i a1 = permute4i <\n        (uint32_t)i0 < 4 ? i0 : -1,\n        (uint32_t)i1 < 4 ? i1 : -1,\n        (uint32_t)i2 < 4 ? i2 : -1,\n        (uint32_t)i3 < 4 ? i3 : -1  > (a);\n    __m128i b1 = permute4i <\n        (uint32_t)(i0^4) < 4 ? (i0^4) : -1,\n        (uint32_t)(i1^4) < 4 ? (i1^4) : -1,\n        (uint32_t)(i2^4) < 4 ? (i2^4) : -1,\n        (uint32_t)(i3^4) < 4 ? (i3^4) : -1  > (b);\n    return  _mm_or_si128(a1,b1);\n#endif\n}\n\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline Vec4ui blend4ui(Vec4ui const & a, Vec4ui const & b) {\n    return Vec4ui (blend4i<i0,i1,i2,i3> (a,b));\n}\n\ntemplate <int i0, int i1>\nstatic inline Vec2q blend2q(Vec2q const & a, Vec2q const & b) {\n\n    // Combine all the indexes into a single bitfield, with 8 bits for each\n    const int m1 = (i0&3) | (i1&3)<<8; \n\n    // Mask to zero out negative indexes\n    const int mz = (i0 < 0 ? 0 : 0xFF) | (i1 < 0 ? 0 : 0xFF) << 8;\n\n    // no elements from b\n    if ((m1 & 0x0202 & mz) == 0) {\n        return permute2q <i0, i1> (a);\n    }\n    // no elements from a\n    if (((m1^0x0202) & 0x0202 & mz) == 0) {\n        return permute2q <i0 & ~2, i1 & ~2> (b);\n    }\n    // (all cases where one index is -1 or -256 would go to the above cases)\n\n    // special case: PUNPCKLQDQ \n    if (i0 == 0 && i1 == 2) {\n        return _mm_unpacklo_epi64(a, b);\n    }\n    if (i0 == 2 && i1 == 0) {\n        return _mm_unpacklo_epi64(b, a);\n    }\n    // special case: PUNPCKHQDQ \n    if (i0 == 1 && i1 == 3) {\n        return _mm_unpackhi_epi64(a, b);\n    }\n    if (i0 == 3 && i1 == 1) {\n        return _mm_unpackhi_epi64(b, a);\n    }\n\n#if  INSTRSET >= 4  // SSSE3\n    // special case: shift left\n    if (i0 == 1 && i1 == 2) {\n        return _mm_alignr_epi8(b, a, 8);\n    }\n    // special case: shift right\n    if (i0 == 3 && i1 == 0) {\n        return _mm_alignr_epi8(a, b, 8);\n    }\n#endif // SSSE3\n\n#if INSTRSET >= 5   // SSE4.1 supported\n    if (((m1 & ~0x0202) ^ 0x0100) == 0 && mz == 0xFFFF) {\n        // blending without permuting\n        return _mm_blend_epi16(a, b, (i0>>1 & 1) * 0xF | ((i1>>1 & 1) * 0xF) << 4 );\n    }\n#endif // SSE4.1\n\n    // general case. combine two permutes \n    // (all cases are caught by the above special cases if SSE4.1 or higher is supported)\n    __m128i a1, b1;\n    a1 = permute2q <(uint32_t)i0 < 2 ? i0 : -1, (uint32_t)i1 < 2 ? i1 : -1 > (a);\n    b1 = permute2q <(uint32_t)(i0^2) < 2 ? (i0^2) : -1, (uint32_t)(i1^2) < 2 ? (i1^2) : -1 > (b);\n    return  _mm_or_si128(a1,b1);\n}\n\ntemplate <int i0, int i1>\nstatic inline Vec2uq blend2uq(Vec2uq const & a, Vec2uq const & b) {\n    return Vec2uq (blend2q <i0, i1> ((__m128i)a, (__m128i)b));\n}\n\n\n\n/*****************************************************************************\n*\n*          Vector lookup functions\n*\n******************************************************************************\n*\n* These functions use vector elements as indexes into a table.\n* The table is given as one or more vectors or as an array.\n*\n* This can be used for several purposes:\n*  - table lookup\n*  - permute or blend with variable indexes\n*  - blend from more than two sources\n*  - gather non-contiguous data\n*\n* An index out of range may produce any value - the actual value produced is\n* implementation dependent and may be different for different instruction\n* sets. An index out of range does not produce an error message or exception.\n*\n* Example:\n* Vec4i a(2,0,0,3);           // index a is (  2,   0,   0,   3)\n* Vec4i b(100,101,102,103);   // table b is (100, 101, 102, 103)\n* Vec4i c;\n* c = lookup4 (a,b);          // c is (102, 100, 100, 103)\n*\n*****************************************************************************/\n\nstatic inline Vec16c lookup16(Vec16c const & index, Vec16c const & table) {\n#if INSTRSET >= 5  // SSSE3\n    return _mm_shuffle_epi8(table, index);\n#else\n    uint8_t ii[16];\n    int8_t  tt[16], rr[16];\n    table.store(tt);  index.store(ii);\n    for (int j = 0; j < 16; j++) rr[j] = tt[ii[j] & 0x0F];\n    return Vec16c().load(rr);\n#endif\n}\n\nstatic inline Vec16c lookup32(Vec16c const & index, Vec16c const & table0, Vec16c const & table1) {\n#ifdef __XOP__  // AMD XOP instruction set. Use VPPERM\n    return _mm_perm_epi8(table0, table1, index);\n#elif INSTRSET >= 5  // SSSE3\n    Vec16c r0 = _mm_shuffle_epi8(table0, index + 0x70);           // make negative index for values >= 16\n    Vec16c r1 = _mm_shuffle_epi8(table1, (index ^ 0x10) + 0x70);  // make negative index for values <  16\n    return r0 | r1;\n#else\n    uint8_t ii[16];\n    int8_t  tt[16], rr[16];\n    table0.store(tt);  table1.store(tt+16);  index.store(ii);\n    for (int j = 0; j < 16; j++) rr[j] = tt[ii[j] & 0x1F];\n    return Vec16c().load(rr);\n#endif\n}\n\ntemplate <int n>\nstatic inline Vec16c lookup(Vec16c const & index, void const * table) {\n    if (n <=  0) return 0;\n    if (n <= 16) return lookup16(index, Vec16c().load(table));\n    if (n <= 32) return lookup32(index, Vec16c().load(table), Vec16c().load((int8_t*)table + 16));\n    // n > 32. Limit index\n    Vec16uc index1;\n    if ((n & (n-1)) == 0) {\n        // n is a power of 2, make index modulo n\n        index1 = Vec16uc(index) & uint8_t(n-1);\n    }\n    else {\n        // n is not a power of 2, limit to n-1\n        index1 = min(Vec16uc(index), uint8_t(n-1));\n    }\n    uint8_t ii[16];  index1.store(ii);\n    int8_t  rr[16];\n    for (int j = 0; j < 16; j++) {\n        rr[j] = ((int8_t*)table)[ii[j]];\n    }\n    return Vec16c().load(rr);\n}\n\nstatic inline Vec8s lookup8(Vec8s const & index, Vec8s const & table) {\n#if INSTRSET >= 5  // SSSE3\n    return _mm_shuffle_epi8(table, index * 0x202 + 0x100);\n#else\n    int16_t ii[8], tt[8], rr[8];\n    table.store(tt);  index.store(ii);\n    for (int j = 0; j < 8; j++) rr[j] = tt[ii[j] & 0x07];\n    return Vec8s().load(rr);\n#endif\n}\n\nstatic inline Vec8s lookup16(Vec8s const & index, Vec8s const & table0, Vec8s const & table1) {\n#ifdef __XOP__  // AMD XOP instruction set. Use VPPERM\n    return _mm_perm_epi8(table0, table1, index * 0x202 + 0x100);\n#elif INSTRSET >= 5  // SSSE3\n    Vec8s r0 = _mm_shuffle_epi8(table0, Vec16c(index * 0x202) + Vec16c(Vec8s(0x7170)));\n    Vec8s r1 = _mm_shuffle_epi8(table1, Vec16c(index * 0x202 ^ 0x1010) + Vec16c(Vec8s(0x7170)));\n    return r0 | r1;\n#else\n    int16_t ii[16], tt[32], rr[16];\n    table0.store(tt);  table1.store(tt+8);  index.store(ii);\n    for (int j = 0; j < 16; j++) rr[j] = tt[ii[j] & 0x1F];\n    return Vec8s().load(rr);\n#endif\n}\n\ntemplate <int n>\nstatic inline Vec8s lookup(Vec8s const & index, void const * table) {\n    if (n <=  0) return 0;\n    if (n <=  8) return lookup8 (index, Vec8s().load(table));\n    if (n <= 16) return lookup16(index, Vec8s().load(table), Vec8s().load((int16_t*)table + 8));\n    // n > 16. Limit index\n    Vec8us index1;\n    if ((n & (n-1)) == 0) {\n        // n is a power of 2, make index modulo n\n        index1 = Vec8us(index) & (n-1);\n    }\n    else {\n        // n is not a power of 2, limit to n-1\n        index1 = min(Vec8us(index), n-1);\n    }\n#if INSTRSET >= 8 // AVX2. Use VPERMD\n    Vec8s t1 = _mm_i32gather_epi32((const int *)table, __m128i((Vec4i(index1)) & (Vec4i(0x0000FFFF))), 2);  // even positions\n    Vec8s t2 = _mm_i32gather_epi32((const int *)table, _mm_srli_epi32(index1, 16) , 2);  // odd  positions\n    return blend8s<0,8,2,10,4,12,6,14>(t1, t2);\n#else\n    uint16_t ii[8];  index1.store(ii);\n    return Vec8s(((int16_t*)table)[ii[0]], ((int16_t*)table)[ii[1]], ((int16_t*)table)[ii[2]], ((int16_t*)table)[ii[3]],\n                 ((int16_t*)table)[ii[4]], ((int16_t*)table)[ii[5]], ((int16_t*)table)[ii[6]], ((int16_t*)table)[ii[7]]);\n#endif\n}\n\n\nstatic inline Vec4i lookup4(Vec4i const & index, Vec4i const & table) {\n#if INSTRSET >= 5  // SSSE3\n    return _mm_shuffle_epi8(table, index * 0x04040404 + 0x03020100);\n#else\n    return Vec4i(table[index[0]],table[index[1]],table[index[2]],table[index[3]]);\n#endif\n}\n\nstatic inline Vec4i lookup8(Vec4i const & index, Vec4i const & table0, Vec4i const & table1) {\n    // return Vec4i(lookup16(Vec8s(index * 0x20002 + 0x10000), Vec8s(table0), Vec8s(table1)));\n#ifdef __XOP__  // AMD XOP instruction set. Use VPPERM\n    return _mm_perm_epi8(table0, table1, index * 0x04040404 + 0x03020100);\n#elif INSTRSET >= 8 // AVX2. Use VPERMD\n    __m256i table01 = _mm256_inserti128_si256(_mm256_castsi128_si256(table0), table1, 1); // join tables into 256 bit vector\n\n#if defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)\n    // bug in MS VS 11 beta: operands in wrong order\n    return _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(_mm256_castsi128_si256(index), table01));\n#elif defined (GCC_VERSION) && GCC_VERSION <= 40700 && !defined(__INTEL_COMPILER) && !defined(__clang__)\n    // Gcc 4.7.0 also has operands in wrong order\n    return _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(_mm256_castsi128_si256(index), table01));\n#else\n    return _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(table01, _mm256_castsi128_si256(index)));\n#endif // bug\n\n#elif INSTRSET >= 4  // SSSE3\n    Vec4i r0 = _mm_shuffle_epi8(table0, Vec16c(index * 0x04040404) + Vec16c(Vec4i(0x73727170)));\n    Vec4i r1 = _mm_shuffle_epi8(table1, Vec16c(index * 0x04040404 ^ 0x10101010) + Vec16c(Vec4i(0x73727170)));\n    return r0 | r1;\n#else    // SSE2\n    int32_t ii[4], tt[8], rr[4];\n    table0.store(tt);  table1.store(tt+4);  index.store(ii);\n    for (int j = 0; j < 4; j++) rr[j] = tt[ii[j] & 0x07];\n    return Vec4i().load(rr);\n#endif\n}\n\nstatic inline Vec4i lookup16(Vec4i const & index, Vec4i const & table0, Vec4i const & table1, Vec4i const & table2, Vec4i const & table3) {\n#if INSTRSET >= 8 // AVX2. Use VPERMD\n    __m256i table01 = _mm256_inserti128_si256(_mm256_castsi128_si256(table0), table1, 1); // join tables into 256 bit vector\n    __m256i table23 = _mm256_inserti128_si256(_mm256_castsi128_si256(table2), table3, 1); // join tables into 256 bit vector\n#if defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)\n    // bug in MS VS 11 beta: operands in wrong order\n    __m128i r0 = _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(_mm256_castsi128_si256(index    ), table01));\n    __m128i r1 = _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(_mm256_castsi128_si256(index ^ 8), table23));\n#elif defined (GCC_VERSION) && GCC_VERSION <= 40700 && !defined(__INTEL_COMPILER) && !defined(__clang__)\n    // Gcc 4.7.0 also has operands in wrong order\n    __m128i r0 = _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(_mm256_castsi128_si256(index    ), table01));\n    __m128i r1 = _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(_mm256_castsi128_si256(index ^ 8), table23));\n#else\n    __m128i r0 = _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(table01, _mm256_castsi128_si256(index)));\n    __m128i r1 = _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(table23, _mm256_castsi128_si256(index ^ 8)));\n#endif // bug\n    return _mm_blendv_epi8(r0, r1, index > 8);\n\n#elif defined (__XOP__)  // AMD XOP instruction set. Use VPPERM\n    Vec4i r0 = _mm_perm_epi8(table0, table1, ((index    ) * 0x04040404u + 0x63626160u) & 0X9F9F9F9Fu);\n    Vec4i r1 = _mm_perm_epi8(table2, table3, ((index ^ 8) * 0x04040404u + 0x63626160u) & 0X9F9F9F9Fu);\n    return r0 | r1;\n\n#elif INSTRSET >= 5  // SSSE3\n    Vec16c aa = Vec16c(Vec4i(0x73727170));\n    Vec4i r0 = _mm_shuffle_epi8(table0, Vec16c((index     ) * 0x04040404) + aa);\n    Vec4i r1 = _mm_shuffle_epi8(table1, Vec16c((index ^  4) * 0x04040404) + aa);\n    Vec4i r2 = _mm_shuffle_epi8(table2, Vec16c((index ^  8) * 0x04040404) + aa);\n    Vec4i r3 = _mm_shuffle_epi8(table3, Vec16c((index ^ 12) * 0x04040404) + aa);\n    return (r0 | r1) | (r2 | r3);\n\n#else    // SSE2\n    int32_t ii[4], tt[16], rr[4];\n    table0.store(tt);  table1.store(tt+4);  table2.store(tt+8);  table3.store(tt+12);\n    index.store(ii);\n    for (int j = 0; j < 4; j++) rr[j] = tt[ii[j] & 0x0F];\n    return Vec4i().load(rr);\n#endif\n}\n\ntemplate <int n>\nstatic inline Vec4i lookup(Vec4i const & index, void const * table) {\n    if (n <= 0) return 0;\n    if (n <= 4) return lookup4(index, Vec4i().load(table));\n    if (n <= 8) return lookup8(index, Vec4i().load(table), Vec4i().load((int32_t*)table + 4));\n    // n > 8. Limit index\n    Vec4ui index1;\n    if ((n & (n-1)) == 0) {\n        // n is a power of 2, make index modulo n\n        index1 = Vec4ui(index) & (n-1);\n    }\n    else {\n        // n is not a power of 2, limit to n-1\n        index1 = min(Vec4ui(index), n-1);\n    }\n#if INSTRSET >= 8 // AVX2. Use VPERMD\n    return _mm_i32gather_epi32((const int *)table, index1, 4);\n#else\n    uint32_t ii[4];  index1.store(ii);\n    return Vec4i(((int32_t*)table)[ii[0]], ((int32_t*)table)[ii[1]], ((int32_t*)table)[ii[2]], ((int32_t*)table)[ii[3]]);\n#endif\n}\n\n\nstatic inline Vec2q lookup2(Vec2q const & index, Vec2q const & table) {\n#if INSTRSET >= 5  // SSSE3\n    return _mm_shuffle_epi8(table, index * 0x0808080808080808ll + 0x0706050403020100ll);\n#else\n    int64_t ii[2], tt[2];\n    table.store(tt);  index.store(ii);\n    return Vec2q(tt[int(ii[0])], tt[int(ii[1])]);\n#endif\n}\n\ntemplate <int n>\nstatic inline Vec2q lookup(Vec2q const & index, void const * table) {\n    if (n <= 0) return 0;\n    // n > 0. Limit index\n    Vec2uq index1;\n    if ((n & (n-1)) == 0) {\n        // n is a power of 2, make index modulo n\n        index1 = Vec2uq(index) & (n-1);\n    }\n    else {\n        // n is not a power of 2, limit to n-1.\n        // There is no 64-bit min instruction, but we can use the 32-bit unsigned min,\n        // since n is a 32-bit integer\n        index1 = Vec2uq(min(Vec2uq(index), constant4i<n-1, 0, n-1, 0>()));\n    }\n    uint32_t ii[4];  index1.store(ii);  // use only lower 32 bits of each index\n    int64_t const * tt = (int64_t const *)table;\n    return Vec2q(tt[ii[0]], tt[ii[2]]);\n}\n\n\n/*****************************************************************************\n*\n*          Other permutations with variable indexes\n*\n*****************************************************************************/\n\n// Function shift_bytes_up: shift whole vector left by b bytes.\n// You may use a permute function instead if b is a compile-time constant\nstatic inline Vec16c shift_bytes_up(Vec16c const & a, int b) {\n    if ((uint32_t)b > 15) return _mm_setzero_si128();\n#if INSTRSET >= 4    // SSSE3\n    static const char mask[32] = {-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1, 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15};    \n    return Vec16c(_mm_shuffle_epi8(a, Vec16c().load(mask+16-b)));\n#else\n    Vec2uq a1 = Vec2uq(a);\n    if (b < 8) {    \n        a1 = (a1 << (b*8)) | (permute2uq<-1,0>(a1) >> (64 - (b*8)));\n    }\n    else {\n        a1 = permute2uq<-1,0>(a1) << ((b-8)*8);\n    }\n    return Vec16c(a1);\n#endif\n}\n\n// Function shift_bytes_down: shift whole vector right by b bytes\n// You may use a permute function instead if b is a compile-time constant\nstatic inline Vec16c shift_bytes_down(Vec16c const & a, int b) {\n    if ((uint32_t)b > 15) return _mm_setzero_si128();\n#if INSTRSET >= 4    // SSSE3\n    static const char mask[32] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1};\n    return Vec16c(_mm_shuffle_epi8(a, Vec16c().load(mask+b)));\n#else\n    Vec2uq a1 = Vec2uq(a);\n    if (b < 8) {    \n        a1 = (a1 >> (b*8)) | (permute2uq<1,-1>(a1) << (64 - (b*8)));\n    }\n    else {\n        a1 = permute2uq<1,-1>(a1) >> ((b-8)*8); \n    }\n    return Vec16c(a1);\n#endif\n}\n\n/*****************************************************************************\n*\n*          Gather functions with fixed indexes\n*\n*****************************************************************************/\n// Load elements from array a with indices i0, i1, i2, i3\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline Vec4i gather4i(void const * a) {\n    Static_error_check<(i0|i1|i2|i3)>=0> Negative_array_index;  // Error message if index is negative\n    const int i01min = i0 < i1 ? i0 : i1;\n    const int i23min = i2 < i3 ? i2 : i3;\n    const int imin   = i01min < i23min ? i01min : i23min;\n    const int i01max = i0 > i1 ? i0 : i1;\n    const int i23max = i2 > i3 ? i2 : i3;\n    const int imax   = i01max > i23max ? i01max : i23max;\n    if (imax - imin <= 3) {\n        // load one contiguous block and permute\n        if (imax > 3) {\n            // make sure we don't read past the end of the array\n            Vec4i b = Vec4i().load((int32_t const *)a + imax-3);\n            return permute4i<i0-imax+3, i1-imax+3, i2-imax+3, i3-imax+3>(b);\n        }\n        else {\n            Vec4i b = Vec4i().load((int32_t const *)a + imin);\n            return permute4i<i0-imin, i1-imin, i2-imin, i3-imin>(b);\n        }\n    }\n    if ((i0<imin+4 || i0>imax-4) && (i1<imin+4 || i1>imax-4) && (i2<imin+4 || i2>imax-4) && (i3<imin+4 || i3>imax-4)) {\n        // load two contiguous blocks and blend\n        Vec4i b = Vec4i().load((int32_t const *)a + imin);\n        Vec4i c = Vec4i().load((int32_t const *)a + imax-3);\n        const int j0 = i0<imin+4 ? i0-imin : 7-imax+i0;\n        const int j1 = i1<imin+4 ? i1-imin : 7-imax+i1;\n        const int j2 = i2<imin+4 ? i2-imin : 7-imax+i2;\n        const int j3 = i3<imin+4 ? i3-imin : 7-imax+i3;\n        return blend4i<j0, j1, j2, j3>(b, c);\n    }\n    // use AVX2 gather if available\n#if INSTRSET >= 8\n    return _mm_i32gather_epi32((const int *)a, Vec4i(i0,i1,i2,i3), 4);\n#else\n    return lookup<imax+1>(Vec4i(i0,i1,i2,i3), a);\n#endif\n}\n\n// Load elements from array a with indices i0, i1\ntemplate <int i0, int i1>\nstatic inline Vec2q gather2q(void const * a) {\n    Static_error_check<(i0|i1)>=0> Negative_array_index;  // Error message if index is negative\n    const int imin = i0 < i1 ? i0 : i1;\n    const int imax = i0 > i1 ? i0 : i1;\n    if (imax - imin <= 1) {\n        // load one contiguous block and permute\n        if (imax > 1) {\n            // make sure we don't read past the end of the array\n            Vec2q b = Vec2q().load((int64_t const *)a + imax-1);\n            return permute2q<i0-imax+1, i1-imax+1>(b);\n        }\n        else {\n            Vec2q b = Vec2q().load((int64_t const *)a + imin);\n            return permute2q<i0-imin, i1-imin>(b);\n        }\n    }\n    return Vec2q(((int64_t*)a)[i0], ((int64_t*)a)[i1]);\n}\n\n\n/*****************************************************************************\n*\n*          Vector scatter functions\n*\n******************************************************************************\n*\n* These functions write the elements of a vector to arbitrary positions in an\n* array in memory. Each vector element is written to an array position \n* determined by an index. An element is not written if the corresponding\n* index is out of range.\n* The indexes can be specified as constant template parameters or as an\n* integer vector.\n* \n* The scatter functions are useful if the data are distributed in a sparce\n* manner into the array. If the array is dense then it is more efficient\n* to permute the data into the right positions and then write the whole\n* permuted vector into the array.\n*\n* Example:\n* Vec8q a(10,11,12,13,14,15,16,17);\n* int64_t b[16] = {0};\n* scatter<0,2,14,10,1,-1,5,9>(a,b); \n* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}\n*\n*****************************************************************************/\n\ntemplate <int i0, int i1, int i2, int i3>\nstatic inline void scatter(Vec4i data, void * array) {\n#if defined (__AVX512VL__)\n    __m128i indx = constant4i<i0,i1,i2,i3>();\n    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3);\n    _mm_mask_i32scatter_epi32((int*)array, mask, indx, data, 4);\n#else\n    int32_t* arr = (int32_t*)array;\n    const int index[4] = {i0,i1,i2,i3};\n    for (int i = 0; i < 4; i++) {\n        if (index[i] >= 0) arr[index[i]] = data[i];\n    }\n#endif\n}\n\ntemplate <int i0, int i1>\nstatic inline void scatter(Vec2q data, void * array) {\n    int64_t* arr = (int64_t*)array;\n    if (i0 >= 0) arr[i0] = data[0];\n    if (i1 >= 0) arr[i1] = data[1];\n}\n\nstatic inline void scatter(Vec4i index, uint32_t limit, Vec4i data, void * array) {\n#if defined (__AVX512VL__)\n    __mmask16 mask = _mm_cmplt_epu32_mask(index, Vec4ui(limit));\n    _mm_mask_i32scatter_epi32((int*)array, mask, index, data, 4);\n#else\n    int32_t* arr = (int32_t*)array;\n    for (int i = 0; i < 4; i++) {\n        if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];\n    }\n#endif\n}\n\nstatic inline void scatter(Vec2q index, uint32_t limit, Vec2q data, void * array) {\n    int64_t* arr = (int64_t*)array;\n    if (uint64_t(index[0]) < uint64_t(limit)) arr[index[0]] = data[0];\n    if (uint64_t(index[1]) < uint64_t(limit)) arr[index[1]] = data[1];\n} \n\nstatic inline void scatter(Vec4i index, uint32_t limit, Vec2q data, void * array) {\n    int64_t* arr = (int64_t*)array;\n    if (uint32_t(index[0]) < limit) arr[index[0]] = data[0];\n    if (uint32_t(index[1]) < limit) arr[index[1]] = data[1];\n} \n\n/*****************************************************************************\n*\n*          Functions for conversion between integer sizes\n*\n*****************************************************************************/\n\n// Extend 8-bit integers to 16-bit integers, signed and unsigned\n\n// Function extend_low : extends the low 8 elements to 16 bits with sign extension\nstatic inline Vec8s extend_low (Vec16c const & a) {\n    __m128i sign = _mm_cmpgt_epi8(_mm_setzero_si128(),a);  // 0 > a\n    return         _mm_unpacklo_epi8(a,sign);              // interleave with sign extensions\n}\n\n// Function extend_high : extends the high 8 elements to 16 bits with sign extension\nstatic inline Vec8s extend_high (Vec16c const & a) {\n    __m128i sign = _mm_cmpgt_epi8(_mm_setzero_si128(),a);  // 0 > a\n    return         _mm_unpackhi_epi8(a,sign);              // interleave with sign extensions\n}\n\n// Function extend_low : extends the low 8 elements to 16 bits with zero extension\nstatic inline Vec8us extend_low (Vec16uc const & a) {\n    return    _mm_unpacklo_epi8(a,_mm_setzero_si128());    // interleave with zero extensions\n}\n\n// Function extend_high : extends the high 8 elements to 16 bits with zero extension\nstatic inline Vec8us extend_high (Vec16uc const & a) {\n    return    _mm_unpackhi_epi8(a,_mm_setzero_si128());    // interleave with zero extensions\n}\n\n// Extend 16-bit integers to 32-bit integers, signed and unsigned\n\n// Function extend_low : extends the low 4 elements to 32 bits with sign extension\nstatic inline Vec4i extend_low (Vec8s const & a) {\n    __m128i sign = _mm_srai_epi16(a,15);                   // sign bit\n    return         _mm_unpacklo_epi16(a,sign);             // interleave with sign extensions\n}\n\n// Function extend_high : extends the high 4 elements to 32 bits with sign extension\nstatic inline Vec4i extend_high (Vec8s const & a) {\n    __m128i sign = _mm_srai_epi16(a,15);                   // sign bit\n    return         _mm_unpackhi_epi16(a,sign);             // interleave with sign extensions\n}\n\n// Function extend_low : extends the low 4 elements to 32 bits with zero extension\nstatic inline Vec4ui extend_low (Vec8us const & a) {\n    return    _mm_unpacklo_epi16(a,_mm_setzero_si128());   // interleave with zero extensions\n}\n\n// Function extend_high : extends the high 4 elements to 32 bits with zero extension\nstatic inline Vec4ui extend_high (Vec8us const & a) {\n    return    _mm_unpackhi_epi16(a,_mm_setzero_si128());   // interleave with zero extensions\n}\n\n// Extend 32-bit integers to 64-bit integers, signed and unsigned\n\n// Function extend_low : extends the low 2 elements to 64 bits with sign extension\nstatic inline Vec2q extend_low (Vec4i const & a) {\n    __m128i sign = _mm_srai_epi32(a,31);                   // sign bit\n    return         _mm_unpacklo_epi32(a,sign);             // interleave with sign extensions\n}\n\n// Function extend_high : extends the high 2 elements to 64 bits with sign extension\nstatic inline Vec2q extend_high (Vec4i const & a) {\n    __m128i sign = _mm_srai_epi32(a,31);                   // sign bit\n    return         _mm_unpackhi_epi32(a,sign);             // interleave with sign extensions\n}\n\n// Function extend_low : extends the low 2 elements to 64 bits with zero extension\nstatic inline Vec2uq extend_low (Vec4ui const & a) {\n    return    _mm_unpacklo_epi32(a,_mm_setzero_si128());   // interleave with zero extensions\n}\n\n// Function extend_high : extends the high 2 elements to 64 bits with zero extension\nstatic inline Vec2uq extend_high (Vec4ui const & a) {\n    return    _mm_unpackhi_epi32(a,_mm_setzero_si128());   // interleave with zero extensions\n}\n\n// Compress 16-bit integers to 8-bit integers, signed and unsigned, with and without saturation\n\n// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers\n// Overflow wraps around\nstatic inline Vec16c compress (Vec8s const & low, Vec8s const & high) {\n    __m128i mask  = _mm_set1_epi32(0x00FF00FF);            // mask for low bytes\n    __m128i lowm  = _mm_and_si128(low,mask);               // bytes of low\n    __m128i highm = _mm_and_si128(high,mask);              // bytes of high\n    return  _mm_packus_epi16(lowm,highm);                  // unsigned pack\n}\n\n// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers\n// Signed, with saturation\nstatic inline Vec16c compress_saturated (Vec8s const & low, Vec8s const & high) {\n    return  _mm_packs_epi16(low,high);\n}\n\n// Function compress : packs two vectors of 16-bit integers to one vector of 8-bit integers\n// Unsigned, overflow wraps around\nstatic inline Vec16uc compress (Vec8us const & low, Vec8us const & high) {\n    return  Vec16uc (compress((Vec8s)low, (Vec8s)high));\n}\n\n// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers\n// Unsigned, with saturation\nstatic inline Vec16uc compress_saturated (Vec8us const & low, Vec8us const & high) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    __m128i maxval  = _mm_set1_epi32(0x00FF00FF);          // maximum value\n    __m128i minval  = _mm_setzero_si128();                 // minimum value = 0\n    __m128i low1    = _mm_min_epu16(low,maxval);           // upper limit\n    __m128i high1   = _mm_min_epu16(high,maxval);          // upper limit\n    __m128i low2    = _mm_max_epu16(low1,minval);          // lower limit\n    __m128i high2   = _mm_max_epu16(high1,minval);         // lower limit\n    return            _mm_packus_epi16(low2,high2);        // this instruction saturates from signed 32 bit to unsigned 16 bit\n#else\n    __m128i zero    = _mm_setzero_si128();                 // 0\n    __m128i signlow = _mm_cmpgt_epi16(zero,low);           // sign bit of low\n    __m128i signhi  = _mm_cmpgt_epi16(zero,high);          // sign bit of high\n    __m128i slow2   = _mm_srli_epi16(signlow,8);           // FF if low negative\n    __m128i shigh2  = _mm_srli_epi16(signhi,8);            // FF if high negative\n    __m128i maskns  = _mm_set1_epi32(0x7FFF7FFF);          // mask for removing sign bit\n    __m128i lowns   = _mm_and_si128(low,maskns);           // low,  with sign bit removed\n    __m128i highns  = _mm_and_si128(high,maskns);          // high, with sign bit removed\n    __m128i lowo    = _mm_or_si128(lowns,slow2);           // low,  sign bit replaced by 00FF\n    __m128i higho   = _mm_or_si128(highns,shigh2);         // high, sign bit replaced by 00FF\n    return            _mm_packus_epi16(lowo,higho);        // this instruction saturates from signed 16 bit to unsigned 8 bit\n#endif\n}\n\n// Compress 32-bit integers to 16-bit integers, signed and unsigned, with and without saturation\n\n// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers\n// Overflow wraps around\nstatic inline Vec8s compress (Vec4i const & low, Vec4i const & high) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    __m128i mask  = _mm_set1_epi32(0x0000FFFF);            // mask for low words\n    __m128i lowm  = _mm_and_si128(low,mask);               // bytes of low\n    __m128i highm = _mm_and_si128(high,mask);              // bytes of high\n    return  _mm_packus_epi32(lowm,highm);                  // unsigned pack\n#else\n    __m128i low1  = _mm_shufflelo_epi16(low,0xD8);         // low words in place\n    __m128i high1 = _mm_shufflelo_epi16(high,0xD8);        // low words in place\n    __m128i low2  = _mm_shufflehi_epi16(low1,0xD8);        // low words in place\n    __m128i high2 = _mm_shufflehi_epi16(high1,0xD8);       // low words in place\n    __m128i low3  = _mm_shuffle_epi32(low2,0xD8);          // low dwords of low  to pos. 0 and 32\n    __m128i high3 = _mm_shuffle_epi32(high2,0xD8);         // low dwords of high to pos. 0 and 32\n    return  _mm_unpacklo_epi64(low3,high3);                // interleave\n#endif\n}\n\n// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers\n// Signed with saturation\nstatic inline Vec8s compress_saturated (Vec4i const & low, Vec4i const & high) {\n    return  _mm_packs_epi32(low,high);                     // pack with signed saturation\n}\n\n// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers\n// Overflow wraps around\nstatic inline Vec8us compress (Vec4ui const & low, Vec4ui const & high) {\n    return Vec8us (compress((Vec4i)low, (Vec4i)high));\n}\n\n// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers\n// Unsigned, with saturation\nstatic inline Vec8us compress_saturated (Vec4ui const & low, Vec4ui const & high) {\n#if INSTRSET >= 5   // SSE4.1 supported\n    __m128i maxval  = _mm_set1_epi32(0x0000FFFF);          // maximum value\n    __m128i minval  = _mm_setzero_si128();                 // minimum value = 0\n    __m128i low1    = _mm_min_epu32(low,maxval);           // upper limit\n    __m128i high1   = _mm_min_epu32(high,maxval);          // upper limit\n    __m128i low2    = _mm_max_epu32(low1,minval);          // lower limit\n    __m128i high2   = _mm_max_epu32(high1,minval);         // lower limit\n    return            _mm_packus_epi32(low2,high2);        // this instruction saturates from signed 32 bit to unsigned 16 bit\n#else\n    __m128i zero     = _mm_setzero_si128();                // 0\n    __m128i lowzero  = _mm_cmpeq_epi16(low,zero);          // for each word is zero\n    __m128i highzero = _mm_cmpeq_epi16(high,zero);         // for each word is zero\n    __m128i mone     = _mm_set1_epi32(-1);                 // FFFFFFFF\n    __m128i lownz    = _mm_xor_si128(lowzero,mone);        // for each word is nonzero\n    __m128i highnz   = _mm_xor_si128(highzero,mone);       // for each word is nonzero\n    __m128i lownz2   = _mm_srli_epi32(lownz,16);           // shift down to low dword\n    __m128i highnz2  = _mm_srli_epi32(highnz,16);          // shift down to low dword\n    __m128i lowsatur = _mm_or_si128(low,lownz2);           // low, saturated\n    __m128i hisatur  = _mm_or_si128(high,highnz2);         // high, saturated\n    return  Vec8us (compress(Vec4i(lowsatur), Vec4i(hisatur)));\n#endif\n}\n\n// Compress 64-bit integers to 32-bit integers, signed and unsigned, with and without saturation\n\n// Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers\n// Overflow wraps around\nstatic inline Vec4i compress (Vec2q const & low, Vec2q const & high) {\n    __m128i low2  = _mm_shuffle_epi32(low,0xD8);           // low dwords of low  to pos. 0 and 32\n    __m128i high2 = _mm_shuffle_epi32(high,0xD8);          // low dwords of high to pos. 0 and 32\n    return  _mm_unpacklo_epi64(low2,high2);                // interleave\n}\n\n// Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers\n// Signed, with saturation\n// This function is very inefficient unless the SSE4.2 instruction set is supported\nstatic inline Vec4i compress_saturated (Vec2q const & low, Vec2q const & high) {\n    Vec2q maxval = _mm_set_epi32(0,0x7FFFFFFF,0,0x7FFFFFFF);\n    Vec2q minval = _mm_set_epi32(-1,0x80000000,-1,0x80000000);\n    Vec2q low1   = min(low,maxval);\n    Vec2q high1  = min(high,maxval);\n    Vec2q low2   = max(low1,minval);\n    Vec2q high2  = max(high1,minval);\n    return compress(low2,high2);\n}\n\n// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers\n// Overflow wraps around\nstatic inline Vec4ui compress (Vec2uq const & low, Vec2uq const & high) {\n    return Vec4ui (compress((Vec2q)low, (Vec2q)high));\n}\n\n// Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers\n// Unsigned, with saturation\nstatic inline Vec4ui compress_saturated (Vec2uq const & low, Vec2uq const & high) {\n    __m128i zero     = _mm_setzero_si128();                // 0\n    __m128i lowzero  = _mm_cmpeq_epi32(low,zero);          // for each dword is zero\n    __m128i highzero = _mm_cmpeq_epi32(high,zero);         // for each dword is zero\n    __m128i mone     = _mm_set1_epi32(-1);                 // FFFFFFFF\n    __m128i lownz    = _mm_xor_si128(lowzero,mone);        // for each dword is nonzero\n    __m128i highnz   = _mm_xor_si128(highzero,mone);       // for each dword is nonzero\n    __m128i lownz2   = _mm_srli_epi64(lownz,32);           // shift down to low dword\n    __m128i highnz2  = _mm_srli_epi64(highnz,32);          // shift down to low dword\n    __m128i lowsatur = _mm_or_si128(low,lownz2);           // low, saturated\n    __m128i hisatur  = _mm_or_si128(high,highnz2);         // high, saturated\n    return  Vec4ui (compress(Vec2q(lowsatur), Vec2q(hisatur)));\n}\n\n/*****************************************************************************\n*\n*          Helper functions for division and bit scan\n*\n*****************************************************************************/\n\n// Define popcount function. Gives sum of bits\n#if INSTRSET >= 6   // SSE4.2\n    // popcnt instruction is not officially part of the SSE4.2 instruction set,\n    // but available in all known processors with SSE4.2\n#if defined (__GNUC__) || defined(__clang__)\nstatic inline uint32_t vml_popcnt (uint32_t a) __attribute__ ((pure));\nstatic inline uint32_t vml_popcnt (uint32_t a) {	\n    uint32_t r;\n    __asm(\"popcnt %1, %0\" : \"=r\"(r) : \"r\"(a) : );\n    return r;\n}\n#else\nstatic inline uint32_t vml_popcnt (uint32_t a) {	\n    return _mm_popcnt_u32(a);  // MS intrinsic\n}\n#endif // platform\n#else  // no SSE4.2\nstatic inline uint32_t vml_popcnt (uint32_t a) {	\n    // popcnt instruction not available\n    uint32_t b = a - ((a >> 1) & 0x55555555);\n    uint32_t c = (b & 0x33333333) + ((b >> 2) & 0x33333333);\n    uint32_t d = (c + (c >> 4)) & 0x0F0F0F0F;\n    uint32_t e = d * 0x01010101;\n    return   e >> 24;\n}\n#endif\n\n\n// Define bit-scan-forward function. Gives index to lowest set bit\n#if defined (__GNUC__) || defined(__clang__)\nstatic inline uint32_t bit_scan_forward (uint32_t a) __attribute__ ((pure));\nstatic inline uint32_t bit_scan_forward (uint32_t a) {	\n    uint32_t r;\n    __asm(\"bsfl %1, %0\" : \"=r\"(r) : \"r\"(a) : );\n    return r;\n}\n#else\nstatic inline uint32_t bit_scan_forward (uint32_t a) {	\n    unsigned long r;\n    _BitScanForward(&r, a);                      // defined in intrin.h for MS and Intel compilers\n    return r;\n}\n#endif\n\n// Define bit-scan-reverse function. Gives index to highest set bit = floor(log2(a))\n#if defined (__GNUC__) || defined(__clang__)\nstatic inline uint32_t bit_scan_reverse (uint32_t a) __attribute__ ((pure));\nstatic inline uint32_t bit_scan_reverse (uint32_t a) {	\n    uint32_t r;\n    __asm(\"bsrl %1, %0\" : \"=r\"(r) : \"r\"(a) : );\n    return r;\n}\n#else\nstatic inline uint32_t bit_scan_reverse (uint32_t a) {	\n    unsigned long r;\n    _BitScanReverse(&r, a);                      // defined in intrin.h for MS and Intel compilers\n    return r;\n}\n#endif\n\n// Same function, for compile-time constants.\n// We need template metaprogramming for calculating this function at compile time.\n// This may take a long time to compile because of the template recursion.\n// Todo: replace this with a constexpr function when C++14 becomes available\ntemplate <uint32_t n> \nstruct BitScanR {\n    enum {val = (\n        n >= 0x10 ? 4 + (BitScanR<(n>>4)>::val) :\n        n  <    2 ? 0 :\n        n  <    4 ? 1 :\n        n  <    8 ? 2 : 3 )                       };\n};\ntemplate <> struct BitScanR<0> {enum {val = 0};};          // Avoid infinite template recursion\n\n#define bit_scan_reverse_const(n)  (BitScanR<n>::val)      // n must be a valid compile-time constant\n\n\n/*****************************************************************************\n*\n*          Integer division operators\n*\n******************************************************************************\n*\n* The instruction set does not support integer vector division. Instead, we\n* are using a method for fast integer division based on multiplication and\n* shift operations. This method is faster than simple integer division if the\n* same divisor is used multiple times.\n*\n* All elements in a vector are divided by the same divisor. It is not possible\n* to divide different elements of the same vector by different divisors.\n*\n* The parameters used for fast division are stored in an object of a \n* Divisor class. This object can be created implicitly, for example in:\n*        Vec4i a, b; int c;\n*        a = b / c;\n* or explicitly as:\n*        a = b / Divisor_i(c);\n*\n* It takes more time to compute the parameters used for fast division than to\n* do the division. Therefore, it is advantageous to use the same divisor object\n* multiple times. For example, to divide 80 unsigned short integers by 10:\n*\n*        uint16_t dividends[80], quotients[80];         // numbers to work with\n*        Divisor_us div10(10);                          // make divisor object for dividing by 10\n*        Vec8us temp;                                   // temporary vector\n*        for (int i = 0; i < 80; i += 8) {              // loop for 4 elements per iteration\n*            temp.load(dividends+i);                    // load 4 elements\n*            temp /= div10;                             // divide each element by 10\n*            temp.store(quotients+i);                   // store 4 elements\n*        }\n* \n* The parameters for fast division can also be computed at compile time. This is\n* an advantage if the divisor is known at compile time. Use the const_int or const_uint\n* macro to do this. For example, for signed integers:\n*        Vec8s a, b;\n*        a = b / const_int(10);\n* Or, for unsigned integers:\n*        Vec8us a, b;\n*        a = b / const_uint(10);\n*\n* The division of a vector of 16-bit integers is faster than division of a vector \n* of other integer sizes.\n*\n* \n* Mathematical formula, used for signed division with fixed or variable divisor:\n* (From T. Granlund and P. L. Montgomery: Division by Invariant Integers Using Multiplication,\n* Proceedings of the SIGPLAN 1994 Conference on Programming Language Design and Implementation.\n* http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.2556 )\n* x = dividend\n* d = abs(divisor)\n* w = integer word size, bits\n* L = ceil(log2(d)) = bit_scan_reverse(d-1)+1\n* L = max(L,1)\n* m = 1 + 2^(w+L-1)/d - 2^w                      [division should overflow to 0 if d = 1]\n* sh1 = L-1\n* q = x + (m*x >> w)                             [high part of signed multiplication with 2w bits]\n* q = (q >> sh1) - (x<0 ? -1 : 0)\n* if (divisor < 0) q = -q \n* result trunc(x/d) = q\n*\n* Mathematical formula, used for unsigned division with variable divisor:\n* (Also from T. Granlund and P. L. Montgomery)\n* x = dividend\n* d = divisor\n* w = integer word size, bits\n* L = ceil(log2(d)) = bit_scan_reverse(d-1)+1\n* m = 1 + 2^w * (2^L-d) / d                      [2^L should overflow to 0 if L = w]\n* sh1 = min(L,1)\n* sh2 = max(L-1,0)\n* t = m*x >> w                                   [high part of unsigned multiplication with 2w bits]\n* result floor(x/d) = (((x-t) >> sh1) + t) >> sh2\n*\n* Mathematical formula, used for unsigned division with fixed divisor:\n* (From Terje Mathisen, unpublished)\n* x = dividend\n* d = divisor\n* w = integer word size, bits\n* b = floor(log2(d)) = bit_scan_reverse(d)\n* f = 2^(w+b) / d                                [exact division]\n* If f is an integer then d is a power of 2 then go to case A\n* If the fractional part of f is < 0.5 then go to case B\n* If the fractional part of f is > 0.5 then go to case C\n* Case A:  [shift only]\n* result = x >> b\n* Case B:  [round down f and compensate by adding one to x]\n* result = ((x+1)*floor(f)) >> (w+b)             [high part of unsigned multiplication with 2w bits]\n* Case C:  [round up f, no compensation for rounding error]\n* result = (x*ceil(f)) >> (w+b)                  [high part of unsigned multiplication with 2w bits]\n*\n*\n*****************************************************************************/\n\n// encapsulate parameters for fast division on vector of 4 32-bit signed integers\nclass Divisor_i {\nprotected:\n    __m128i multiplier;                                    // multiplier used in fast division\n    __m128i shift1;                                        // shift count used in fast division\n    __m128i sign;                                          // sign of divisor\npublic:\n    Divisor_i() {};                                        // Default constructor\n    Divisor_i(int32_t d) {                                 // Constructor with divisor\n        set(d);\n    }\n    Divisor_i(int m, int s1, int sgn) {                    // Constructor with precalculated multiplier, shift and sign\n        multiplier = _mm_set1_epi32(m);\n        shift1     = _mm_cvtsi32_si128(s1);\n        sign       = _mm_set1_epi32(sgn);\n    }\n    void set(int32_t d) {                                  // Set or change divisor, calculate parameters\n        const int32_t d1 = ::abs(d);\n        int32_t sh, m;\n        if (d1 > 1) {\n            sh = bit_scan_reverse(d1-1);                   // shift count = ceil(log2(d1))-1 = (bit_scan_reverse(d1-1)+1)-1\n            m = int32_t((int64_t(1) << (32+sh)) / d1 - ((int64_t(1) << 32) - 1)); // calculate multiplier\n        }\n        else {\n            m  = 1;                                        // for d1 = 1\n            sh = 0;\n            if (d == 0) m /= d;                            // provoke error here if d = 0\n            if (uint32_t(d) == 0x80000000u) {              // fix overflow for this special case\n                m  = 0x80000001;\n                sh = 30;\n            }\n        }\n        multiplier = _mm_set1_epi32(m);                    // broadcast multiplier\n        shift1     = _mm_setr_epi32(sh, 0, 0, 0);          // shift count\n        sign       = _mm_set1_epi32(d < 0 ? -1 : 0);       // sign of divisor\n    }\n    __m128i getm() const {                                 // get multiplier\n        return multiplier;\n    }\n    __m128i gets1() const {                                // get shift count\n        return shift1;\n    }\n    __m128i getsign() const {                              // get sign of divisor\n        return sign;\n    }\n};\n\n// encapsulate parameters for fast division on vector of 4 32-bit unsigned integers\nclass Divisor_ui {\nprotected:\n    __m128i multiplier;                                    // multiplier used in fast division\n    __m128i shift1;                                        // shift count 1 used in fast division\n    __m128i shift2;                                        // shift count 2 used in fast division\npublic:\n    Divisor_ui() {};                                       // Default constructor\n    Divisor_ui(uint32_t d) {                               // Constructor with divisor\n        set(d);\n    }\n    Divisor_ui(uint32_t m, int s1, int s2) {               // Constructor with precalculated multiplier and shifts\n        multiplier = _mm_set1_epi32(m);\n        shift1     = _mm_setr_epi32(s1, 0, 0, 0);\n        shift2     = _mm_setr_epi32(s2, 0, 0, 0);\n    }\n    void set(uint32_t d) {                                 // Set or change divisor, calculate parameters\n        uint32_t L, L2, sh1, sh2, m;\n        switch (d) {\n        case 0:\n            m = sh1 = sh2 = 1 / d;                         // provoke error for d = 0\n            break;\n        case 1:\n            m = 1; sh1 = sh2 = 0;                          // parameters for d = 1\n            break;\n        case 2:\n            m = 1; sh1 = 1; sh2 = 0;                       // parameters for d = 2\n            break;\n        default:                                           // general case for d > 2\n            L  = bit_scan_reverse(d-1)+1;                  // ceil(log2(d))\n            L2 = L < 32 ? 1 << L : 0;                      // 2^L, overflow to 0 if L = 32\n            m  = 1 + uint32_t((uint64_t(L2 - d) << 32) / d); // multiplier\n            sh1 = 1;  sh2 = L - 1;                         // shift counts\n        }\n        multiplier = _mm_set1_epi32(m);\n        shift1     = _mm_setr_epi32(sh1, 0, 0, 0);\n        shift2     = _mm_setr_epi32(sh2, 0, 0, 0);\n    }\n    __m128i getm() const {                                 // get multiplier\n        return multiplier;\n    }\n    __m128i gets1() const {                                // get shift count 1\n        return shift1;\n    }\n    __m128i gets2() const {                                // get shift count 2\n        return shift2;\n    }\n};\n\n\n// encapsulate parameters for fast division on vector of 8 16-bit signed integers\nclass Divisor_s {\nprotected:\n    __m128i multiplier;                                    // multiplier used in fast division\n    __m128i shift1;                                        // shift count used in fast division\n    __m128i sign;                                          // sign of divisor\npublic:\n    Divisor_s() {};                                        // Default constructor\n    Divisor_s(int16_t d) {                                 // Constructor with divisor\n        set(d);\n    }\n    Divisor_s(int16_t m, int s1, int sgn) {                // Constructor with precalculated multiplier, shift and sign\n        multiplier = _mm_set1_epi16(m);\n        shift1     = _mm_setr_epi32(s1, 0, 0, 0);\n        sign       = _mm_set1_epi32(sgn);\n    }\n    void set(int16_t d) {                                  // Set or change divisor, calculate parameters\n        const int32_t d1 = ::abs(d);\n        int32_t sh, m;\n        if (d1 > 1) {\n            sh = bit_scan_reverse(d1-1);                   // shift count = ceil(log2(d1))-1 = (bit_scan_reverse(d1-1)+1)-1\n            m = ((int32_t(1) << (16+sh)) / d1 - ((int32_t(1) << 16) - 1)); // calculate multiplier\n        }\n        else {\n            m  = 1;                                        // for d1 = 1\n            sh = 0;\n            if (d == 0) m /= d;                            // provoke error here if d = 0\n            if (uint16_t(d) == 0x8000u) {                  // fix overflow for this special case\n                m  = 0x8001;\n                sh = 14;\n            }\n        }\n        multiplier = _mm_set1_epi16(int16_t(m));           // broadcast multiplier\n        shift1     = _mm_setr_epi32(sh, 0, 0, 0);          // shift count\n        sign       = _mm_set1_epi32(d < 0 ? -1 : 0);       // sign of divisor\n    }\n    __m128i getm() const {                                 // get multiplier\n        return multiplier;\n    }\n    __m128i gets1() const {                                // get shift count\n        return shift1;\n    }\n    __m128i getsign() const {                              // get sign of divisor\n        return sign;\n    }\n};\n\n\n// encapsulate parameters for fast division on vector of 8 16-bit unsigned integers\nclass Divisor_us {\nprotected:\n    __m128i multiplier;                                    // multiplier used in fast division\n    __m128i shift1;                                        // shift count 1 used in fast division\n    __m128i shift2;                                        // shift count 2 used in fast division\npublic:\n    Divisor_us() {};                                       // Default constructor\n    Divisor_us(uint16_t d) {                               // Constructor with divisor\n        set(d);\n    }\n    Divisor_us(uint16_t m, int s1, int s2) {               // Constructor with precalculated multiplier and shifts\n        multiplier = _mm_set1_epi16(m);\n        shift1     = _mm_setr_epi32(s1, 0, 0, 0);\n        shift2     = _mm_setr_epi32(s2, 0, 0, 0);\n    }\n    void set(uint16_t d) {                                 // Set or change divisor, calculate parameters\n        uint16_t L, L2, sh1, sh2, m;\n        switch (d) {\n        case 0:\n            m = sh1 = sh2 = 1 / d;                         // provoke error for d = 0\n            break;\n        case 1:\n            m = 1; sh1 = sh2 = 0;                          // parameters for d = 1\n            break;\n        case 2:\n            m = 1; sh1 = 1; sh2 = 0;                       // parameters for d = 2\n            break;\n        default:                                           // general case for d > 2\n            L  = (uint16_t)bit_scan_reverse(d-1)+1;        // ceil(log2(d))\n            L2 = uint16_t(1 << L);                         // 2^L, overflow to 0 if L = 16\n            m  = 1 + uint16_t((uint32_t(L2 - d) << 16) / d); // multiplier\n            sh1 = 1;  sh2 = L - 1;                         // shift counts\n        }\n        multiplier = _mm_set1_epi16(m);\n        shift1     = _mm_setr_epi32(sh1, 0, 0, 0);\n        shift2     = _mm_setr_epi32(sh2, 0, 0, 0);\n    }\n    __m128i getm() const {                                 // get multiplier\n        return multiplier;\n    }\n    __m128i gets1() const {                                // get shift count 1\n        return shift1;\n    }\n    __m128i gets2() const {                                // get shift count 2\n        return shift2;\n    }\n};\n\n\n// vector operator / : divide each element by divisor\n\n// vector of 4 32-bit signed integers\nstatic inline Vec4i operator / (Vec4i const & a, Divisor_i const & d) {\n#if defined (__XOP__) && defined (GCC_VERSION) && GCC_VERSION <= 40702/*??*/ && !defined(__INTEL_COMPILER) && !defined(__clang__)\n#define XOP_MUL_BUG                                       // GCC has bug in XOP multiply\n// Bug found in GCC version 4.7.0 and 4.7.1\n#endif\n// todo: test this when GCC bug is fixed\n#if defined (__XOP__) && !defined (XOP_MUL_BUG)\n    __m128i t1  = _mm_mul_epi32(a,d.getm());               // 32x32->64 bit signed multiplication of a[0] and a[2]\n    __m128i t2  = _mm_srli_epi64(t1,32);                   // high dword of result 0 and 2\n    __m128i t3  = _mm_macchi_epi32(a,d.getm(),_mm_setzero_si128());// 32x32->64 bit signed multiplication of a[1] and a[3]\n    __m128i t5  = _mm_set_epi32(-1,0,-1,0);                // mask of dword 1 and 3\n    __m128i t7  = _mm_blendv_epi8(t2,t3,t5);               // blend two results\n    __m128i t8  = _mm_add_epi32(t7,a);                     // add\n    __m128i t9  = _mm_sra_epi32(t8,d.gets1());             // shift right arithmetic\n    __m128i t10 = _mm_srai_epi32(a,31);                    // sign of a\n    __m128i t11 = _mm_sub_epi32(t10,d.getsign());          // sign of a - sign of d\n    __m128i t12 = _mm_sub_epi32(t9,t11);                   // + 1 if a < 0, -1 if d < 0\n    return        _mm_xor_si128(t12,d.getsign());          // change sign if divisor negative\n\n#elif INSTRSET >= 5 && !defined (XOP_MUL_BUG)  // SSE4.1 supported \n    __m128i t1  = _mm_mul_epi32(a,d.getm());               // 32x32->64 bit signed multiplication of a[0] and a[2]\n    __m128i t2  = _mm_srli_epi64(t1,32);                   // high dword of result 0 and 2\n    __m128i t3  = _mm_srli_epi64(a,32);                    // get a[1] and a[3] into position for multiplication\n    __m128i t4  = _mm_mul_epi32(t3,d.getm());              // 32x32->64 bit signed multiplication of a[1] and a[3]\n    __m128i t5  = _mm_set_epi32(-1,0,-1,0);                // mask of dword 1 and 3\n    __m128i t7  = _mm_blendv_epi8(t2,t4,t5);               // blend two results\n    __m128i t8  = _mm_add_epi32(t7,a);                     // add\n    __m128i t9  = _mm_sra_epi32(t8,d.gets1());             // shift right arithmetic\n    __m128i t10 = _mm_srai_epi32(a,31);                    // sign of a\n    __m128i t11 = _mm_sub_epi32(t10,d.getsign());          // sign of a - sign of d\n    __m128i t12 = _mm_sub_epi32(t9,t11);                   // + 1 if a < 0, -1 if d < 0\n    return        _mm_xor_si128(t12,d.getsign());          // change sign if divisor negative\n#else  // not SSE4.1\n    __m128i t1  = _mm_mul_epu32(a,d.getm());               // 32x32->64 bit unsigned multiplication of a[0] and a[2]\n    __m128i t2  = _mm_srli_epi64(t1,32);                   // high dword of result 0 and 2\n    __m128i t3  = _mm_srli_epi64(a,32);                    // get a[1] and a[3] into position for multiplication\n    __m128i t4  = _mm_mul_epu32(t3,d.getm());              // 32x32->64 bit unsigned multiplication of a[1] and a[3]\n    __m128i t5  = _mm_set_epi32(-1,0,-1,0);                // mask of dword 1 and 3\n    __m128i t6  = _mm_and_si128(t4,t5);                    // high dword of result 1 and 3\n    __m128i t7  = _mm_or_si128(t2,t6);                     // combine all four results of unsigned high mul into one vector\n    // convert unsigned to signed high multiplication (from: H S Warren: Hacker's delight, 2003, p. 132)\n    __m128i u1  = _mm_srai_epi32(a,31);                    // sign of a\n    __m128i u2  = _mm_srai_epi32(d.getm(),31);             // sign of m [ m is always negative, except for abs(d) = 1 ]\n    __m128i u3  = _mm_and_si128 (d.getm(),u1);             // m * sign of a\n    __m128i u4  = _mm_and_si128 (a,u2);                    // a * sign of m\n    __m128i u5  = _mm_add_epi32 (u3,u4);                   // sum of sign corrections\n    __m128i u6  = _mm_sub_epi32 (t7,u5);                   // high multiplication result converted to signed\n    __m128i t8  = _mm_add_epi32(u6,a);                     // add a\n    __m128i t9  = _mm_sra_epi32(t8,d.gets1());             // shift right arithmetic\n    __m128i t10 = _mm_sub_epi32(u1,d.getsign());           // sign of a - sign of d\n    __m128i t11 = _mm_sub_epi32(t9,t10);                   // + 1 if a < 0, -1 if d < 0\n    return        _mm_xor_si128(t11,d.getsign());          // change sign if divisor negative\n#endif\n}\n\n// vector of 4 32-bit unsigned integers\nstatic inline Vec4ui operator / (Vec4ui const & a, Divisor_ui const & d) {\n    __m128i t1  = _mm_mul_epu32(a,d.getm());               // 32x32->64 bit unsigned multiplication of a[0] and a[2]\n    __m128i t2  = _mm_srli_epi64(t1,32);                   // high dword of result 0 and 2\n    __m128i t3  = _mm_srli_epi64(a,32);                    // get a[1] and a[3] into position for multiplication\n    __m128i t4  = _mm_mul_epu32(t3,d.getm());              // 32x32->64 bit unsigned multiplication of a[1] and a[3]\n    __m128i t5  = _mm_set_epi32(-1,0,-1,0);                // mask of dword 1 and 3\n#if INSTRSET >= 5   // SSE4.1 supported\n    __m128i t7  = _mm_blendv_epi8(t2,t4,t5);               // blend two results\n#else\n    __m128i t6  = _mm_and_si128(t4,t5);                    // high dword of result 1 and 3\n    __m128i t7  = _mm_or_si128(t2,t6);                     // combine all four results into one vector\n#endif\n    __m128i t8  = _mm_sub_epi32(a,t7);                     // subtract\n    __m128i t9  = _mm_srl_epi32(t8,d.gets1());             // shift right logical\n    __m128i t10 = _mm_add_epi32(t7,t9);                    // add\n    return        _mm_srl_epi32(t10,d.gets2());            // shift right logical \n}\n\n// vector of 8 16-bit signed integers\nstatic inline Vec8s operator / (Vec8s const & a, Divisor_s const & d) {\n    __m128i t1  = _mm_mulhi_epi16(a, d.getm());            // multiply high signed words\n    __m128i t2  = _mm_add_epi16(t1,a);                     // + a\n    __m128i t3  = _mm_sra_epi16(t2,d.gets1());             // shift right arithmetic\n    __m128i t4  = _mm_srai_epi16(a,15);                    // sign of a\n    __m128i t5  = _mm_sub_epi16(t4,d.getsign());           // sign of a - sign of d\n    __m128i t6  = _mm_sub_epi16(t3,t5);                    // + 1 if a < 0, -1 if d < 0\n    return        _mm_xor_si128(t6,d.getsign());           // change sign if divisor negative\n}\n\n// vector of 8 16-bit unsigned integers\nstatic inline Vec8us operator / (Vec8us const & a, Divisor_us const & d) {\n    __m128i t1  = _mm_mulhi_epu16(a, d.getm());            // multiply high unsigned words\n    __m128i t2  = _mm_sub_epi16(a,t1);                     // subtract\n    __m128i t3  = _mm_srl_epi16(t2,d.gets1());             // shift right logical\n    __m128i t4  = _mm_add_epi16(t1,t3);                    // add\n    return        _mm_srl_epi16(t4,d.gets2());             // shift right logical \n}\n\n \n// vector of 16 8-bit signed integers\nstatic inline Vec16c operator / (Vec16c const & a, Divisor_s const & d) {\n    // expand into two Vec8s\n    Vec8s low  = extend_low(a)  / d;\n    Vec8s high = extend_high(a) / d;\n    return compress(low,high);\n}\n\n// vector of 16 8-bit unsigned integers\nstatic inline Vec16uc operator / (Vec16uc const & a, Divisor_us const & d) {\n    // expand into two Vec8s\n    Vec8us low  = extend_low(a)  / d;\n    Vec8us high = extend_high(a) / d;\n    return compress(low,high);\n}\n\n// vector operator /= : divide\nstatic inline Vec8s & operator /= (Vec8s & a, Divisor_s const & d) {\n    a = a / d;\n    return a;\n}\n\n// vector operator /= : divide\nstatic inline Vec8us & operator /= (Vec8us & a, Divisor_us const & d) {\n    a = a / d;\n    return a;\n}\n\n// vector operator /= : divide\nstatic inline Vec4i & operator /= (Vec4i & a, Divisor_i const & d) {\n    a = a / d;\n    return a;\n}\n\n// vector operator /= : divide\nstatic inline Vec4ui & operator /= (Vec4ui & a, Divisor_ui const & d) {\n    a = a / d;\n    return a;\n}\n\n// vector operator /= : divide\nstatic inline Vec16c & operator /= (Vec16c & a, Divisor_s const & d) {\n    a = a / d;\n    return a;\n}\n\n// vector operator /= : divide\nstatic inline Vec16uc & operator /= (Vec16uc & a, Divisor_us const & d) {\n    a = a / d;\n    return a;\n}\n\n/*****************************************************************************\n*\n*          Integer division 2: divisor is a compile-time constant\n*\n*****************************************************************************/\n\n// Divide Vec4i by compile-time constant\ntemplate <int32_t d>\nstatic inline Vec4i divide_by_i(Vec4i const & x) {\n    Static_error_check<(d!=0)> Dividing_by_zero;                     // Error message if dividing by zero\n    if (d ==  1) return  x;\n    if (d == -1) return -x;\n    if (uint32_t(d) == 0x80000000u) return Vec4i(x == Vec4i(0x80000000)) & 1; // prevent overflow when changing sign\n    const uint32_t d1 = d > 0 ? uint32_t(d) : uint32_t(-d);          // compile-time abs(d). (force GCC compiler to treat d as 32 bits, not 64 bits)\n    if ((d1 & (d1-1)) == 0) {\n        // d1 is a power of 2. use shift\n        const int k = bit_scan_reverse_const(d1);\n        __m128i sign;\n        if (k > 1) sign = _mm_srai_epi32(x, k-1); else sign = x;     // k copies of sign bit\n        __m128i bias    = _mm_srli_epi32(sign, 32-k);                // bias = x >= 0 ? 0 : k-1\n        __m128i xpbias  = _mm_add_epi32 (x, bias);                   // x + bias\n        __m128i q       = _mm_srai_epi32(xpbias, k);                 // (x + bias) >> k\n        if (d > 0)      return q;                                    // d > 0: return  q\n        return _mm_sub_epi32(_mm_setzero_si128(), q);                // d < 0: return -q\n    }\n    // general case\n    const int32_t sh = bit_scan_reverse_const(uint32_t(d1)-1);            // ceil(log2(d1)) - 1. (d1 < 2 handled by power of 2 case)\n    const int32_t mult = int(1 + (uint64_t(1) << (32+sh)) / uint32_t(d1) - (int64_t(1) << 32));   // multiplier\n    const Divisor_i div(mult, sh, d < 0 ? -1 : 0);\n    return x / div;\n}\n\n// define Vec4i a / const_int(d)\ntemplate <int32_t d>\nstatic inline Vec4i operator / (Vec4i const & a, Const_int_t<d>) {\n    return divide_by_i<d>(a);\n}\n\n// define Vec4i a / const_uint(d)\ntemplate <uint32_t d>\nstatic inline Vec4i operator / (Vec4i const & a, Const_uint_t<d>) {\n    Static_error_check< (d<0x80000000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned\n    return divide_by_i<int32_t(d)>(a);                               // signed divide\n}\n\n// vector operator /= : divide\ntemplate <int32_t d>\nstatic inline Vec4i & operator /= (Vec4i & a, Const_int_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n// vector operator /= : divide\ntemplate <uint32_t d>\nstatic inline Vec4i & operator /= (Vec4i & a, Const_uint_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n\n// Divide Vec4ui by compile-time constant\ntemplate <uint32_t d>\nstatic inline Vec4ui divide_by_ui(Vec4ui const & x) {\n    Static_error_check<(d!=0)> Dividing_by_zero;                     // Error message if dividing by zero\n    if (d == 1) return x;                                            // divide by 1\n    const int b = bit_scan_reverse_const(d);                         // floor(log2(d))\n    if ((uint32_t(d) & (uint32_t(d)-1)) == 0) {\n        // d is a power of 2. use shift\n        return    _mm_srli_epi32(x, b);                              // x >> b\n    }\n    // general case (d > 2)\n    uint32_t mult = uint32_t((uint64_t(1) << (b+32)) / d);           // multiplier = 2^(32+b) / d\n    const uint64_t rem = (uint64_t(1) << (b+32)) - uint64_t(d)*mult; // remainder 2^(32+b) % d\n    const bool round_down = (2*rem < d);                             // check if fraction is less than 0.5\n    if (!round_down) {\n        mult = mult + 1;                                             // round up mult\n    }\n    // do 32*32->64 bit unsigned multiplication and get high part of result\n    const __m128i multv = _mm_set_epi32(0,mult,0,mult);              // zero-extend mult and broadcast\n    __m128i t1  = _mm_mul_epu32(x,multv);                            // 32x32->64 bit unsigned multiplication of x[0] and x[2]\n    if (round_down) {\n        t1      = _mm_add_epi64(t1,multv);                           // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow\n    }\n    __m128i t2  = _mm_srli_epi64(t1,32);                             // high dword of result 0 and 2\n    __m128i t3  = _mm_srli_epi64(x,32);                              // get x[1] and x[3] into position for multiplication\n    __m128i t4  = _mm_mul_epu32(t3,multv);                           // 32x32->64 bit unsigned multiplication of x[1] and x[3]\n    if (round_down) {\n        t4      = _mm_add_epi64(t4,multv);                           // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow\n    }\n    __m128i t5  = _mm_set_epi32(-1,0,-1,0);                          // mask of dword 1 and 3\n#if INSTRSET >= 5   // SSE4.1 supported\n    __m128i t7  = _mm_blendv_epi8(t2,t4,t5);                         // blend two results\n#else\n    __m128i t6  = _mm_and_si128(t4,t5);                              // high dword of result 1 and 3\n    __m128i t7  = _mm_or_si128(t2,t6);                               // combine all four results into one vector\n#endif\n    Vec4ui q    = _mm_srli_epi32(t7, b);                             // shift right by b\n    return q;                                                    // no overflow possible\n}\n\n// define Vec4ui a / const_uint(d)\ntemplate <uint32_t d>\nstatic inline Vec4ui operator / (Vec4ui const & a, Const_uint_t<d>) {\n    return divide_by_ui<d>(a);\n}\n\n// define Vec4ui a / const_int(d)\ntemplate <int32_t d>\nstatic inline Vec4ui operator / (Vec4ui const & a, Const_int_t<d>) {\n    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous\n    return divide_by_ui<d>(a);                                       // unsigned divide\n}\n\n// vector operator /= : divide\ntemplate <uint32_t d>\nstatic inline Vec4ui & operator /= (Vec4ui & a, Const_uint_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n// vector operator /= : divide\ntemplate <int32_t d>\nstatic inline Vec4ui & operator /= (Vec4ui & a, Const_int_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n\n// Divide Vec8s by compile-time constant \ntemplate <int d>\nstatic inline Vec8s divide_by_i(Vec8s const & x) {\n    const int16_t d0 = int16_t(d);                                   // truncate d to 16 bits\n    Static_error_check<(d0 != 0)> Dividing_by_zero;                  // Error message if dividing by zero\n    if (d0 ==  1) return  x;                                         // divide by  1\n    if (d0 == -1) return -x;                                         // divide by -1\n    if (uint16_t(d0) == 0x8000u) return Vec8s(x == Vec8s(0x8000)) & 1;// prevent overflow when changing sign\n    // if (d > 0x7FFF || d < -0x8000) return 0;                      // not relevant when d truncated to 16 bits\n    const uint16_t d1 = d0 > 0 ? d0 : -d0;                           // compile-time abs(d0)\n    if ((d1 & (d1-1)) == 0) {\n        // d is a power of 2. use shift\n        const int k = bit_scan_reverse_const(uint32_t(d1));\n        __m128i sign;\n        if (k > 1) sign = _mm_srai_epi16(x, k-1); else sign = x;     // k copies of sign bit\n        __m128i bias    = _mm_srli_epi16(sign, 16-k);                // bias = x >= 0 ? 0 : k-1\n        __m128i xpbias  = _mm_add_epi16 (x, bias);                   // x + bias\n        __m128i q       = _mm_srai_epi16(xpbias, k);                 // (x + bias) >> k\n        if (d0 > 0)  return q;                                       // d0 > 0: return  q\n        return _mm_sub_epi16(_mm_setzero_si128(), q);                // d0 < 0: return -q\n    }\n    // general case\n    const int L = bit_scan_reverse_const(uint16_t(d1-1)) + 1;        // ceil(log2(d)). (d < 2 handled above)\n    const int16_t mult = int16_t(1 + (1u << (15+L)) / uint32_t(d1) - 0x10000);// multiplier\n    const int shift1 = L - 1;\n    const Divisor_s div(mult, shift1, d0 > 0 ? 0 : -1);\n    return x / div;\n}\n\n// define Vec8s a / const_int(d)\ntemplate <int d>\nstatic inline Vec8s operator / (Vec8s const & a, Const_int_t<d>) {\n    return divide_by_i<d>(a);\n}\n\n// define Vec8s a / const_uint(d)\ntemplate <uint32_t d>\nstatic inline Vec8s operator / (Vec8s const & a, Const_uint_t<d>) {\n    Static_error_check< (d<0x8000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned\n    return divide_by_i<int(d)>(a);                                   // signed divide\n}\n\n// vector operator /= : divide\ntemplate <int32_t d>\nstatic inline Vec8s & operator /= (Vec8s & a, Const_int_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n// vector operator /= : divide\ntemplate <uint32_t d>\nstatic inline Vec8s & operator /= (Vec8s & a, Const_uint_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n\n// Divide Vec8us by compile-time constant\ntemplate <uint32_t d>\nstatic inline Vec8us divide_by_ui(Vec8us const & x) {\n    const uint16_t d0 = uint16_t(d);                                 // truncate d to 16 bits\n    Static_error_check<(d0 != 0)> Dividing_by_zero;                  // Error message if dividing by zero\n    if (d0 == 1) return x;                                           // divide by 1\n    const int b = bit_scan_reverse_const(d0);                        // floor(log2(d))\n    if ((d0 & (d0-1)) == 0) {\n        // d is a power of 2. use shift\n        return  _mm_srli_epi16(x, b);                                // x >> b\n    }\n    // general case (d > 2)\n    uint16_t mult = uint16_t((uint32_t(1) << (b+16)) / d0);          // multiplier = 2^(32+b) / d\n    const uint32_t rem = (uint32_t(1) << (b+16)) - uint32_t(d0)*mult;// remainder 2^(32+b) % d\n    const bool round_down = (2*rem < d0);                            // check if fraction is less than 0.5\n    Vec8us x1 = x;\n    if (round_down) {\n        x1 = x1 + 1;                                                 // round down mult and compensate by adding 1 to x\n    }\n    else {\n        mult = mult + 1;                                             // round up mult. no compensation needed\n    }\n    const __m128i multv = _mm_set1_epi16(mult);                      // broadcast mult\n    __m128i xm = _mm_mulhi_epu16(x1, multv);                         // high part of 16x16->32 bit unsigned multiplication\n    Vec8us q    = _mm_srli_epi16(xm, b);                             // shift right by b\n    if (round_down) {\n        Vec8sb overfl = (x1 == (Vec8us)_mm_setzero_si128());         // check for overflow of x+1\n        return select(overfl, Vec8us(mult >> b), q);                 // deal with overflow (rarely needed)\n    }\n    else {\n        return q;                                                    // no overflow possible\n    }\n}\n\n// define Vec8us a / const_uint(d)\ntemplate <uint32_t d>\nstatic inline Vec8us operator / (Vec8us const & a, Const_uint_t<d>) {\n    return divide_by_ui<d>(a);\n}\n\n// define Vec8us a / const_int(d)\ntemplate <int d>\nstatic inline Vec8us operator / (Vec8us const & a, Const_int_t<d>) {\n    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous\n    return divide_by_ui<d>(a);                                       // unsigned divide\n}\n\n// vector operator /= : divide\ntemplate <uint32_t d>\nstatic inline Vec8us & operator /= (Vec8us & a, Const_uint_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n// vector operator /= : divide\ntemplate <int32_t d>\nstatic inline Vec8us & operator /= (Vec8us & a, Const_int_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n\n// define Vec16c a / const_int(d)\ntemplate <int d>\nstatic inline Vec16c operator / (Vec16c const & a, Const_int_t<d>) {\n    // expand into two Vec8s\n    Vec8s low  = extend_low(a)  / Const_int_t<d>();\n    Vec8s high = extend_high(a) / Const_int_t<d>();\n    return compress(low,high);\n}\n\n// define Vec16c a / const_uint(d)\ntemplate <uint32_t d>\nstatic inline Vec16c operator / (Vec16c const & a, Const_uint_t<d>) {\n    Static_error_check< (uint8_t(d)<0x80u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned\n    return a / Const_int_t<d>();                              // signed divide\n}\n\n// vector operator /= : divide\ntemplate <int32_t d>\nstatic inline Vec16c & operator /= (Vec16c & a, Const_int_t<d> b) {\n    a = a / b;\n    return a;\n}\n// vector operator /= : divide\ntemplate <uint32_t d>\nstatic inline Vec16c & operator /= (Vec16c & a, Const_uint_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n// define Vec16uc a / const_uint(d)\ntemplate <uint32_t d>\nstatic inline Vec16uc operator / (Vec16uc const & a, Const_uint_t<d>) {\n    // expand into two Vec8usc\n    Vec8us low  = extend_low(a)  / Const_uint_t<d>();\n    Vec8us high = extend_high(a) / Const_uint_t<d>();\n    return compress(low,high);\n}\n\n// define Vec16uc a / const_int(d)\ntemplate <int d>\nstatic inline Vec16uc operator / (Vec16uc const & a, Const_int_t<d>) {\n    Static_error_check< (int8_t(d)>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous\n    return a / Const_uint_t<d>();                         // unsigned divide\n}\n\n// vector operator /= : divide\ntemplate <uint32_t d>\nstatic inline Vec16uc & operator /= (Vec16uc & a, Const_uint_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n// vector operator /= : divide\ntemplate <int32_t d>\nstatic inline Vec16uc & operator /= (Vec16uc & a, Const_int_t<d> b) {\n    a = a / b;\n    return a;\n}\n\n/*****************************************************************************\n*\n*          Horizontal scan functions\n*\n*****************************************************************************/\n\n// Get index to the first element that is true. Return -1 if all are false\nstatic inline int horizontal_find_first(Vec16cb const & x) {\n    uint32_t a = _mm_movemask_epi8(x);\n    if (a == 0) return -1;\n    int32_t b = bit_scan_forward(a);\n    return b;\n}\n\nstatic inline int horizontal_find_first(Vec8sb const & x) {\n    return horizontal_find_first(Vec16cb(x)) >> 1;   // must use signed shift\n}\n\nstatic inline int horizontal_find_first(Vec4ib const & x) {\n    return horizontal_find_first(Vec16cb(x)) >> 2;   // must use signed shift\n}\n\nstatic inline int horizontal_find_first(Vec2qb const & x) {\n    return horizontal_find_first(Vec16cb(x)) >> 3;   // must use signed shift\n}\n\n// Count the number of elements that are true\nstatic inline uint32_t horizontal_count(Vec16cb const & x) {\n    uint32_t a = _mm_movemask_epi8(x);\n    return vml_popcnt(a);\n}\n\nstatic inline uint32_t horizontal_count(Vec8sb const & x) {\n    return horizontal_count(Vec16cb(x)) >> 1;\n}\n\nstatic inline uint32_t horizontal_count(Vec4ib const & x) {\n    return horizontal_count(Vec16cb(x)) >> 2;\n}\n\nstatic inline uint32_t horizontal_count(Vec2qb const & x) {\n    return horizontal_count(Vec16cb(x)) >> 3;\n}\n\n\n/*****************************************************************************\n*\n*          Boolean <-> bitfield conversion functions\n*\n*****************************************************************************/\n\n// to_bits: convert boolean vector to integer bitfield\nstatic inline uint16_t to_bits(Vec16cb const & x) {\n    return (uint16_t)_mm_movemask_epi8(x);\n}\n\n// to_Vec16bc: convert integer bitfield to boolean vector\nstatic inline Vec16cb to_Vec16cb(uint16_t x) {\n    static const uint32_t table[16] = {  // lookup-table\n        0x00000000, 0x000000FF, 0x0000FF00, 0x0000FFFF, \n        0x00FF0000, 0x00FF00FF, 0x00FFFF00, 0x00FFFFFF, \n        0xFF000000, 0xFF0000FF, 0xFF00FF00, 0xFF00FFFF, \n        0xFFFF0000, 0xFFFF00FF, 0xFFFFFF00, 0xFFFFFFFF}; \n    uint32_t a0 = table[x       & 0xF];\n    uint32_t a1 = table[(x>>4)  & 0xF];\n    uint32_t a2 = table[(x>>8)  & 0xF];\n    uint32_t a3 = table[(x>>12) & 0xF];\n    return Vec16cb(Vec16c(Vec4ui(a0, a1, a2, a3)));\n}\n\n// to_bits: convert boolean vector to integer bitfield\nstatic inline uint8_t to_bits(Vec8sb const & x) {\n    __m128i a = _mm_packs_epi16(x, x);  // 16-bit words to bytes\n    return (uint8_t)_mm_movemask_epi8(a);\n}\n\n// to_Vec8sb: convert integer bitfield to boolean vector\nstatic inline Vec8sb to_Vec8sb(uint8_t x) {\n    static const uint32_t table[16] = {  // lookup-table\n        0x00000000, 0x000000FF, 0x0000FF00, 0x0000FFFF, \n        0x00FF0000, 0x00FF00FF, 0x00FFFF00, 0x00FFFFFF, \n        0xFF000000, 0xFF0000FF, 0xFF00FF00, 0xFF00FFFF, \n        0xFFFF0000, 0xFFFF00FF, 0xFFFFFF00, 0xFFFFFFFF}; \n    uint32_t a0 = table[x       & 0xF];\n    uint32_t a1 = table[(x>>4)  & 0xF];\n    Vec4ui   b  = Vec4ui(a0, a1, a0, a1);\n    return _mm_unpacklo_epi8(b, b);  // duplicate bytes to 16-bit words\n}\n\n#if INSTRSET < 9 || MAX_VECTOR_SIZE < 512\n// These functions are defined in Vectori512.h if AVX512 instruction set is used\n\n// to_bits: convert boolean vector to integer bitfield\nstatic inline uint8_t to_bits(Vec4ib const & x) {\n    __m128i a = _mm_packs_epi32(x, x);  // 32-bit dwords to 16-bit words\n    __m128i b = _mm_packs_epi16(a, a);  // 16-bit words to bytes\n    return _mm_movemask_epi8(b) & 0xF;\n}\n\n// to_Vec4ib: convert integer bitfield to boolean vector\nstatic inline Vec4ib to_Vec4ib(uint8_t x) {\n    static const uint32_t table[16] = {    // lookup-table\n        0x00000000, 0x000000FF, 0x0000FF00, 0x0000FFFF, \n        0x00FF0000, 0x00FF00FF, 0x00FFFF00, 0x00FFFFFF, \n        0xFF000000, 0xFF0000FF, 0xFF00FF00, 0xFF00FFFF, \n        0xFFFF0000, 0xFFFF00FF, 0xFFFFFF00, 0xFFFFFFFF}; \n    uint32_t a = table[x & 0xF];           // 4 bytes\n    __m128i b = _mm_cvtsi32_si128(a);      // transfer to vector register\n    __m128i c = _mm_unpacklo_epi8(b, b);   // duplicate bytes to 16-bit words\n    __m128i d = _mm_unpacklo_epi16(c, c);  // duplicate 16-bit words to 32-bit dwords\n    return d;\n}\n\n// to_bits: convert boolean vector to integer bitfield\nstatic inline uint8_t to_bits(Vec2qb const & x) {\n    uint32_t a = _mm_movemask_epi8(x);\n    return (a & 1) | ((a >> 7) & 2);\n}\n\n// to_Vec2qb: convert integer bitfield to boolean vector\nstatic inline Vec2qb to_Vec2qb(uint8_t x) {\n    return Vec2qb(Vec2q(-(x&1), -((x>>1)&1)));\n}\n\n#else  // function prototypes here only\n\n// to_bits: convert boolean vector to integer bitfield\nstatic inline uint8_t to_bits(Vec4ib x);\n\n// to_Vec4ib: convert integer bitfield to boolean vector\nstatic inline Vec4ib to_Vec4ib(uint8_t x);\n\n// to_bits: convert boolean vector to integer bitfield\nstatic inline uint8_t to_bits(Vec2qb x);\n\n// to_Vec2qb: convert integer bitfield to boolean vector\nstatic inline Vec2qb to_Vec2qb(uint8_t x);\n\n#endif  // INSTRSET < 9 || MAX_VECTOR_SIZE < 512\n\n#ifdef VCL_NAMESPACE\n}\n#endif\n\n#endif // VECTORI128_H\n",
			"settings":
			{
				"buffer_size": 248704,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/simd.t.cc",
			"settings":
			{
				"buffer_size": 2002,
				"line_ending": "Windows"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 104.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"c++",
				"Set Syntax: C++"
			],
			[
				"c+",
				"Set Syntax: C++"
			],
			[
				"pack",
				"Package Control: Remove Package"
			],
			[
				"install",
				"Package Control: Install Package"
			],
			[
				"instal",
				"Package Control: Install Package"
			]
		],
		"width": 400.0
	},
	"console":
	{
		"height": 146.0,
		"history":
		[
			"import urllib.request,os,hashlib; h = 'df21e130d211cfc94d9b0905775a7c0f' + '1e3d39e33b79698005270310898eea76'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/Z/documents/programming/projetos-c++/game-engine/projects/VS/compile.bat",
		"/D/Program Files (x86)/Microsoft Visual Studio 14.0/Common7/Tools/VsDevCmd.bat",
		"/Z/documents/programming/projetos-c++/game-engine/projects/VS/start.bat",
		"/D/Program Files (x86)/Microsoft Visual Studio 14.0/VC/vcvarsall.bat",
		"/D/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/vcvars64.bat",
		"/Z/documents/programming/projetos-c++/game-engine/src/math/point.cc",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/point.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/fixed.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/math.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/vertex.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/scene.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/render.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/camera.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/color.h",
		"/Z/documents/programming/projetos-c++/game-engine/src/math/sparse.cc",
		"/X/home/caio/Documents/programming/sdl-engine/include/graphics/color.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/graphics/camera.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/graphics/render.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/math/sparse.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/graphics/vertex.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/graphics/scene.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/math/util.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/settings.h",
		"/Z/documents/programming/projetos-c++/game-engine/src/graphics/camera.cc",
		"/Z/documents/programming/projetos-c++/game-engine/src/graphics/render.cc",
		"/Z/documents/programming/projetos-c++/game-engine/src/graphics/scene.cc",
		"/Z/documents/programming/projetos-c++/game-engine/src/graphics/color.cc",
		"/Z/documents/programming/projetos-c++/game-engine/include/graphics/camera.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/math/point.h",
		"/Z/documents/programming/projetos-c++/game-engine/include/graphics/color.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/graphics/light.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/graphics/render.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/game/game.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/game/settings.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/graphics/vertex.h",
		"/X/home/caio/Documents/programming/sdl-engine/include/math/sparse.h",
		"/X/home/caio/Documents/programming/sdl-engine/src/sparse.c",
		"/X/home/caio/Documents/programming/sdl-engine/src/contiguous.c",
		"/X/home/caio/Documents/programming/sdl-engine/include/scene/scene.h",
		"/X/home/caio/Documents/programming/sdl-engine/src/scene.c",
		"/X/home/caio/Documents/programming/sdl-engine/include/math/vertex.h",
		"/X/home/caio/Documents/programming/sdl-engine/src/settings.c",
		"/X/home/caio/Documents/programming/sdl-engine/src/game.c",
		"/X/home/caio/Documents/programming/sdl-engine/src/render.c",
		"/X/home/caio/Documents/programming/sdl-engine/src/color.c",
		"/X/home/caio/Documents/programming/sdl-engine/src/point.c",
		"/X/home/caio/Documents/programming/sdl-engine/src/camera.c",
		"/X/home/caio/Documents/programming/sdl-engine/include/math/point.h",
		"/D/Program Files (x86)/Steam/steamapps/common/Sid Meier's Civilization V/Assets/DLC/Expansion2/Gameplay/XML/GameInfo/CIV5HandicapInfos.xml",
		"/D/Program Files (x86)/Steam/steamapps/common/TreeOfSavior/release/languageData/English/UI.tsv",
		"/D/Program Files (x86)/Magic Set Editor 2/data/magic-watermarks.mse-include/watermark-names",
		"/D/Program Files (x86)/Magic Set Editor 2/data/magic-watermarks.mse-include/watermarks",
		"/Z/documents/games/magic/starcraft-set/StarCraft.mse-set",
		"/Z/documents/programming/HTTP_Client/keywords.txt",
		"/Z/documents/programming/HTTP_Client/HTTP_Client.cpp",
		"/Z/documents/programming/HTTP_Client/HTTP_Client.h",
		"/Z/documents/programming/HttpClient/keywords.txt",
		"/Z/documents/programming/HttpClient/library.properties",
		"/Z/documents/programming/HttpClient/HTTP_Client.cpp",
		"/Z/documents/programming/HttpClient/HTTP_Client.h",
		"/Z/documents/programming/HttpClient/b64.h"
	],
	"find":
	{
		"height": 21.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"576",
			"97",
			"auto",
			"vec4i",
			"canMul",
			"low = _mm_srli_epi64(low,P);",
			"_mm_srli_epi64(",
			",_mm_shuffle_epi",
			",_mm",
			"_mm256_srli_epi64(",
			"_mm_",
			"128i",
			"si128",
			"16",
			"_MM256_SHUFFLE(",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"(a",
			"_MM_SETR",
			"(a",
			"AS_4CHARS",
			"int",
			"Math::fixed<Math::INT32,P>::ROUND",
			"Engine::",
			"AS_4CHARS",
			"AS_2CHARS",
			"char",
			",",
			"int32_t",
			"int16_t",
			"Math::",
			"\\",
			"FIXED_SIMD",
			"/",
			"_mm_castsi128_ps(",
			"256_SHUFFLE(",
			"256_SH",
			"256_sh",
			"256_SH",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"_MM_",
			"_mm_",
			"128",
			"simd",
			"_mm_shuffle_epi32(",
			"rhs",
			"_mm_castps_si128",
			"rhs",
			"hi",
			"_MM_SHUFFLE(",
			"_mm_shuffle",
			"_MM_SHUFFLE",
			"_mm_shuffle",
			"_MM_SHUFFLE",
			"1000000",
			"\n	\n",
			"\n		\n",
			"\"hasAVX2\"",
			"Engine::Processor::",
			"int",
			"1000000",
			"100000",
			"1000000",
			"100000000",
			"> ",
			"200000000",
			"800000000",
			"400000000",
			"100000000",
			"8000000",
			"64",
			"640000",
			"1000000",
			"6400",
			"num_threads",
			"640000",
			"640000000",
			"))\n		",
			"32",
			"32000000",
			"3200000",
			" 3200000]",
			"[3",
			"32000000",
			"32000",
			"3200",
			"1600",
			"*i",
			"16000",
			"32000",
			"1600",
			"a",
			"inline ",
			"void ",
			"false;}/",
			"Processor::hasSSE2()",
			"s == AVX ? Processor::hasAVX() : Processor::hasSSE()",
			"16",
			"ore_",
			"load_s",
			"load_p",
			"8",
			"mul",
			"8",
			"sub",
			"add",
			"mul",
			"false;}",
			"64"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": true,
		"replace_history":
		[
			"\\t"
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 2,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/sparse.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5630,
						"regions":
						{
						},
						"selection":
						[
							[
								4485,
								4485
							]
						],
						"settings":
						{
							"rulers":
							[
								100
							],
							"syntax": "Packages/C++/C++.sublime-syntax",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 2422.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/graphics/ui.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1192,
						"regions":
						{
						},
						"selection":
						[
							[
								680,
								680
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 17,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/string.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2875,
						"regions":
						{
						},
						"selection":
						[
							[
								602,
								602
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 126.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/simd.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11047,
						"regions":
						{
						},
						"selection":
						[
							[
								127,
								127
							]
						],
						"settings":
						{
							"rulers":
							[
								100
							],
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/variadic.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1892,
						"regions":
						{
						},
						"selection":
						[
							[
								1395,
								1395
							]
						],
						"settings":
						{
							"rulers":
							[
								100
							],
							"syntax": "Packages/C++/C++.sublime-syntax",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 11,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/core/kmp.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 701,
						"regions":
						{
						},
						"selection":
						[
							[
								244,
								244
							]
						],
						"settings":
						{
							"auto_name": "#pragma once",
							"syntax": "Packages/C++/C++.sublime-syntax",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 12,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/util.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 312,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				}
			]
		},
		{
			"selected": 11,
			"sheets":
			[
				{
					"buffer": 7,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/fixed.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5793,
						"regions":
						{
						},
						"selection":
						[
							[
								3925,
								3925
							]
						],
						"settings":
						{
							"rulers":
							[
								100
							],
							"syntax": "Packages/C++/C++.sublime-syntax",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 1176.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 8,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7659,
						"regions":
						{
						},
						"selection":
						[
							[
								896,
								896
							]
						],
						"settings":
						{
							"auto_name": "/****************************  instrset.h   ******",
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 756.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/sparse.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 37,
						"regions":
						{
						},
						"selection":
						[
							[
								37,
								37
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 22,
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/sparse.t.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 711,
						"regions":
						{
						},
						"selection":
						[
							[
								37,
								37
							]
						],
						"settings":
						{
							"auto_name": "#include <game-engine/math/sparse.h>",
							"syntax": "Packages/C++11/C++11.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 21,
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/point.t.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2140,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"auto_name": "#include <game-engine/math/point.h>",
							"syntax": "Packages/C++11/C++11.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 20,
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 36,
						"regions":
						{
						},
						"selection":
						[
							[
								36,
								36
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 16,
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed.t.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1272,
						"regions":
						{
						},
						"selection":
						[
							[
								1022,
								1022
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 15,
					"type": "text"
				},
				{
					"buffer": 14,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed_simd.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 42,
						"regions":
						{
						},
						"selection":
						[
							[
								42,
								42
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 13,
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/math/fixed_simd.t.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 660,
						"regions":
						{
						},
						"selection":
						[
							[
								647,
								647
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 16,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/string.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6929,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"rulers":
							[
								70
							],
							"syntax": "Packages/C++11/C++11.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 3514.0,
						"zoom_level": 1.0
					},
					"stack_index": 18,
					"type": "text"
				},
				{
					"buffer": 17,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/simd.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2073,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 14,
					"type": "text"
				},
				{
					"buffer": 18,
					"file": "/Z/documents/programming/projetos-c++/game-engine/projects/VS/compile.bat",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 95,
						"regions":
						{
						},
						"selection":
						[
							[
								95,
								95
							]
						],
						"settings":
						{
							"syntax": "Packages/Batch File/Batch File.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 19,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/string.t.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 113,
						"regions":
						{
						},
						"selection":
						[
							[
								113,
								113
							]
						],
						"settings":
						{
							"auto_name": "#include <game-engine/core/string.h>",
							"syntax": "Packages/C++11/C++11.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 19,
					"type": "text"
				}
			]
		},
		{
			"selected": 2,
			"sheets":
			[
				{
					"buffer": 20,
					"file": "/Z/documents/programming/projetos-c++/game-engine/include/game-engine/math/fixed_simd.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7876,
						"regions":
						{
						},
						"selection":
						[
							[
								1876,
								1876
							]
						],
						"settings":
						{
							"auto_name": "#pragma once",
							"rulers":
							[
								100
							],
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 294.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 21,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 248704,
						"regions":
						{
						},
						"selection":
						[
							[
								209345,
								209345
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 75868.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 22,
					"file": "/Z/documents/programming/projetos-c++/game-engine/src/core/simd.t.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2002,
						"regions":
						{
						},
						"selection":
						[
							[
								1916,
								1916
							]
						],
						"settings":
						{
							"syntax": "Packages/C++11/C++11.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 21.0
	},
	"input":
	{
		"height": 0.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			],
			[
				1,
				0,
				2,
				1
			],
			[
				2,
				0,
				3,
				1
			]
		],
		"cols":
		[
			0.0,
			0.33,
			0.66,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 208.0
	},
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "game-engine.sublime-project",
	"replace":
	{
		"height": 38.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 156.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
